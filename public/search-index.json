[{"content":"","length":0,"title":null,"type":"cover","url":"http://localhost:8080/"},{"content":"","length":0,"title":"Contents","type":"table-of-contents","url":"http://localhost:8080/contents/"},{"content":"","length":0,"subtitle":null,"title":"Director's Foreword","type":"essay","url":"http://localhost:8080/foreword/"},{"content":"<p><em>The Imitation Game</em> surveys the extraordinary uses (and abuses) of artificial intelligence<sup class=\"footnote-ref\"><a href=\"#fn1\" id=\"fnref1\">1</a></sup> (AI) in the production of contemporary visual culture. The exhibition follows a chronological narrative that first examines the development of artificial intelligence from the 1950s to the present. Building on this foundation, it then emphasizes the explosive growth of AI over the past decade across creative disciplines—including animation, architecture, art, fashion, graphic design, urban design and video games.</p>\n<p>From the early moments of its creation, AI has captured the imaginations of cultural producers around the world. The idea that machines could think and express themselves independently of their human makers has been received with great skepticism, some joy, and a healthy dose of anxiety.</p>\n<p>Unsurprisingly, much of the early research on AI engaged with human-centred ideas of imitation and emulation. Most notably, in 1950, Alan Turing formulated an “imitation game” for testing a machine’s capacity to display intelligent behaviour in a manner that would be indistinguishable from natural human behaviour. Around the same time, researchers began to explore the possibility of an artificial neural network<sup class=\"footnote-ref\"><a href=\"#fn2\" id=\"fnref2\">2</a></sup> modelled directly on the human brain.</p>\n<p>These early investigations set frameworks for much of the invention that followed in the 20th century. In the past decade, huge advances in the design and production of computing hardware have laid the groundwork for an unprecedented growth of AI as a fundamental tool with wide creative applications.</p>\n<p>The exhibition begins with an interactive introduction inviting visitors to identify diverse areas of cultural production influenced by AI. Twenty “objects of wonder” have been selected to offer a chronological history of AI and visual culture. And two special projects by artists Sougwen Chung and Scott Eaton offer compelling insights into the collaborative and creative powers of AI.</p>\n<p>This exhibition is organized by the Vancouver Art Gallery and curated by Bruce Grenville, Senior Curator and Glenn Entis, Guest Curator</p>\n<br/>\n<h2>An Interactive Introduction</h2>\n<figure id=\"interactiveintro\" class=\"q-figure q-figure--image\"> <a class=\"q-figure__modal-link\" href=\"#interactiveintro\" data-outputs-include=\"html\"><img alt=\".\" class=\"q-figure__image\" src=\"\\_assets\\images\\install\\interactive-intro.jpg\"> </a> <figcaption class=\"q-figure__caption\" data-outputs-include=\"html\"> <span class=\"q-figure__caption-content\">.</span> <span class=\"q-figure__credit\">Photo: Ian Lefebvre and Jessica Jacobson, Vancouver Art Gallery</span> </figcaption> <img alt=\".\" class=\"q-figure__image\" src=\"\\_assets\\images\\install\\interactive-intro.jpg\" data-outputs-include=\"epub,pdf\"> <figcaption class=\"q-figure__caption\" data-outputs-include=\"epub,pdf\"> <span class=\"q-figure__caption-content\">.</span> <span class=\"q-figure__credit\">Photo: Ian Lefebvre and Jessica Jacobson, Vancouver Art Gallery</span> </figcaption> </figure>\n<p>This interactive introduction invites visitors to explore the range of images, objects and ideas that shape this exhibition. Visitors may be familiar with the use of artificial intelligence (AI) in their encounters with the preference engines<sup class=\"footnote-ref\"><a href=\"#fn3\" id=\"fnref3\">3</a></sup> that select their online music, videos or shopping, but the use of AI as an essential tool in a wide range of creative processes is often invisible.</p>\n<p>The interactive installation employs a basic form of AI that combines computer vision, machine learning and neural networks that analyze hand gestures in real time and trigger an appropriate response to visitors’ interactions. The images and information on this wall are addressed in greater depth throughout the exhibition.</p>\n<p>The interactive Introduction to <em>The Imitation Game</em> is the result of a collaboration with Vancouver’s Centre for Digital Media, under the direction of Larry Bafia. Graduate students in this program worked as a team to design, program and produce this wall. Many thanks to that talented team:</p>\n<ul>\n<li>Allyson Zhong – Project Manager/UX</li>\n<li>Luisa Martinez Riaño – UX/UI</li>\n<li>Sooq Won – Video Artist/Producer</li>\n<li>Farbod Tabaei – Concept Artist/Developer</li>\n<li>Pieteke MacMahon – Software Developer/UX/UI Designer</li>\n<li>Larry Bafia – team advisor and program director</li>\n</ul>\n<p>Cindy Shi contributed post-production programming support.</p>\n<p>The Centre for Digital Media was founded in 2007 and is a unique graduate program whose degree is imprinted with the seals of its four partner institutions: University of British Columbia, Emily Carr University of Art + Design, Simon Fraser University and British Columbia Institute of Technology.</p>\n<div class=\"backmatter\">\n<h2>Notes</h2>\n</div><section class=\"footnotes\">\n<ol class=\"footnotes-list\">\n<li id=\"fn1\" class=\"footnote-item\"><p>Artificial Intelligence (AI) is the ability of a digital computer or computer-controlled robot to perform tasks commonly associated with intelligent beings. <a href=\"#fnref1\" class=\"footnote-backref\">↩︎</a></p>\n</li>\n<li id=\"fn2\" class=\"footnote-item\"><p>Neural networks are computing systems with interconnected nodes that work much like neurons in the human brain. Using algorithms, they can recognize hidden patterns and correlations in raw data, cluster and classify it, and—over time—continuously learn and improve. <a href=\"#fnref2\" class=\"footnote-backref\">↩︎</a></p>\n</li>\n<li id=\"fn3\" class=\"footnote-item\"><p>A preference engine, or a recommender system, or a recommendation system is a subclass of information filtering system that seeks to predict the ‘rating’ or ‘preference’ a user would give to an item. <a href=\"#fnref3\" class=\"footnote-backref\">↩︎</a></p>\n</li>\n</ol>\n</section>\n","length":706,"title":"Introduction","type":"essay","url":"http://localhost:8080/introduction/"},{"content":"<p>The 20 Objects of Wonder selected for <em>The Imitation Game</em> offer a unique, chronological insight into the history of artificial intelligence (AI), including the critical advances that have shaped its present configuration, and those that point the way toward its future uses. Seen together, these many images, objects and events reveal the breadth and depth of influence exerted by AI on visual culture.</p>\n<p>Early advances in AI laid the foundations for an intimate relationship between humans and machines, and many of the early theorists and researchers actively speculated on fundamental questions of consciousness, creativity and intelligence.</p>\n<p>The exponential growth of computer processing capacity in the past twenty years has rapidly accelerated AI research, and widely distributed its use across all fields of human endeavour. The increased availability of moderately priced computers and sophisticated programs with accessible interfaces has further expanded the reach of AI-assisted thinking and making, so that today it is reasonable to say that AI is a critical component of any creative practice.</p>\n<p>From Norbert Wiener’s cybernetic moth (1949) to Neri Oxman’s <em>Synthetic Apiary</em> (2020), the evolution of AI is marked by a deep and abiding commitment to research, experimentation and creativity.</p>\n","length":191,"title":"20 Objects of Wonder","type":"table-of-contents","url":"http://localhost:8080/20-objects/"},{"content":"<p>Norbert Wiener was an American mathematician and philosopher who joined the faculty at MIT in 1919. He is renowned for his collaborative work in the 1920s on early computers, and during World War II was active in a small group of creative interdisciplinary thinkers who produced the first intelligent automated machines. This interdisciplinary view spawned several new fields of research including: communications, computation, automation, information theory, neuroscience, and most notably, cybernetics.<sup class=\"footnote-ref\"><a href=\"#fn1\" id=\"fnref1\">1</a></sup> The latter was named by Wiener in his book, <em>Cybernetics: Control and Communication in the Animal and the Machine</em> (1948).</p>\n<p>Wiener recognized the value of sharing new ideas with a broad public and played the role of showman well. His “Moth” has a place in a long history of mechanical animals that were used to describe advances in technology and design. In order to build this public representation of “cybernetics,” Wiener started from the proposition that all systems—organic, mechanical, social or aesthetic—are defined by their ability to acquire, use, retain and transmit information.</p>\n<section id=\"section-the-moth\" class=\"accordion-section\" data-outputs-include=\"epub,pdf\"> <h2>The Moth</h2> <p>In the late 1940s, Norbert Wiener, then a professor at MIT, teamed up with J. Wiesner (from the MIT Research Laboratory of Electronics) and a fabricator named H. Singleton to build a demonstration machine.</p> <p>The machine was a three-wheeled cart with two front facing photocells (sensors that detect light) and two more on the sides. The output from the cells was communicated to the steering mechanism on the front wheel, moving the cart toward or away from a light source. Depending on the intensity of the light, the Moth demonstrated a jittering tremor, which closely resembled animal neurological responses to stimuli observed in current neuroscience studies.</p> <p>In the images on the right, Wiener poses with the Moth in a portrait for <em>Life</em> magazine; a timelapse image traces the path of the Moth as it follows the movement of a flashlight down a corridor; and an open view of the Moth reveals its intricate construction.</p> </section> <details class=\"accordion-section\" id=\"section-the-moth\"  data-outputs-include=\"html\"> <summary class=\"accordion-section__heading accordion-section__heading-level-null accordion-section__controls accordion-section__controls--arrow\" tabindex=\"1\"> <button aria-label=\"Copy page section link to clipboard\" class=\"accordion-section__copy-link-button\" data-outputs-exclude=\"pdf,epub\" value=\"#section-the-moth\" tabindex=\"2\" > § </button> <span aria-hidden=\"true\" class=\"accordion-tooltip\" data-outputs-exclude=\"pdf,epub\" > Copied page section link to clipboard </span> <h2>The Moth</h2> </summary> <section class=\"accordion-section__body\"><p>In the late 1940s, Norbert Wiener, then a professor at MIT, teamed up with J. Wiesner (from the MIT Research Laboratory of Electronics) and a fabricator named H. Singleton to build a demonstration machine.</p> <p>The machine was a three-wheeled cart with two front facing photocells (sensors that detect light) and two more on the sides. The output from the cells was communicated to the steering mechanism on the front wheel, moving the cart toward or away from a light source. Depending on the intensity of the light, the Moth demonstrated a jittering tremor, which closely resembled animal neurological responses to stimuli observed in current neuroscience studies.</p> <p>In the images on the right, Wiener poses with the Moth in a portrait for <em>Life</em> magazine; a timelapse image traces the path of the Moth as it follows the movement of a flashlight down a corridor; and an open view of the Moth reveals its intricate construction.</p> </section> </details>\n<section id=\"section-cybernetics-and-society\" class=\"accordion-section\" data-outputs-include=\"epub,pdf\"> <h2>Cybernetics and Society</h2> <p>In 1950, Wiener published <em>The Human Use of Human Beings</em> in which he decisively described the fundamental role of communication in modern life:</p> <blockquote> <p><em>It is the thesis of this book that society can only be understood through a study of the messages and the communication facilities which belong to it; and that in the future development of these messages and communication facilities, messages between man and machines, between machines and man, and between machine and machine, are destined to play an ever-increasing part.</em></p> </blockquote> </section> <details class=\"accordion-section\" id=\"section-cybernetics-and-society\"  data-outputs-include=\"html\"> <summary class=\"accordion-section__heading accordion-section__heading-level-null accordion-section__controls accordion-section__controls--arrow\" tabindex=\"1\"> <button aria-label=\"Copy page section link to clipboard\" class=\"accordion-section__copy-link-button\" data-outputs-exclude=\"pdf,epub\" value=\"#section-cybernetics-and-society\" tabindex=\"2\" > § </button> <span aria-hidden=\"true\" class=\"accordion-tooltip\" data-outputs-exclude=\"pdf,epub\" > Copied page section link to clipboard </span> <h2>Cybernetics and Society</h2> </summary> <section class=\"accordion-section__body\"><p>In 1950, Wiener published <em>The Human Use of Human Beings</em> in which he decisively described the fundamental role of communication in modern life:</p> <blockquote> <p><em>It is the thesis of this book that society can only be understood through a study of the messages and the communication facilities which belong to it; and that in the future development of these messages and communication facilities, messages between man and machines, between machines and man, and between machine and machine, are destined to play an ever-increasing part.</em></p> </blockquote> </section> </details>\n<section id=\"section-w-grey-walter-s-tortoises\" class=\"accordion-section\" data-outputs-include=\"epub,pdf\"> <h2>W. Grey Walter’s Tortoises</h2> <p>W. Grey Walter began his study of mechanical animals in the 1940s as part of his research into the neurophysiology of the brain. Walter, working independently of Norbert Wiener and his circle, saw the value of building simple machines that mimicked the mental processes of humans and animals. Using light-sensitive and touch-sensitive control mechanisms, Walter’s “tortoises” responded to their environment—bumping and jostling their way around a room.</p> <p>In a 1950 article for <em>Scientific American</em>, entitled “An Imitation of Life,” Walter described the close relationship between the animal brain and the recent design of computing machines. He went on to argue that his “tortoises” and their behaviours—characterized by “uncertainty, randomness, free will and independence”—illustrated the possibility of a machine consciousness.</p> </section> <details class=\"accordion-section\" id=\"section-w-grey-walter-s-tortoises\"  data-outputs-include=\"html\"> <summary class=\"accordion-section__heading accordion-section__heading-level-null accordion-section__controls accordion-section__controls--arrow\" tabindex=\"1\"> <button aria-label=\"Copy page section link to clipboard\" class=\"accordion-section__copy-link-button\" data-outputs-exclude=\"pdf,epub\" value=\"#section-w-grey-walter-s-tortoises\" tabindex=\"2\" > § </button> <span aria-hidden=\"true\" class=\"accordion-tooltip\" data-outputs-exclude=\"pdf,epub\" > Copied page section link to clipboard </span> <h2>W. Grey Walter’s Tortoises</h2> </summary> <section class=\"accordion-section__body\"><p>W. Grey Walter began his study of mechanical animals in the 1940s as part of his research into the neurophysiology of the brain. Walter, working independently of Norbert Wiener and his circle, saw the value of building simple machines that mimicked the mental processes of humans and animals. Using light-sensitive and touch-sensitive control mechanisms, Walter’s “tortoises” responded to their environment—bumping and jostling their way around a room.</p> <p>In a 1950 article for <em>Scientific American</em>, entitled “An Imitation of Life,” Walter described the close relationship between the animal brain and the recent design of computing machines. He went on to argue that his “tortoises” and their behaviours—characterized by “uncertainty, randomness, free will and independence”—illustrated the possibility of a machine consciousness.</p> </section> </details>\n<section id=\"section-the-hopkins-beast\" class=\"accordion-section\" data-outputs-include=\"epub,pdf\"> <h2>The Hopkins Beast</h2> <p>The Hopkins Beast was a mobile robot built in the 1960s at the Johns Hopkins University Applied Physics Laboratory. Like Norbert Wiener’s Moth and Grey Walter’s “tortoises,” the Hopkins Beast was cybernetic, relying on signals, feedback and control systems to govern its responses and movements.</p> <p>The Hopkins Beast used photocell optics and sonar to navigate the rooms and halls of the Applied Physics Laboratory, while searching for black coloured charging outlets that would allow it to plug-in, charge and then continue its exploration. Stairways, doors, pipes and other obstacles were recognized by ultrasonic transducers and appropriate actions were taken to avoid collision or entrapment. This demonstration of multiple co-ordinated behaviours most closely resembles something like the capacity of a large nucleated cell.</p> </section> <details class=\"accordion-section\" id=\"section-the-hopkins-beast\"  data-outputs-include=\"html\"> <summary class=\"accordion-section__heading accordion-section__heading-level-null accordion-section__controls accordion-section__controls--arrow\" tabindex=\"1\"> <button aria-label=\"Copy page section link to clipboard\" class=\"accordion-section__copy-link-button\" data-outputs-exclude=\"pdf,epub\" value=\"#section-the-hopkins-beast\" tabindex=\"2\" > § </button> <span aria-hidden=\"true\" class=\"accordion-tooltip\" data-outputs-exclude=\"pdf,epub\" > Copied page section link to clipboard </span> <h2>The Hopkins Beast</h2> </summary> <section class=\"accordion-section__body\"><p>The Hopkins Beast was a mobile robot built in the 1960s at the Johns Hopkins University Applied Physics Laboratory. Like Norbert Wiener’s Moth and Grey Walter’s “tortoises,” the Hopkins Beast was cybernetic, relying on signals, feedback and control systems to govern its responses and movements.</p> <p>The Hopkins Beast used photocell optics and sonar to navigate the rooms and halls of the Applied Physics Laboratory, while searching for black coloured charging outlets that would allow it to plug-in, charge and then continue its exploration. Stairways, doors, pipes and other obstacles were recognized by ultrasonic transducers and appropriate actions were taken to avoid collision or entrapment. This demonstration of multiple co-ordinated behaviours most closely resembles something like the capacity of a large nucleated cell.</p> </section> </details>\n<section id=\"section-aibo-and-spot\" class=\"accordion-section\" data-outputs-include=\"epub,pdf\"> <h2>Aibo and Spot</h2> <p>Aibo and Spot are part of a centuries-old tradition of designing machine animals that hold the dual purpose of introducing new technologies and delighting imaginations.</p> <p>Boston Dynamics’ Spot is a mobile robot with extraordinary capacity. Unlike its predecessors, it uses onboard artificial intelligence to guide its movement and responses. Employing neural networks built on deep learning models, Spot can analyze objects and environments, and learn from its accumulated data and experiences. Its incredible physical agility is hilariously demonstrated in Boston Dynamics’ widely shared video marketing campaigns.</p> <p>Aibo is designed as a human companion. Its lifelike movements and interactions are intended to engage its human owners and to learn from those interactions. A massive array of sensors all over its body feed data to its facial and voice recognition software, and deep learning algorithms train Aibo to respond uniquely to the various individuals that might occupy a household.</p> </section> <details class=\"accordion-section\" id=\"section-aibo-and-spot\"  data-outputs-include=\"html\"> <summary class=\"accordion-section__heading accordion-section__heading-level-null accordion-section__controls accordion-section__controls--arrow\" tabindex=\"1\"> <button aria-label=\"Copy page section link to clipboard\" class=\"accordion-section__copy-link-button\" data-outputs-exclude=\"pdf,epub\" value=\"#section-aibo-and-spot\" tabindex=\"2\" > § </button> <span aria-hidden=\"true\" class=\"accordion-tooltip\" data-outputs-exclude=\"pdf,epub\" > Copied page section link to clipboard </span> <h2>Aibo and Spot</h2> </summary> <section class=\"accordion-section__body\"><p>Aibo and Spot are part of a centuries-old tradition of designing machine animals that hold the dual purpose of introducing new technologies and delighting imaginations.</p> <p>Boston Dynamics’ Spot is a mobile robot with extraordinary capacity. Unlike its predecessors, it uses onboard artificial intelligence to guide its movement and responses. Employing neural networks built on deep learning models, Spot can analyze objects and environments, and learn from its accumulated data and experiences. Its incredible physical agility is hilariously demonstrated in Boston Dynamics’ widely shared video marketing campaigns.</p> <p>Aibo is designed as a human companion. Its lifelike movements and interactions are intended to engage its human owners and to learn from those interactions. A massive array of sensors all over its body feed data to its facial and voice recognition software, and deep learning algorithms train Aibo to respond uniquely to the various individuals that might occupy a household.</p> </section> </details>\n<section id=\"section-roomba-j7\" class=\"accordion-section\" data-outputs-include=\"epub,pdf\"> <h2>Roomba j7</h2> <p>The Roomba j7 vacuum is described as having Home Intelligence—essentially an adaptive artificial intelligence that allows it to map the architecture of your home and to recognize and negotiate any obstacles it encounters. It bears an uncanny resemblance to its machine-animal predecessors and, like Grey Walter’s “tortoises,” it will seek out its recharging station when its battery runs low.</p> <p>Computer-controlled by onboard software, the Roomba j7’s acoustic sensors calculate the volume of dirt and adjust its cleaning method accordingly. The capacity to independently learn through repetition and error is critical to this robot’s success.</p> <p>Roomba and similar autonomous robotic vacuum cleaners have attracted a large community of hackers who modify the vacuums’ components to produce alternative behaviours, or add equipment to extend their capacity. Drawing machines, remote cameras, delivery bots, pet minders—the opportunities are seemingly endless.</p> </section> <details class=\"accordion-section\" id=\"section-roomba-j7\"  data-outputs-include=\"html\"> <summary class=\"accordion-section__heading accordion-section__heading-level-null accordion-section__controls accordion-section__controls--arrow\" tabindex=\"1\"> <button aria-label=\"Copy page section link to clipboard\" class=\"accordion-section__copy-link-button\" data-outputs-exclude=\"pdf,epub\" value=\"#section-roomba-j7\" tabindex=\"2\" > § </button> <span aria-hidden=\"true\" class=\"accordion-tooltip\" data-outputs-exclude=\"pdf,epub\" > Copied page section link to clipboard </span> <h2>Roomba j7</h2> </summary> <section class=\"accordion-section__body\"><p>The Roomba j7 vacuum is described as having Home Intelligence—essentially an adaptive artificial intelligence that allows it to map the architecture of your home and to recognize and negotiate any obstacles it encounters. It bears an uncanny resemblance to its machine-animal predecessors and, like Grey Walter’s “tortoises,” it will seek out its recharging station when its battery runs low.</p> <p>Computer-controlled by onboard software, the Roomba j7’s acoustic sensors calculate the volume of dirt and adjust its cleaning method accordingly. The capacity to independently learn through repetition and error is critical to this robot’s success.</p> <p>Roomba and similar autonomous robotic vacuum cleaners have attracted a large community of hackers who modify the vacuums’ components to produce alternative behaviours, or add equipment to extend their capacity. Drawing machines, remote cameras, delivery bots, pet minders—the opportunities are seemingly endless.</p> </section> </details>\n<br>\n<div class=\"backmatter\">\n</div>\n<section class=\"footnotes\">\n<ol class=\"footnotes-list\">\n<li id=\"fn1\" class=\"footnote-item\"><p>Norbert Wiener introduced the term ‘cybernetics’ in 1948 and described it as ‘the science of control and communications in the animal and machine.’ <a href=\"#fnref1\" class=\"footnote-backref\">↩︎</a></p>\n</li>\n</ol>\n</section>\n","length":2026,"subtitle":null,"title":"Norbert Wiener's Moth","type":"entry","url":"http://localhost:8080/20-objects/01-wieners-moth/"},{"content":"<p>“I propose to consider the question, ‘Can machines think?’” With this deceptively simple declaration Alan Turing started a conversation that has resonated for more than 70 years, and has decisively shaped the development of artificial intelligence.</p>\n<p>Alan Turing was an English-born mathematician, computer scientist, philosopher and theoretical biologist. Turing’s early research provided the theoretical framework for a universal computing machine. As a cryptologist at Bletchley Park during World War II, he showed the power of computing in mechanizing expert human procedures and judgements. In the latter years of the War, he began to speculate that machines could simulate the operation of human brains.</p>\n<p>The “Imitation Game” is the name of a test that Turing sketched out in his landmark paper, “Computing Machinery and Intelligence” (1950). Taking a simple party game as the starting point for his analysis, Turing described a scenario involving three subjects: a person, a machine and an interrogator. The interrogator is placed in a separate room and asked to question the machine and the person in a way that will help the interrogator decide which respondent is human.</p>\n<p>The seeming simplicity of this test was not lost on Turing, and in his paper, he also considers the validity of various possible objections to his proposition: the Theological Objection, the Mathematical Objection, the Lady Lovelace Objection, and so on. Turing concluded his paper with the observation that new developments in computing machine design—including the possibility of a “learning machine” based on the model of child learning—should be actively pursued.</p>\n<p>Ultimately, Turing intended the “Imitation Game” as a tool to think about the possibility of a future computing machine, and the conditions that might be needed to achieve that goal.</p>\n<p>Turing’s “Imitation Game,” or the Turing Test<sup class=\"footnote-ref\"><a href=\"#fn1\" id=\"fnref1\">1</a></sup> as it was later called, affirms the human mind as the ideal model for the formulation of a thinking machine. This isn’t a surprising conceit: humans have long imagined themselves as the ideal model for most things. But Turing chooses to illustrate his argument with a test that is also based in doubt, confusion and ambiguity. This may be a game, he seems to suggest, but what are the implications of a thinking machine that could fool us into believing it is human?</p>\n<section id=\"section-mechanical-turk\" class=\"accordion-section\" data-outputs-include=\"epub,pdf\"> <h2>Mechanical Turk</h2> <p>The history of automata (or self-operating machines) is as old as human civilization, with examples of automata found in China, Greece and North Africa hundreds of years before the common era. Mechanical animals were a favourite subject, but in 18th century Europe sophisticated human automata became popular.</p> <p>In 1770, a chess playing automaton was created by Wolfgang von Kempelen. It immediately captured the imagination of large audiences and toured Europe and North America until it was destroyed in a fire in 1854. The automaton was regarded as a skilled player and won most of the games played while on tour.</p> <p>The possibility of an intelligent machine that could play chess (and defeat humans) has been a persistent dream for hundreds of years, but it wasn’t until the construction of IBM’s Deep Blue in 1997 that machines consistently achieved that goal. Von Kempelen’s Mechanical Turk<sup class=\"footnote-ref\"><a href=\"#fn2\" id=\"fnref2\">2</a></sup> was unfortunately a ruse; the machine was in fact guided by the hand of a human chess-master who was elaborately hidden in the interior of the cabinet.</p> <p>Dressed in Ottoman robes and a turban, Von Kempelen’s automaton was dubbed the casually racist moniker “the Mechanical Turk,” undoubtedly with the intent to acknowledge the early adoption of chess in Persia and the Middle East. It was also just as likely an attempt to appeal to the exoticism and Orientalism that persistently shaped European perceptions of the Middle East during the 18th and 19th centuries.</p> <div class=\"backmatter\"> </div> <section class=\"footnotes\"> <ol class=\"footnotes-list\"> <li id=\"fn2\" class=\"footnote-item\"><p>The Turk, also known as the Mechanical Turk or Automaton Chess Player, was a fake chess-playing machine constructed in the late 18th century. <a href=\"#fnref2\" class=\"footnote-backref\">↩︎</a></p> </li> </ol> </section> </section> <details class=\"accordion-section\" id=\"section-mechanical-turk\"  data-outputs-include=\"html\"> <summary class=\"accordion-section__heading accordion-section__heading-level-null accordion-section__controls accordion-section__controls--arrow\" tabindex=\"1\"> <button aria-label=\"Copy page section link to clipboard\" class=\"accordion-section__copy-link-button\" data-outputs-exclude=\"pdf,epub\" value=\"#section-mechanical-turk\" tabindex=\"2\" > § </button> <span aria-hidden=\"true\" class=\"accordion-tooltip\" data-outputs-exclude=\"pdf,epub\" > Copied page section link to clipboard </span> <h2>Mechanical Turk</h2> </summary> <section class=\"accordion-section__body\"><p>The history of automata (or self-operating machines) is as old as human civilization, with examples of automata found in China, Greece and North Africa hundreds of years before the common era. Mechanical animals were a favourite subject, but in 18th century Europe sophisticated human automata became popular.</p> <p>In 1770, a chess playing automaton was created by Wolfgang von Kempelen. It immediately captured the imagination of large audiences and toured Europe and North America until it was destroyed in a fire in 1854. The automaton was regarded as a skilled player and won most of the games played while on tour.</p> <p>The possibility of an intelligent machine that could play chess (and defeat humans) has been a persistent dream for hundreds of years, but it wasn’t until the construction of IBM’s Deep Blue in 1997 that machines consistently achieved that goal. Von Kempelen’s Mechanical Turk<sup class=\"footnote-ref\"><a href=\"#fn2\" id=\"fnref2\">2</a></sup> was unfortunately a ruse; the machine was in fact guided by the hand of a human chess-master who was elaborately hidden in the interior of the cabinet.</p> <p>Dressed in Ottoman robes and a turban, Von Kempelen’s automaton was dubbed the casually racist moniker “the Mechanical Turk,” undoubtedly with the intent to acknowledge the early adoption of chess in Persia and the Middle East. It was also just as likely an attempt to appeal to the exoticism and Orientalism that persistently shaped European perceptions of the Middle East during the 18th and 19th centuries.</p> <div class=\"backmatter\"> </div> <section class=\"footnotes\"> <ol class=\"footnotes-list\"> <li id=\"fn2\" class=\"footnote-item\"><p>The Turk, also known as the Mechanical Turk or Automaton Chess Player, was a fake chess-playing machine constructed in the late 18th century. <a href=\"#fnref2\" class=\"footnote-backref\">↩︎</a></p> </li> </ol> </section> </section> </details>\n<section id=\"section-ada-lovelace\" class=\"accordion-section\" data-outputs-include=\"epub,pdf\"> <h2>Ada Lovelace</h2> <p>Citing the likely objections of Ada Lovelace to his question “Can machines think?,” Alan Turing devoted a subsection of his “Computing Machinery” paper to her thoughts on the likelihood of an “engine” that might compose or weave according to its own criteria. It is a remarkable “conversation” in which Turing anticipates and counters what he imagines would be Lovelace’s objections, nearly 100 years after her death.</p> <p>Lovelace was a mathematician, scientist and writer who worked closely with Charles Babbage, the inventor of the Difference Engine (a mechanical calculator), and the Analytical Engine (essentially a programmable computer). In 1843, in her notes for a published translation of Babbage’s 1840 lecture on the analytical Engine, Lovelace speculated on the engine’s potential use on things other than numbers:</p> <blockquote> <p><em>Supposing, for instance, that the fundamental relations of pitched sounds in the science of harmony and of musical composition were susceptible of such expression and adaptations, the engine might compose elaborate and scientific pieces of music of any degree of complexity or extent.</em></p> </blockquote> <p>In other notes, Lovelace expresses doubt that the engine could be truly creative or produce something new. Rather, she believes that the engine would compose only what it was instructed to compose. Turing acknowledges why Lovelace might make this objection, but also offers a counter argument: we should be open to “surprises” when considering the capacity of machines, and that creativity (a “creative mental act”) was certainly not out of the question, especially in the case of a “learning machine.”</p> </section> <details class=\"accordion-section\" id=\"section-ada-lovelace\"  data-outputs-include=\"html\"> <summary class=\"accordion-section__heading accordion-section__heading-level-null accordion-section__controls accordion-section__controls--arrow\" tabindex=\"1\"> <button aria-label=\"Copy page section link to clipboard\" class=\"accordion-section__copy-link-button\" data-outputs-exclude=\"pdf,epub\" value=\"#section-ada-lovelace\" tabindex=\"2\" > § </button> <span aria-hidden=\"true\" class=\"accordion-tooltip\" data-outputs-exclude=\"pdf,epub\" > Copied page section link to clipboard </span> <h2>Ada Lovelace</h2> </summary> <section class=\"accordion-section__body\"><p>Citing the likely objections of Ada Lovelace to his question “Can machines think?,” Alan Turing devoted a subsection of his “Computing Machinery” paper to her thoughts on the likelihood of an “engine” that might compose or weave according to its own criteria. It is a remarkable “conversation” in which Turing anticipates and counters what he imagines would be Lovelace’s objections, nearly 100 years after her death.</p> <p>Lovelace was a mathematician, scientist and writer who worked closely with Charles Babbage, the inventor of the Difference Engine (a mechanical calculator), and the Analytical Engine (essentially a programmable computer). In 1843, in her notes for a published translation of Babbage’s 1840 lecture on the analytical Engine, Lovelace speculated on the engine’s potential use on things other than numbers:</p> <blockquote> <p><em>Supposing, for instance, that the fundamental relations of pitched sounds in the science of harmony and of musical composition were susceptible of such expression and adaptations, the engine might compose elaborate and scientific pieces of music of any degree of complexity or extent.</em></p> </blockquote> <p>In other notes, Lovelace expresses doubt that the engine could be truly creative or produce something new. Rather, she believes that the engine would compose only what it was instructed to compose. Turing acknowledges why Lovelace might make this objection, but also offers a counter argument: we should be open to “surprises” when considering the capacity of machines, and that creativity (a “creative mental act”) was certainly not out of the question, especially in the case of a “learning machine.”</p> </section> </details>\n<section id=\"section-the-voight-kampff-machine-test\" class=\"accordion-section\" data-outputs-include=\"epub,pdf\"> <h2>The Voight-Kampff Machine Test</h2> <p>In 1982, Ridley Scott adapted Philip K. Dick’s 1968 novel <em>Do Androids Dream of Electric Sheep?</em> to produce the film <em>Blade Runner</em>. It is a dystopian story set in a near future (2019) in which synthetic humans called “replicants” have escaped from their job as forced labourers on distant off-world colonies. Rick Deckard is a retired “blade runner” brought back to hunt down a particularly virulent group of replicants.</p> <p>The replicants are bio-engineered androids—highly sophisticated robots made of flesh-like material—who have begun to show increasing independence of thought and highly complex emotions. Some replicants have been given false memories and have no awareness of their identity.</p> <p>Deckard, and other blade runners, administer a test—the Voight-Kampff Test—to distinguish between humans and replicants. It is, of course, a Turing Test, but with ominous consequences, as the discovered replicants are quickly and violently dispatched. The test used in the film has many of the characteristics of a polygraph or lie detector test, but with a focus on empathy rather than intelligence. The questions asked by the interrogator are emotionally charged; the replicant’s response is measured by observing breathing, heart rate and pupillary response.</p> <p>Syd Mead was a renowned concept artist whose futuristic visions gave shape to a wide range of films including <em>Blade Runner</em> (1982), <em>Tron</em> (1982), <em>Aliens</em> (1986), and <em>Johnny Mnemonic</em> (1995). For these films he developed the look of entire cities, crowded streetscapes, and elaborate electronic vehicles and equipment that gave a gritty realism to even the most fantastical ideas.</p> <p>Mead’s concept drawing for the Voight-Kampff machine is at once futuristic and anachronistic. It is an intricate automated device with 19th century embellishments such as mechanical bellows and a stylized monocle. In the hands of Rick Deckard, the interrogator who administers the Voight-Kampff test, the machine is a menacing beast that appears to breathe and move of its own accord.</p> </section> <details class=\"accordion-section\" id=\"section-the-voight-kampff-machine-test\"  data-outputs-include=\"html\"> <summary class=\"accordion-section__heading accordion-section__heading-level-null accordion-section__controls accordion-section__controls--arrow\" tabindex=\"1\"> <button aria-label=\"Copy page section link to clipboard\" class=\"accordion-section__copy-link-button\" data-outputs-exclude=\"pdf,epub\" value=\"#section-the-voight-kampff-machine-test\" tabindex=\"2\" > § </button> <span aria-hidden=\"true\" class=\"accordion-tooltip\" data-outputs-exclude=\"pdf,epub\" > Copied page section link to clipboard </span> <h2>The Voight-Kampff Machine Test</h2> </summary> <section class=\"accordion-section__body\"><p>In 1982, Ridley Scott adapted Philip K. Dick’s 1968 novel <em>Do Androids Dream of Electric Sheep?</em> to produce the film <em>Blade Runner</em>. It is a dystopian story set in a near future (2019) in which synthetic humans called “replicants” have escaped from their job as forced labourers on distant off-world colonies. Rick Deckard is a retired “blade runner” brought back to hunt down a particularly virulent group of replicants.</p> <p>The replicants are bio-engineered androids—highly sophisticated robots made of flesh-like material—who have begun to show increasing independence of thought and highly complex emotions. Some replicants have been given false memories and have no awareness of their identity.</p> <p>Deckard, and other blade runners, administer a test—the Voight-Kampff Test—to distinguish between humans and replicants. It is, of course, a Turing Test, but with ominous consequences, as the discovered replicants are quickly and violently dispatched. The test used in the film has many of the characteristics of a polygraph or lie detector test, but with a focus on empathy rather than intelligence. The questions asked by the interrogator are emotionally charged; the replicant’s response is measured by observing breathing, heart rate and pupillary response.</p> <p>Syd Mead was a renowned concept artist whose futuristic visions gave shape to a wide range of films including <em>Blade Runner</em> (1982), <em>Tron</em> (1982), <em>Aliens</em> (1986), and <em>Johnny Mnemonic</em> (1995). For these films he developed the look of entire cities, crowded streetscapes, and elaborate electronic vehicles and equipment that gave a gritty realism to even the most fantastical ideas.</p> <p>Mead’s concept drawing for the Voight-Kampff machine is at once futuristic and anachronistic. It is an intricate automated device with 19th century embellishments such as mechanical bellows and a stylized monocle. In the hands of Rick Deckard, the interrogator who administers the Voight-Kampff test, the machine is a menacing beast that appears to breathe and move of its own accord.</p> </section> </details>\n<br>\n<div class=\"backmatter\">\n</div><section class=\"footnotes\">\n<ol class=\"footnotes-list\">\n<li id=\"fn1\" class=\"footnote-item\"><p>The Turing test, originally called ‘the imitation game’ by Alan Turing in 1950, is a test of a machine’s ability to exhibit intelligent behavior equivalent to, or indistinguishable from, that of a human. <a href=\"#fnref1\" class=\"footnote-backref\">↩︎</a></p>\n</li>\n</ol>\n</section>\n","length":2234,"subtitle":null,"title":"Alan Turing -- The Imitation Game","type":"entry","url":"http://localhost:8080/20-objects/02-imitation-game/"},{"content":"<p><em>2001: A Space Odyssey</em> (1968) is a landmark in the representation of artificial intelligence in visual culture. From the earliest moments of the film, we are cast about in space and time in ways that distort and upend our perceptions of reality. At the centre of the film is the HAL 9000 computer. HAL is the acronym for a Heuristically programmed Algorithmic computer—which is to say, a computer optimized for problem solving, self-discovery and decision-making, though inherently vulnerable to error. It is adept at independently controlling complex machines, interacting with humans in ways that would certainly pass the Turing Test, and using computer vision to track a wide range of activities, including the reading of lips.</p>\n<p>Left to its own devices, HAL suspects that the humans are sabotaging the planned mission to Jupiter, and it begins to restrict their control, finally deciding to kill them in order to save the mission. But Bowman narrowly escapes, and makes his way to HAL’s computer processing core where he methodically disconnects the various circuits that produce HAL’s consciousness. HAL maintains a calm and controlled voice trying to assure Bowman that it can change its plan. But as Bowman slowly deactivates HAL’s memory, it begs Bowman to stop, expressing its fear, its feeling of loss—then finally devolving to its earliest operational state.</p>\n<p>At this point in the film, viewers are invited to consider our limited assumptions about the nature of sentience, and, specifically, its formation in machines.</p>\n<p>The film’s director, Stanley Kubrick, and his co-writer for the screenplay, Arthur C. Clarke, conceived of an artificial intelligence that had achieved a consciousness that closely resembled that of humans—recognizing within this a very real likelihood of fallibility.</p>\n<p>The question of fallibility and error(s) in judgement produced by the actions of an artificial intelligence is a persistent theme, from the 1950s onward, in literature, art and, most notably, film.</p>\n<section id=\"section-samuel-butler\" class=\"accordion-section\" data-outputs-include=\"epub,pdf\"> <h2>Samuel Butler</h2> <p>Samuel Butler was a British novelist and cultural critic. His novel <em>Erewhon: or, Over the Range</em> was published anonymously in 1872 and takes the form of a utopian narrative of place. Three chapters in the book are identified as “The Book of Machines,” and they describe a society with a conflicted relationship to machines. Citing Charles Darwin’s theories on evolution, Butler represents a world with machines that have evolved consciousness and threaten to transform in ways that cannot be controlled by humans:</p> <blockquote> <p><em>I would repeat that I fear none of the existing machines; what I fear is the extraordinary rapidity with which they are becoming something very different to what they are at present. No class of beings have in any time past made so rapid a movement forward. Should not that movement be jealously watched, and checked while we can still check it? And is it not necessary for this end to destroy the more advanced of the machines which are in use at present, though it is admitted that they are in themselves harmless?</em></p> </blockquote> </section> <details class=\"accordion-section\" id=\"section-samuel-butler\"  data-outputs-include=\"html\"> <summary class=\"accordion-section__heading accordion-section__heading-level-null accordion-section__controls accordion-section__controls--arrow\" tabindex=\"1\"> <button aria-label=\"Copy page section link to clipboard\" class=\"accordion-section__copy-link-button\" data-outputs-exclude=\"pdf,epub\" value=\"#section-samuel-butler\" tabindex=\"2\" > § </button> <span aria-hidden=\"true\" class=\"accordion-tooltip\" data-outputs-exclude=\"pdf,epub\" > Copied page section link to clipboard </span> <h2>Samuel Butler</h2> </summary> <section class=\"accordion-section__body\"><p>Samuel Butler was a British novelist and cultural critic. His novel <em>Erewhon: or, Over the Range</em> was published anonymously in 1872 and takes the form of a utopian narrative of place. Three chapters in the book are identified as “The Book of Machines,” and they describe a society with a conflicted relationship to machines. Citing Charles Darwin’s theories on evolution, Butler represents a world with machines that have evolved consciousness and threaten to transform in ways that cannot be controlled by humans:</p> <blockquote> <p><em>I would repeat that I fear none of the existing machines; what I fear is the extraordinary rapidity with which they are becoming something very different to what they are at present. No class of beings have in any time past made so rapid a movement forward. Should not that movement be jealously watched, and checked while we can still check it? And is it not necessary for this end to destroy the more advanced of the machines which are in use at present, though it is admitted that they are in themselves harmless?</em></p> </blockquote> </section> </details>\n<section id=\"section-the-matrix\" class=\"accordion-section\" data-outputs-include=\"epub,pdf\"> <h2>The Matrix</h2> <p>The Wachowskis’ film, <em>The Matrix</em> (1999), set new standards in the use of computer driven special effects in film, and offered an enduringly tangible representation of a simulated world run by an artificial intelligence.</p> <p>Dreams play an integral part in our perception of consciousness. At its furthest extension, the world of dreams opens the door to the possibility that the world we experience is itself a dream. <em>The Matrix</em> takes this ancient narrative and turns it toward the world of artificial intelligence. What if our lives are nothing more than a highly effective virtual reality seamlessly rendered by a very powerful computer? Once again, the narrative of an all-controlling artificial intelligence challenges our complacency and demands vigilance over the machine.</p> <p>The special effects in <em>The Matrix</em> are legendary. By using complex virtual cinematography to choreograph the real camera movements, they were able to achieve new and compelling camera angles that give the film its otherworldliness. And give us yet another instance of the virtual world seizing control of the real world.</p> </section> <details class=\"accordion-section\" id=\"section-the-matrix\"  data-outputs-include=\"html\"> <summary class=\"accordion-section__heading accordion-section__heading-level-null accordion-section__controls accordion-section__controls--arrow\" tabindex=\"1\"> <button aria-label=\"Copy page section link to clipboard\" class=\"accordion-section__copy-link-button\" data-outputs-exclude=\"pdf,epub\" value=\"#section-the-matrix\" tabindex=\"2\" > § </button> <span aria-hidden=\"true\" class=\"accordion-tooltip\" data-outputs-exclude=\"pdf,epub\" > Copied page section link to clipboard </span> <h2>The Matrix</h2> </summary> <section class=\"accordion-section__body\"><p>The Wachowskis’ film, <em>The Matrix</em> (1999), set new standards in the use of computer driven special effects in film, and offered an enduringly tangible representation of a simulated world run by an artificial intelligence.</p> <p>Dreams play an integral part in our perception of consciousness. At its furthest extension, the world of dreams opens the door to the possibility that the world we experience is itself a dream. <em>The Matrix</em> takes this ancient narrative and turns it toward the world of artificial intelligence. What if our lives are nothing more than a highly effective virtual reality seamlessly rendered by a very powerful computer? Once again, the narrative of an all-controlling artificial intelligence challenges our complacency and demands vigilance over the machine.</p> <p>The special effects in <em>The Matrix</em> are legendary. By using complex virtual cinematography to choreograph the real camera movements, they were able to achieve new and compelling camera angles that give the film its otherworldliness. And give us yet another instance of the virtual world seizing control of the real world.</p> </section> </details>\n<section id=\"section-ex-machina\" class=\"accordion-section\" data-outputs-include=\"epub,pdf\"> <h2>Ex Machina</h2> <p>This scene from the 2014 film <em>Ex Machina</em> by writer and director Alex Garland describes an encounter between Caleb and Ava. Caleb is a young programmer who has been brought to the home of Nathan, a wealthy software developer, to test the capacity of a new robot with artificial intelligence named Ava. In an early meeting, Caleb asks Ava a series of questions that recall the parameters of a Turing Test. Ava later turns the tables on Caleb by asking a series of penetrating questions that reveal a compelling intelligence on its part. The larger narrative of the film is also introduced in this scene when Ava warns Caleb that the developer is a dangerous liar.</p> <p>The conventional narrative of a dangerous artificial intelligence is here played in reverse; Nathan, and not the robot, is the threatening and unpredictable consciousness that seeks to control the world. But in his representation of artificial intelligence, Garland seems unable to move beyond the conventions of overt sexualization and gendered stereotyping used in the depiction of robots, and, apparently, software developers.</p> </section> <details class=\"accordion-section\" id=\"section-ex-machina\"  data-outputs-include=\"html\"> <summary class=\"accordion-section__heading accordion-section__heading-level-null accordion-section__controls accordion-section__controls--arrow\" tabindex=\"1\"> <button aria-label=\"Copy page section link to clipboard\" class=\"accordion-section__copy-link-button\" data-outputs-exclude=\"pdf,epub\" value=\"#section-ex-machina\" tabindex=\"2\" > § </button> <span aria-hidden=\"true\" class=\"accordion-tooltip\" data-outputs-exclude=\"pdf,epub\" > Copied page section link to clipboard </span> <h2>Ex Machina</h2> </summary> <section class=\"accordion-section__body\"><p>This scene from the 2014 film <em>Ex Machina</em> by writer and director Alex Garland describes an encounter between Caleb and Ava. Caleb is a young programmer who has been brought to the home of Nathan, a wealthy software developer, to test the capacity of a new robot with artificial intelligence named Ava. In an early meeting, Caleb asks Ava a series of questions that recall the parameters of a Turing Test. Ava later turns the tables on Caleb by asking a series of penetrating questions that reveal a compelling intelligence on its part. The larger narrative of the film is also introduced in this scene when Ava warns Caleb that the developer is a dangerous liar.</p> <p>The conventional narrative of a dangerous artificial intelligence is here played in reverse; Nathan, and not the robot, is the threatening and unpredictable consciousness that seeks to control the world. But in his representation of artificial intelligence, Garland seems unable to move beyond the conventions of overt sexualization and gendered stereotyping used in the depiction of robots, and, apparently, software developers.</p> </section> </details>\n<section id=\"section-ben-bogart\" class=\"accordion-section\" data-outputs-include=\"epub,pdf\"> <h2>Ben Bogart</h2> <p>Ben Bogart’s video installation <em>Watching (2001: A Space Odyssey)</em> offers an intricate interaction with Stanley Kubrick’s film <em>2001: A Space Odyssey</em>. This interaction takes the form of an active watching in which the image and sound of the original film are disassembled and then reconstituted using complex algorithms embedded in statistically oriented machine learning<sup class=\"footnote-ref\"><a href=\"#fn1\" id=\"fnref1\">1</a></sup> and computer vision<sup class=\"footnote-ref\"><a href=\"#fn2\" id=\"fnref2\">2</a></sup> programs.</p> <p>The reconstituted film is strangely familiar but the space and time that it describes is ordered in a new and compelling way. Those who have seen Kubrick’s version may struggle to reconcile the old with the new, but the reward comes in surrendering one’s preconceptions, and watching closely the pattern, movement and sounds rendered by Bogart’s artificial intelligence.</p> <p>In this unexpected way, we are offered the surprising opportunity to witness an artificial intelligence “watching” a film, which is itself a study of artificial intelligence, consciousness and perception, across space and time.</p> <div class=\"backmatter\"> </div> <section class=\"footnotes\"> <ol class=\"footnotes-list\"> <li id=\"fn1\" class=\"footnote-item\"><p>Machine learning is an application of AI that enables systems to learn and improve from experience without being explicitly programmed. <a href=\"#fnref1\" class=\"footnote-backref\">↩︎</a></p> </li> <li id=\"fn2\" class=\"footnote-item\"><p>Computer vision is a field of artificial intelligence that enables computers and systems to derive meaningful information from digital images, videos and other visual inputs. <a href=\"#fnref2\" class=\"footnote-backref\">↩︎</a></p> </li> </ol> </section> </section> <details class=\"accordion-section\" id=\"section-ben-bogart\"  data-outputs-include=\"html\"> <summary class=\"accordion-section__heading accordion-section__heading-level-null accordion-section__controls accordion-section__controls--arrow\" tabindex=\"1\"> <button aria-label=\"Copy page section link to clipboard\" class=\"accordion-section__copy-link-button\" data-outputs-exclude=\"pdf,epub\" value=\"#section-ben-bogart\" tabindex=\"2\" > § </button> <span aria-hidden=\"true\" class=\"accordion-tooltip\" data-outputs-exclude=\"pdf,epub\" > Copied page section link to clipboard </span> <h2>Ben Bogart</h2> </summary> <section class=\"accordion-section__body\"><p>Ben Bogart’s video installation <em>Watching (2001: A Space Odyssey)</em> offers an intricate interaction with Stanley Kubrick’s film <em>2001: A Space Odyssey</em>. This interaction takes the form of an active watching in which the image and sound of the original film are disassembled and then reconstituted using complex algorithms embedded in statistically oriented machine learning<sup class=\"footnote-ref\"><a href=\"#fn1\" id=\"fnref1\">1</a></sup> and computer vision<sup class=\"footnote-ref\"><a href=\"#fn2\" id=\"fnref2\">2</a></sup> programs.</p> <p>The reconstituted film is strangely familiar but the space and time that it describes is ordered in a new and compelling way. Those who have seen Kubrick’s version may struggle to reconcile the old with the new, but the reward comes in surrendering one’s preconceptions, and watching closely the pattern, movement and sounds rendered by Bogart’s artificial intelligence.</p> <p>In this unexpected way, we are offered the surprising opportunity to witness an artificial intelligence “watching” a film, which is itself a study of artificial intelligence, consciousness and perception, across space and time.</p> <div class=\"backmatter\"> </div> <section class=\"footnotes\"> <ol class=\"footnotes-list\"> <li id=\"fn1\" class=\"footnote-item\"><p>Machine learning is an application of AI that enables systems to learn and improve from experience without being explicitly programmed. <a href=\"#fnref1\" class=\"footnote-backref\">↩︎</a></p> </li> <li id=\"fn2\" class=\"footnote-item\"><p>Computer vision is a field of artificial intelligence that enables computers and systems to derive meaningful information from digital images, videos and other visual inputs. <a href=\"#fnref2\" class=\"footnote-backref\">↩︎</a></p> </li> </ol> </section> </section> </details>","length":1998,"subtitle":null,"title":"2001: A Space Odyssey","type":"entry","url":"http://localhost:8080/20-objects/03-space-odyssey/"},{"content":"<p><em>Cybernetic Serendipity: The Computer and the Arts</em> was a landmark exhibition mounted at the Institute of Contemporary Arts (ICA), London, in 1968, and organized by Jasia Reichardt, curator and associate director of the ICA.</p>\n<p>Drawing on the term “cybernetics” that had been popularized by Norbert Wiener twenty years earlier, the exhibition explored the potential for advanced technologies to enable new modes of creativity. This was a new age of interdisciplinarity, and the exhibition highlighted collaborations between more than 300 artists, composers, performers, scientists and engineers. The concept of “serendipity” was a theme taken up in many of the individual works, and notions of play, chance and surprise were common threads throughout.</p>\n<p>The exhibition was divided into three sections:</p>\n<ul>\n<li>\n<p>Computer-generated graphics, computer-animated films, computer-composed and played music, and computer poems and texts;</p>\n</li>\n<li>\n<p>Cybernetic devices as works of art, cybernetic environments, remote control robots and painting machines;</p>\n</li>\n<li>\n<p>A “learning zone” with machines demonstrating the uses of computers, and an environment dealing with the history of cybernetics.</p>\n</li>\n</ul>\n<p>The extensive catalogue for the <em>Cybernetic Serendipity</em> exhibition was first published in 1968 as a special issue of <em>Studio International</em>, an influential art magazine based in London. Under the editorial leadership of Peter Townsend, <em>Studio International</em> endorsed interdisciplinary practices, new technologies and an international community of cultural producers.</p>\n<p>With more than 100 pages and 300 images—and featuring contributions by dozens of experts in various fields—the publication was both a primer on the history of the computer, and an avant-garde statement on the uses of new technologies in art, music, poetry, dance, graphics, architecture, installation, and environmental art and film.</p>\n<p>The cover of the catalogue and exhibition poster, which incorporated computer graphics from the exhibition, was designed by the Polish-British painter, filmmaker and stage designer Franciszka Themerson.</p>\n<p>Much of the original exhibition was subsequently shown at the Corcoran Annex in Washington, D.C., and then at the newly opened Exploratorium in San Francisco in 1969.</p>\n<section id=\"section-john-h-whitney-permutations\" class=\"accordion-section\" data-outputs-include=\"epub,pdf\"> <h2>John H. Whitney, Permutations</h2> <p>John H. Whitney was an animator, filmmaker and key figure in the early development of computer graphics. His collaboration on the animated title sequence for Alfred Hitchcock’s <em>Vertigo</em> in 1958 was widely acknowledged for its innovation and dynamic imagery—including the spirographic images (based on the mathematical forms known as Lissajous curves<sup class=\"footnote-ref\"><a href=\"#fn1\" id=\"fnref1\">1</a></sup>) that also feature prominently in <em>Permutations</em>.</p> <p>Whitney saw the computer as a fundamental tool to link music and visual art. Through the use of sound and computer graphics, he created harmonic events in audio-visual presentations. He describes the process as follows:</p> <blockquote> <p><em>In</em> Permutations <em>each point moves at a different speed and moves in a direction independent according to natural laws quite as valid as those of Pythagoras, while moving in their circular field. Their action produces a phenomenon more or less equivalent to the musical harmonies. When the points reach certain relationships (harmonic) numerical to other parameters of the equation, they form elementary figures.</em></p> </blockquote> <div class=\"backmatter\"> </div> <section class=\"footnotes\"> <ol class=\"footnotes-list\"> <li id=\"fn1\" class=\"footnote-item\"><p>A Lissajous curve is the graph of a system of parametric equations which describe complex harmonic motion. <a href=\"#fnref1\" class=\"footnote-backref\">↩︎</a></p> </li> </ol> </section> </section> <details class=\"accordion-section\" id=\"section-john-h-whitney-permutations\"  data-outputs-include=\"html\"> <summary class=\"accordion-section__heading accordion-section__heading-level-null accordion-section__controls accordion-section__controls--arrow\" tabindex=\"1\"> <button aria-label=\"Copy page section link to clipboard\" class=\"accordion-section__copy-link-button\" data-outputs-exclude=\"pdf,epub\" value=\"#section-john-h-whitney-permutations\" tabindex=\"2\" > § </button> <span aria-hidden=\"true\" class=\"accordion-tooltip\" data-outputs-exclude=\"pdf,epub\" > Copied page section link to clipboard </span> <h2>John H. Whitney, Permutations</h2> </summary> <section class=\"accordion-section__body\"><p>John H. Whitney was an animator, filmmaker and key figure in the early development of computer graphics. His collaboration on the animated title sequence for Alfred Hitchcock’s <em>Vertigo</em> in 1958 was widely acknowledged for its innovation and dynamic imagery—including the spirographic images (based on the mathematical forms known as Lissajous curves<sup class=\"footnote-ref\"><a href=\"#fn1\" id=\"fnref1\">1</a></sup>) that also feature prominently in <em>Permutations</em>.</p> <p>Whitney saw the computer as a fundamental tool to link music and visual art. Through the use of sound and computer graphics, he created harmonic events in audio-visual presentations. He describes the process as follows:</p> <blockquote> <p><em>In</em> Permutations <em>each point moves at a different speed and moves in a direction independent according to natural laws quite as valid as those of Pythagoras, while moving in their circular field. Their action produces a phenomenon more or less equivalent to the musical harmonies. When the points reach certain relationships (harmonic) numerical to other parameters of the equation, they form elementary figures.</em></p> </blockquote> <div class=\"backmatter\"> </div> <section class=\"footnotes\"> <ol class=\"footnotes-list\"> <li id=\"fn1\" class=\"footnote-item\"><p>A Lissajous curve is the graph of a system of parametric equations which describe complex harmonic motion. <a href=\"#fnref1\" class=\"footnote-backref\">↩︎</a></p> </li> </ol> </section> </section> </details>\n<section id=\"section-tony-pritchett-the-flexipede\" class=\"accordion-section\" data-outputs-include=\"epub,pdf\"> <h2>Tony Pritchett, The Flexipede</h2> <p><em>The Flexipede</em> had its premiere in <em>Cybernetic Serendipity</em>, and was reputed to be the first computer-generated animated film made in the UK. Tony Pritchett produced it on the University of London’s Atlas Computer (one of the world’s first supercomputers) using FORTRAN IV—a programming language that was employed in computationally intensive research, such as numerical weather prediction, finite element analysis, computational fluid dynamics, and so on. FORTRAN IV was widely used by artists and programmers in the <em>Cybernetic Serendipity</em> exhibition.</p> <p>Pritchett wrote the subroutines (program instructions) using punch cards, then transferred the output to tape using a FORTRAN-based graphics software package called GHOST. The tapes were used to produce an output on a plotter printer and then filmed by a microfilm recorder.</p> </section> <details class=\"accordion-section\" id=\"section-tony-pritchett-the-flexipede\"  data-outputs-include=\"html\"> <summary class=\"accordion-section__heading accordion-section__heading-level-null accordion-section__controls accordion-section__controls--arrow\" tabindex=\"1\"> <button aria-label=\"Copy page section link to clipboard\" class=\"accordion-section__copy-link-button\" data-outputs-exclude=\"pdf,epub\" value=\"#section-tony-pritchett-the-flexipede\" tabindex=\"2\" > § </button> <span aria-hidden=\"true\" class=\"accordion-tooltip\" data-outputs-exclude=\"pdf,epub\" > Copied page section link to clipboard </span> <h2>Tony Pritchett, The Flexipede</h2> </summary> <section class=\"accordion-section__body\"><p><em>The Flexipede</em> had its premiere in <em>Cybernetic Serendipity</em>, and was reputed to be the first computer-generated animated film made in the UK. Tony Pritchett produced it on the University of London’s Atlas Computer (one of the world’s first supercomputers) using FORTRAN IV—a programming language that was employed in computationally intensive research, such as numerical weather prediction, finite element analysis, computational fluid dynamics, and so on. FORTRAN IV was widely used by artists and programmers in the <em>Cybernetic Serendipity</em> exhibition.</p> <p>Pritchett wrote the subroutines (program instructions) using punch cards, then transferred the output to tape using a FORTRAN-based graphics software package called GHOST. The tapes were used to produce an output on a plotter printer and then filmed by a microfilm recorder.</p> </section> </details>\n<section id=\"section-alison-knowles-the-house-of-dust\" class=\"accordion-section\" data-outputs-include=\"epub,pdf\"> <h2>Alison Knowles, The House of Dust</h2> <p>Alison Knowles is renowned for her work as an artist in a diverse range of media including performance, installation, sound works and publications. She was a founding member of Fluxus—an international, interdisciplinary group of artists, composers, designers and poets active during the 1960s. Her poem, <em>The House of Dust</em>, was conceived in collaboration with James Tenney, a composer-in-residence at Bell Labs in New Jersey, and an expert on the IBM compiling system known as FORTRAN.</p> <p>Knowles’ interest in chance and indeterminacy coincided with Tenney’s interest in the manipulation and generation of information, and their collaboration was first shown in <em>Cybernetic Serendipity</em>. It took the form of computer-generated poems compiled in four randomly generated lines—indicating a type of house; a material, a site or situation; a light source; and a category of inhabitants.</p> <p>The resulting poems are surprisingly evocative, and full of humour and absurdity. For Knowles, they also offered an opportunity for a compelling meditation on dwelling and architecture. In 1968, she designed and built <em>The House of Dust</em> structure, later installing it at CalArts in Valencia, California, when she moved there to teach in 1969.</p> </section> <details class=\"accordion-section\" id=\"section-alison-knowles-the-house-of-dust\"  data-outputs-include=\"html\"> <summary class=\"accordion-section__heading accordion-section__heading-level-null accordion-section__controls accordion-section__controls--arrow\" tabindex=\"1\"> <button aria-label=\"Copy page section link to clipboard\" class=\"accordion-section__copy-link-button\" data-outputs-exclude=\"pdf,epub\" value=\"#section-alison-knowles-the-house-of-dust\" tabindex=\"2\" > § </button> <span aria-hidden=\"true\" class=\"accordion-tooltip\" data-outputs-exclude=\"pdf,epub\" > Copied page section link to clipboard </span> <h2>Alison Knowles, The House of Dust</h2> </summary> <section class=\"accordion-section__body\"><p>Alison Knowles is renowned for her work as an artist in a diverse range of media including performance, installation, sound works and publications. She was a founding member of Fluxus—an international, interdisciplinary group of artists, composers, designers and poets active during the 1960s. Her poem, <em>The House of Dust</em>, was conceived in collaboration with James Tenney, a composer-in-residence at Bell Labs in New Jersey, and an expert on the IBM compiling system known as FORTRAN.</p> <p>Knowles’ interest in chance and indeterminacy coincided with Tenney’s interest in the manipulation and generation of information, and their collaboration was first shown in <em>Cybernetic Serendipity</em>. It took the form of computer-generated poems compiled in four randomly generated lines—indicating a type of house; a material, a site or situation; a light source; and a category of inhabitants.</p> <p>The resulting poems are surprisingly evocative, and full of humour and absurdity. For Knowles, they also offered an opportunity for a compelling meditation on dwelling and architecture. In 1968, she designed and built <em>The House of Dust</em> structure, later installing it at CalArts in Valencia, California, when she moved there to teach in 1969.</p> </section> </details>","length":1469,"subtitle":null,"title":"Cybernetic Serendipity","type":"entry","url":"http://localhost:8080/20-objects/04-cybernetic-serendipity/"},{"content":"<p>Using some of the basic principles of cybernetics theory—communication and control—Stafford Beer, a consultant to the newly-elected Salvador Allende government (1970–73), devised a plan to enable a dynamic, socialist-driven transformation of the Chilean economy.</p>\n<p>Building on Jay Forrester’s theories of system dynamics<sup class=\"footnote-ref\"><a href=\"#fn1\" id=\"fnref1\">1</a></sup>, and specifically his book <em>Industrial Dynamics</em> (1961), Beer laid out a strategy to capture information feedback from workers, managers, suppliers, shippers and consumers across the country. This data was used to produce a list of relevant indicators that could be monitored and, in turn, used to make adjustments in planning or to redirect goods and supplies.</p>\n<p>Project Cybersyn was composed of four key elements: a national network of teletype machines that would capture information in real time; an IBM System/360 Model 50 computer (1965) running a purpose-built version of Dynamo software for simulating system dynamics models; an Operations Room where ongoing analysis of the received data would provide key indicators; and the resulting indicators used to provide feedback to the producers, shippers and consumers, and to make necessary adjustments.</p>\n<p>After two years of development, Project Cybersyn launched in February 1973. Beer struggled to maintain the project’s goal for greater worker autonomy within a system that was consultative at all levels. These concerns, however, were overshadowed by the Allende government’s struggle to maintain its autonomy. In September 1973, Allende was murdered in a CIA-backed military coup led by General Pinochet, and the Cybersyn Operations Room was destroyed.</p>\n<section id=\"section-the-operations-room\" class=\"accordion-section\" data-outputs-include=\"epub,pdf\"> <h2>The Operations Room</h2> <p>The Operations Room was a key element of Project Cybersyn. Symbolically, its design by Gui Bonsiepe reflected the forward thinking modernism that informed the project as a whole. The Operations Room was a nexus for the information that flowed in and out of the teletypes and mainframe computer. The design team was led by Bonsiepe—a German-born designer trained at the famed Ulm School of Design—who was working as a design consultant in South America during the 1970s. When the Room was up and running in early 1973, a small team of designers were also employed to design the data projected on the screens in real time.</p> <p>Reflecting Bonsiepe’s training at Ulm, the Room was designed with a strict adherence to the principles of gestalt design—closure, proximity, similarity, continuity, perception, organization and symmetry. The seven swivel chairs allow the whole Room to be seen in a glance, and support intimate, egalitarian conversation.</p> <p>The annotated images used in the video slideshow are a selection of drawings, diagrams and photographic documentation that offer some insights into the planning and design of Project Cybersyn.</p> </section> <details class=\"accordion-section\" id=\"section-the-operations-room\"  data-outputs-include=\"html\"> <summary class=\"accordion-section__heading accordion-section__heading-level-null accordion-section__controls accordion-section__controls--arrow\" tabindex=\"1\"> <button aria-label=\"Copy page section link to clipboard\" class=\"accordion-section__copy-link-button\" data-outputs-exclude=\"pdf,epub\" value=\"#section-the-operations-room\" tabindex=\"2\" > § </button> <span aria-hidden=\"true\" class=\"accordion-tooltip\" data-outputs-exclude=\"pdf,epub\" > Copied page section link to clipboard </span> <h2>The Operations Room</h2> </summary> <section class=\"accordion-section__body\"><p>The Operations Room was a key element of Project Cybersyn. Symbolically, its design by Gui Bonsiepe reflected the forward thinking modernism that informed the project as a whole. The Operations Room was a nexus for the information that flowed in and out of the teletypes and mainframe computer. The design team was led by Bonsiepe—a German-born designer trained at the famed Ulm School of Design—who was working as a design consultant in South America during the 1970s. When the Room was up and running in early 1973, a small team of designers were also employed to design the data projected on the screens in real time.</p> <p>Reflecting Bonsiepe’s training at Ulm, the Room was designed with a strict adherence to the principles of gestalt design—closure, proximity, similarity, continuity, perception, organization and symmetry. The seven swivel chairs allow the whole Room to be seen in a glance, and support intimate, egalitarian conversation.</p> <p>The annotated images used in the video slideshow are a selection of drawings, diagrams and photographic documentation that offer some insights into the planning and design of Project Cybersyn.</p> </section> </details>\n<section id=\"section-designing-freedom\" class=\"accordion-section\" data-outputs-include=\"epub,pdf\"> <h2>Designing Freedom</h2> <p>In this excerpt from Stafford Beer’s Massey Lectures for CBC Radio (1973), Beer concludes the six-part series with a plea to recognize the fundamental role of new technologies in designing new models of freedom; and the threat to that freedom if those technologies are mishandled. This conclusion to the lecture series was hastily revised to acknowledge the ongoing coup in Chile. Using the language of cybernetics theory, Beer cites the actions of the Pinochet junta as “the output of a system designed to curb liberty, my message is that we must redesign that system to produce freedom as an output.”</p> </section> <details class=\"accordion-section\" id=\"section-designing-freedom\"  data-outputs-include=\"html\"> <summary class=\"accordion-section__heading accordion-section__heading-level-null accordion-section__controls accordion-section__controls--arrow\" tabindex=\"1\"> <button aria-label=\"Copy page section link to clipboard\" class=\"accordion-section__copy-link-button\" data-outputs-exclude=\"pdf,epub\" value=\"#section-designing-freedom\" tabindex=\"2\" > § </button> <span aria-hidden=\"true\" class=\"accordion-tooltip\" data-outputs-exclude=\"pdf,epub\" > Copied page section link to clipboard </span> <h2>Designing Freedom</h2> </summary> <section class=\"accordion-section__body\"><p>In this excerpt from Stafford Beer’s Massey Lectures for CBC Radio (1973), Beer concludes the six-part series with a plea to recognize the fundamental role of new technologies in designing new models of freedom; and the threat to that freedom if those technologies are mishandled. This conclusion to the lecture series was hastily revised to acknowledge the ongoing coup in Chile. Using the language of cybernetics theory, Beer cites the actions of the Pinochet junta as “the output of a system designed to curb liberty, my message is that we must redesign that system to produce freedom as an output.”</p> </section> </details>\n<section id=\"section-simulacron-3-and-the-tunnel-under-the-world\" class=\"accordion-section\" data-outputs-include=\"epub,pdf\"> <h2>Simulacron-3 and The Tunnel under the World</h2> <p>Daniel F. Galouye’s <em>Simulacron-3</em> tells the story of a computer-generated city simulation that was created to fill the needs of a marketing research company. The simulation is so realistic that the city’s inhabitants don’t realize they are living in a virtual world.</p> <p>Frederick Pohl’s short story, “The Tunnel under the World,” is an early example of a simulated world built by a corporate entity that uses the town’s inhabitants—recently annihilated in an industrial accident and<br> recreated as minuscule robots—to test hard sell marketing strategies.</p> <p>These stories are part of a science fiction subgenre, which emerged in the 1950s and 60s and addressed a growing and unregulated world of corporate marketing and research based in computer intensive systems analysis. This marketing research was rooted in an analysis of the values, choices and desires of increasingly large segments of population. Data gathered through polls, interviews and consumer sales was used to build massive simulations that would confidently predict the future.</p> </section> <details class=\"accordion-section\" id=\"section-simulacron-3-and-the-tunnel-under-the-world\"  data-outputs-include=\"html\"> <summary class=\"accordion-section__heading accordion-section__heading-level-null accordion-section__controls accordion-section__controls--arrow\" tabindex=\"1\"> <button aria-label=\"Copy page section link to clipboard\" class=\"accordion-section__copy-link-button\" data-outputs-exclude=\"pdf,epub\" value=\"#section-simulacron-3-and-the-tunnel-under-the-world\" tabindex=\"2\" > § </button> <span aria-hidden=\"true\" class=\"accordion-tooltip\" data-outputs-exclude=\"pdf,epub\" > Copied page section link to clipboard </span> <h2>Simulacron-3 and The Tunnel under the World</h2> </summary> <section class=\"accordion-section__body\"><p>Daniel F. Galouye’s <em>Simulacron-3</em> tells the story of a computer-generated city simulation that was created to fill the needs of a marketing research company. The simulation is so realistic that the city’s inhabitants don’t realize they are living in a virtual world.</p> <p>Frederick Pohl’s short story, “The Tunnel under the World,” is an early example of a simulated world built by a corporate entity that uses the town’s inhabitants—recently annihilated in an industrial accident and<br> recreated as minuscule robots—to test hard sell marketing strategies.</p> <p>These stories are part of a science fiction subgenre, which emerged in the 1950s and 60s and addressed a growing and unregulated world of corporate marketing and research based in computer intensive systems analysis. This marketing research was rooted in an analysis of the values, choices and desires of increasingly large segments of population. Data gathered through polls, interviews and consumer sales was used to build massive simulations that would confidently predict the future.</p> </section> </details>\n<section id=\"section-world-on-a-wire\" class=\"accordion-section\" data-outputs-include=\"epub,pdf\"> <h2>World on a Wire</h2> <p><em>World on a Wire</em> was a two-part television miniseries directed by Rainer Werner Fassbinder and made for German television. Inspired by Daniel F. Galouye’s novel <em>Simulacron-3</em> (1964), Fassbinder’s film depicts a near-future world in which the Cybernetics and Future Science Institute runs a massive virtual world on its supercomputer named Simulacron. The world contains thousands of “identity units” who exist and go about their everyday lives oblivious of the fact that they are simulations.</p> <p>In the video excerpt shown here, the newly appointed director of the Institute, Dr. Fred Stiller, enters the simulation to investigate a sudden death (the Institute’s previous director) and a rogue unit named Einstein. Einstein has become self-aware and realizes that he is a simulation. He begs Stiller to take him to the real world.</p> <p>The notion of a hidden, simulated world is a consistent trope in the narratives of artificial intelligence. <em>World on a Wire</em> tells the story of a scientific institute that uses a simulation program to predict the future of steel prices for a giant corporation. And its source material, Galouye’s <em>Simulacron-3,</em> pointed to the world of marketing and its use of simulations to predict trends. Both drew on a growing public awareness of the power of computing and the manipulation of public opinion in real life through entities such as the Simulmatics Corporation, which played a critical role in American politics and the Vietnam War during the 1960s.</p> </section> <details class=\"accordion-section\" id=\"section-world-on-a-wire\"  data-outputs-include=\"html\"> <summary class=\"accordion-section__heading accordion-section__heading-level-null accordion-section__controls accordion-section__controls--arrow\" tabindex=\"1\"> <button aria-label=\"Copy page section link to clipboard\" class=\"accordion-section__copy-link-button\" data-outputs-exclude=\"pdf,epub\" value=\"#section-world-on-a-wire\" tabindex=\"2\" > § </button> <span aria-hidden=\"true\" class=\"accordion-tooltip\" data-outputs-exclude=\"pdf,epub\" > Copied page section link to clipboard </span> <h2>World on a Wire</h2> </summary> <section class=\"accordion-section__body\"><p><em>World on a Wire</em> was a two-part television miniseries directed by Rainer Werner Fassbinder and made for German television. Inspired by Daniel F. Galouye’s novel <em>Simulacron-3</em> (1964), Fassbinder’s film depicts a near-future world in which the Cybernetics and Future Science Institute runs a massive virtual world on its supercomputer named Simulacron. The world contains thousands of “identity units” who exist and go about their everyday lives oblivious of the fact that they are simulations.</p> <p>In the video excerpt shown here, the newly appointed director of the Institute, Dr. Fred Stiller, enters the simulation to investigate a sudden death (the Institute’s previous director) and a rogue unit named Einstein. Einstein has become self-aware and realizes that he is a simulation. He begs Stiller to take him to the real world.</p> <p>The notion of a hidden, simulated world is a consistent trope in the narratives of artificial intelligence. <em>World on a Wire</em> tells the story of a scientific institute that uses a simulation program to predict the future of steel prices for a giant corporation. And its source material, Galouye’s <em>Simulacron-3,</em> pointed to the world of marketing and its use of simulations to predict trends. Both drew on a growing public awareness of the power of computing and the manipulation of public opinion in real life through entities such as the Simulmatics Corporation, which played a critical role in American politics and the Vietnam War during the 1960s.</p> </section> </details>\n<br>\n<div class=\"backmatter\">\n</div>\n<section class=\"footnotes\">\n<ol class=\"footnotes-list\">\n<li id=\"fn1\" class=\"footnote-item\"><p>System dynamics (SD) is an approach to understanding the nonlinear behaviour of complex systems over time using stocks, flows, internal feedback loops, table functions and time delays. <a href=\"#fnref1\" class=\"footnote-backref\">↩︎</a></p>\n</li>\n</ol>\n</section>\n","length":1832,"subtitle":null,"title":"Project Cybersyn","type":"entry","url":"http://localhost:8080/20-objects/05-project-cybersyn/"},{"content":"<p>Different than a typical computer game, <em>SimCity</em> is a city simulator—each player (or “SimCity Mayor”) designs and manages their own city, placing buildings, roads, railroads and power lines; setting civic spending priorities; and making key policy decisions such as tax rates, zoning, and the construction of prisons or military facilities. For many players, the potential disasters—earthquakes, tornadoes, fires and even giant monsters—were the most fun part of the gaming experience. <em>SimCity</em> has been credited with inspiring a generation of urban planners and government officials.</p>\n<p>Urban planning and governance are highly complex and fraught with unforeseen interactions and outcomes. Consequently, this area was an early topic of interest for the application of computer simulations and artificial intelligence, notably in the early research and publications of Jay Forrester. More recently, focus has shifted from simulating possible outcomes to using AI programs to monitor and control various systems in real-life cities. Google Sidewalk Labs’ Sidewalk Toronto project offers a striking representation of the issues raised by this recent development in urban design.</p>\n<p>The original 1989 edition of <em>SimCity</em> introduced the basic components that would evolve throughout the 25-year lifetime of the game. Through simple, colourful graphics, <em>SimCity</em> put the power of computer simulation into the hands of millions.</p>\n<p>These video clips present a sample of the features in the original 1989 edition. On a simple map, players can design and manage a city; watch it grow (sometimes in unexpected ways, similar to the pioneering urban simulations of Jay Forrester); and respond to a rich and entertaining set of disasters. <em>SimCity</em>, in contrast to most computer games of the era, was open-ended, noncompetitive and impossible to win or lose, leading the publisher to market it as a “software toy.”</p>\n<p><em>SimCity</em>’s creator, Will Wright, went on to create <em>The Sims</em> and <em>Spore</em>, and pioneered an entire genre of simulator-based games.</p>\n<section id=\"section-jay-forrester\" class=\"accordion-section\" data-outputs-include=\"epub,pdf\"> <h2>Jay Forrester</h2> <p>Jay Forrester pioneered the field of system dynamics—the use of computer simulations to model complex systems and interactions. One of the areas that he and his colleagues at MIT tackled was urban dynamics—the behaviour and sometimes surprising interactions of the various policies and processes that shape a city. Forrester’s dedication to clear mental models and rigorous computer simulations produced an early and influential real-world use of computer intelligence.</p> <p>After publishing <em>Urban Dynamics</em> in 1969, he expanded his work to a global scale, tackling issues of world economy, population and the environment in <em>World Dynamics</em>, published in 1971. Here, he notably predicts the collapse of our socio-technological-natural system by the mid-21st century. In addition to influencing a generation of urban planners and computer scientists, Forrester has been cited by game designer Will Wright as one of the main inspirations behind the urban simulation video game, <em>SimCity</em>.</p> </section> <details class=\"accordion-section\" id=\"section-jay-forrester\"  data-outputs-include=\"html\"> <summary class=\"accordion-section__heading accordion-section__heading-level-null accordion-section__controls accordion-section__controls--arrow\" tabindex=\"1\"> <button aria-label=\"Copy page section link to clipboard\" class=\"accordion-section__copy-link-button\" data-outputs-exclude=\"pdf,epub\" value=\"#section-jay-forrester\" tabindex=\"2\" > § </button> <span aria-hidden=\"true\" class=\"accordion-tooltip\" data-outputs-exclude=\"pdf,epub\" > Copied page section link to clipboard </span> <h2>Jay Forrester</h2> </summary> <section class=\"accordion-section__body\"><p>Jay Forrester pioneered the field of system dynamics—the use of computer simulations to model complex systems and interactions. One of the areas that he and his colleagues at MIT tackled was urban dynamics—the behaviour and sometimes surprising interactions of the various policies and processes that shape a city. Forrester’s dedication to clear mental models and rigorous computer simulations produced an early and influential real-world use of computer intelligence.</p> <p>After publishing <em>Urban Dynamics</em> in 1969, he expanded his work to a global scale, tackling issues of world economy, population and the environment in <em>World Dynamics</em>, published in 1971. Here, he notably predicts the collapse of our socio-technological-natural system by the mid-21st century. In addition to influencing a generation of urban planners and computer scientists, Forrester has been cited by game designer Will Wright as one of the main inspirations behind the urban simulation video game, <em>SimCity</em>.</p> </section> </details>\n<section id=\"section-sidewalk-labs\" class=\"accordion-section\" data-outputs-include=\"epub,pdf\"> <h2>Sidewalk Labs</h2> <p>A “Smart City” uses technology to collect information, make decisions and manage municipal processes intelligently. In contrast to the purely “what if” simulations of Jay Forrester and the video game, <em>SimCity</em>, Smart City systems are taking on a growing role in the actual management of many facets of urban life—transportation, power, water, health services, zoning, crime detection, and, most controversially, surveillance. AI and the Internet of Things (IoT)—things not normally considered to be computers connected to the internet—combine to create a vision of a city as a linked information system whose management can be analyzed and optimized by AI algorithms. Among the many examples of Smart City initiatives, Sidewalk Toronto, which launched in 2015 (and abruptly ended in 2020), was notable for the number of commercial spin-offs it generated. The video displayed here shows glimpses of three such spin-offs:</p> <p><strong>Pebble:</strong> real time parking coordination and management</p> <p><strong>Delve:</strong> generative design and analysis of building developments</p> <p><strong>Mesa:</strong> smart optimization of energy usage within building</p> </section> <details class=\"accordion-section\" id=\"section-sidewalk-labs\"  data-outputs-include=\"html\"> <summary class=\"accordion-section__heading accordion-section__heading-level-null accordion-section__controls accordion-section__controls--arrow\" tabindex=\"1\"> <button aria-label=\"Copy page section link to clipboard\" class=\"accordion-section__copy-link-button\" data-outputs-exclude=\"pdf,epub\" value=\"#section-sidewalk-labs\" tabindex=\"2\" > § </button> <span aria-hidden=\"true\" class=\"accordion-tooltip\" data-outputs-exclude=\"pdf,epub\" > Copied page section link to clipboard </span> <h2>Sidewalk Labs</h2> </summary> <section class=\"accordion-section__body\"><p>A “Smart City” uses technology to collect information, make decisions and manage municipal processes intelligently. In contrast to the purely “what if” simulations of Jay Forrester and the video game, <em>SimCity</em>, Smart City systems are taking on a growing role in the actual management of many facets of urban life—transportation, power, water, health services, zoning, crime detection, and, most controversially, surveillance. AI and the Internet of Things (IoT)—things not normally considered to be computers connected to the internet—combine to create a vision of a city as a linked information system whose management can be analyzed and optimized by AI algorithms. Among the many examples of Smart City initiatives, Sidewalk Toronto, which launched in 2015 (and abruptly ended in 2020), was notable for the number of commercial spin-offs it generated. The video displayed here shows glimpses of three such spin-offs:</p> <p><strong>Pebble:</strong> real time parking coordination and management</p> <p><strong>Delve:</strong> generative design and analysis of building developments</p> <p><strong>Mesa:</strong> smart optimization of energy usage within building</p> </section> </details>","length":1013,"subtitle":null,"title":"SimCity","type":"entry","url":"http://localhost:8080/20-objects/06-simcity/"},{"content":"<p>In 1987, Craig Reynolds, an AI and computer graphics researcher, introduced “boids,”<sup class=\"footnote-ref\"><a href=\"#fn1\" id=\"fnref1\">1</a></sup> a program that simulates complex flocking motions—the collective motion of self-propelled entities like birds—using simple rules that would allow each “boid” to act as an independent agent.</p>\n<p>The complex flocking motion that resulted from these simple rules surprised both Reynolds and the computer graphics community. “Boids,” and the creation of emergent complex motion through the application of relatively simple rules to large numbers of autonomous agents, became an influential concept that was adopted and extended by many researchers, software developers and special effects artists.</p>\n<p>Are “boids” really artificial intelligence, or simply clever algorithmic<sup class=\"footnote-ref\"><a href=\"#fn2\" id=\"fnref2\">2</a></sup> constructions that produce the illusion of complex behaviour without any underlying understanding? It is a subject of some debate, yet the same question can be asked of any AI system. Recalling Alan Turing’s “Imitation Game,” it may be sufficient to say that animating with autonomous agents creates the illusion of life on a grand scale—one that has fooled even expert human observers.</p>\n<p>The three rules (or steering behaviours) for “boids” were simple:</p>\n<ul>\n<li>\n<p><strong>separation:</strong> steer to avoid crowding local flockmates</p>\n</li>\n<li>\n<p><strong>alignment:</strong> steer toward the average heading of local flockmates</p>\n</li>\n<li>\n<p><strong>cohesion:</strong> steer to move toward the average position (centre of mass) of local flockmates</p>\n</li>\n</ul>\n<p>“Boids” was first presented by Craig Reynolds in 1986 as a technical paper at SIGGRAPH, the prestigious annual computer graphics conference. In 1987, Reynolds premiered a short computer animated film, <em>Stanley &amp; Stella</em>, which featured a flock of birds and a school of fish animated using the &quot;boids&quot;algorithm. In 1998, Reynolds was honored with an Academy of Motion Pictures Scientific and Technical Award in recognition of “his pioneering contributions to the development of three-dimensional computer animation for motion picture production.”</p>\n<section id=\"section-weta\" class=\"accordion-section\" data-outputs-include=\"epub,pdf\"> <h2>WETA</h2> <p>WETA is the New Zealand-based studio responsible for the spectacular special effects for the <em>Lord of the Rings</em> films, among many others. The crowd scenes produced by WETA are among the most influential and sophisticated applications of Craig Reynolds’ “boids” algorithm. In the WETA generated scenes shown here, thousands of orcs, humans, dragons and other creatures are visible at the same time, exhibiting varied and complex action that would be impossible for human animators to produce. These extraordinary scenes offer a sophisticated balance of fantastical scale and believable behaviour, satisfying both our imaginations and our rational minds.</p> </section> <details class=\"accordion-section\" id=\"section-weta\"  data-outputs-include=\"html\"> <summary class=\"accordion-section__heading accordion-section__heading-level-null accordion-section__controls accordion-section__controls--arrow\" tabindex=\"1\"> <button aria-label=\"Copy page section link to clipboard\" class=\"accordion-section__copy-link-button\" data-outputs-exclude=\"pdf,epub\" value=\"#section-weta\" tabindex=\"2\" > § </button> <span aria-hidden=\"true\" class=\"accordion-tooltip\" data-outputs-exclude=\"pdf,epub\" > Copied page section link to clipboard </span> <h2>WETA</h2> </summary> <section class=\"accordion-section__body\"><p>WETA is the New Zealand-based studio responsible for the spectacular special effects for the <em>Lord of the Rings</em> films, among many others. The crowd scenes produced by WETA are among the most influential and sophisticated applications of Craig Reynolds’ “boids” algorithm. In the WETA generated scenes shown here, thousands of orcs, humans, dragons and other creatures are visible at the same time, exhibiting varied and complex action that would be impossible for human animators to produce. These extraordinary scenes offer a sophisticated balance of fantastical scale and believable behaviour, satisfying both our imaginations and our rational minds.</p> </section> </details>\n<section id=\"section-autonomous-vehicles\" class=\"accordion-section\" data-outputs-include=\"epub,pdf\"> <h2>Autonomous Vehicles</h2> <p>Autonomous vehicles are another example of a flock of independent agents. The underlying idea is the same, although the sophistication necessary to guide a vehicle is vastly greater than that required for “boids” or movie special effects. Computer vision, ultrasonic sensors, LiDAR (Light Detection and Ranging) and radar, GPS, and up-to-date detailed mapping all must be fast, accurate and reliable. If a “boid” or a special effects soldier collides, no one gets hurt. This is not the case with vehicles, where a single collision could prove lethal. Not surprisingly, there are spirited debates regarding the ethical and moral challenges posed by fully autonomous vehicles.</p> </section> <details class=\"accordion-section\" id=\"section-autonomous-vehicles\"  data-outputs-include=\"html\"> <summary class=\"accordion-section__heading accordion-section__heading-level-null accordion-section__controls accordion-section__controls--arrow\" tabindex=\"1\"> <button aria-label=\"Copy page section link to clipboard\" class=\"accordion-section__copy-link-button\" data-outputs-exclude=\"pdf,epub\" value=\"#section-autonomous-vehicles\" tabindex=\"2\" > § </button> <span aria-hidden=\"true\" class=\"accordion-tooltip\" data-outputs-exclude=\"pdf,epub\" > Copied page section link to clipboard </span> <h2>Autonomous Vehicles</h2> </summary> <section class=\"accordion-section__body\"><p>Autonomous vehicles are another example of a flock of independent agents. The underlying idea is the same, although the sophistication necessary to guide a vehicle is vastly greater than that required for “boids” or movie special effects. Computer vision, ultrasonic sensors, LiDAR (Light Detection and Ranging) and radar, GPS, and up-to-date detailed mapping all must be fast, accurate and reliable. If a “boid” or a special effects soldier collides, no one gets hurt. This is not the case with vehicles, where a single collision could prove lethal. Not surprisingly, there are spirited debates regarding the ethical and moral challenges posed by fully autonomous vehicles.</p> </section> </details>\n<br>\n<div class=\"backmatter\">\n</div>\n<section class=\"footnotes\">\n<ol class=\"footnotes-list\">\n<li id=\"fn1\" class=\"footnote-item\"><p>Boids is an artificial program, developed by Craig Reynolds in 1986, which simulates the flocking behaviour of bids. <a href=\"#fnref1\" class=\"footnote-backref\">↩︎</a></p>\n</li>\n<li id=\"fn2\" class=\"footnote-item\"><p>An algorithm is a set of instructions or a recipe for solving a problem or accomplishing a task. <a href=\"#fnref2\" class=\"footnote-backref\">↩︎</a></p>\n</li>\n</ol>\n</section>\n","length":831,"subtitle":null,"title":"Boids and Autonomous Agents","type":"entry","url":"http://localhost:8080/20-objects/07-boids-autonomous-agents/"},{"content":"<p>In 1973, Muriel Cooper founded the Visible Language Workshop (VLW) at MIT, where she played a pivotal role in the exploration of computer graphics and typography in modern design. The principal mission of the VLW was to develop design strategies and devices for manipulating information in dynamic contexts. In 1985, the VLW was amalgamated with MIT’s Architecture Machine Group and the Center for Advanced Visual Studies to form the MIT Media Lab, which became one of the most influential centres for the study of artificial intelligence in the world.</p>\n<p>One of Cooper’s key areas of interest was responsive design<sup class=\"footnote-ref\"><a href=\"#fn1\" id=\"fnref1\">1</a></sup> systems, incorporating feedback (in the cybernetic sense) into the design process—feedback that responded dynamically to the environment (space and context) of the design. She also focused on layering information, again using 3D space (advancing, receding, rotating) as a key design element.</p>\n<p>Today, it is hard to imagine a time before 3D graphic design, but Cooper was the leading proponent of a new field of visual design. Her work broke the flat space of conventional design and replaced it with a new interface that had depth and movement, and was responsive to input. Concepts such as “behavioural graphics,” “intelligent type,” and “on-the-fly-scaling” required complex algorithms and powerful computers to be realized, and Cooper relied on the most sophisticated graphics computers and programmers of the day to achieve her vision.</p>\n<section id=\"section-information-landscapes-at-ted5\" class=\"accordion-section\" data-outputs-include=\"epub,pdf\"> <h2>Information Landscapes at TED5</h2> <p>In 1993, Silicon Graphics—a leading producer of high performance computer graphics hardware and software—loaned the Visible Language Workshop a computer running their Reality Engine, which allowed Muriel Cooper and her team to experiment with type in motion. With a content limit of a few hundred words and a frame rate of 30 frames per second, the Reality Engine had a lower capacity than many of today’s smartphones. But at the time, it allowed for a whole new concept of dynamic text as well as a fundamental reconfiguration of the space of graphic design.</p> <p>In 1994, Cooper presented <em>Information Landscapes</em> at the TED5 Conference in Monterey, California. This video offered a radical new model of computer interface design. Information was no longer restricted to a 2D linear space, but could now be produced as a landscape to explore in 3D. And with this change came new design tools—blur, transparency, layering, infinite zoom—as well as a new concept of interaction. The reader was now replaced by a user who moved through a 3D space—navigating, browsing, digging deeper.</p> </section> <details class=\"accordion-section\" id=\"section-information-landscapes-at-ted5\"  data-outputs-include=\"html\"> <summary class=\"accordion-section__heading accordion-section__heading-level-null accordion-section__controls accordion-section__controls--arrow\" tabindex=\"1\"> <button aria-label=\"Copy page section link to clipboard\" class=\"accordion-section__copy-link-button\" data-outputs-exclude=\"pdf,epub\" value=\"#section-information-landscapes-at-ted5\" tabindex=\"2\" > § </button> <span aria-hidden=\"true\" class=\"accordion-tooltip\" data-outputs-exclude=\"pdf,epub\" > Copied page section link to clipboard </span> <h2>Information Landscapes at TED5</h2> </summary> <section class=\"accordion-section__body\"><p>In 1993, Silicon Graphics—a leading producer of high performance computer graphics hardware and software—loaned the Visible Language Workshop a computer running their Reality Engine, which allowed Muriel Cooper and her team to experiment with type in motion. With a content limit of a few hundred words and a frame rate of 30 frames per second, the Reality Engine had a lower capacity than many of today’s smartphones. But at the time, it allowed for a whole new concept of dynamic text as well as a fundamental reconfiguration of the space of graphic design.</p> <p>In 1994, Cooper presented <em>Information Landscapes</em> at the TED5 Conference in Monterey, California. This video offered a radical new model of computer interface design. Information was no longer restricted to a 2D linear space, but could now be produced as a landscape to explore in 3D. And with this change came new design tools—blur, transparency, layering, infinite zoom—as well as a new concept of interaction. The reader was now replaced by a user who moved through a 3D space—navigating, browsing, digging deeper.</p> </section> </details>\n<section id=\"section-mit-logo\" class=\"accordion-section\" data-outputs-include=\"epub,pdf\"> <h2>MIT Logo</h2> <p>Pentagram is a renowned international design firm that was founded in London in 1972 within a community of interdisciplinary producers. Among its former partners was Lisa Strausfeld, who was a student at the Visual Language Workshop (VLW) when <em>Information Landscapes</em> (1994) was produced. Strausfeld’s contribution to the video was the design of the “Financial Viewpoints” project, which offered a 3D visualization of Morningstar’s mutual fund data.</p> <p>Muriel Cooper’s influential thinking is widely acknowledged at Pentagram. In 2017—to honour the fiftieth anniversary of Cooper joining the MIT Press—Pentagram created a motion graphic salute to her foresight and immense influence on contemporary graphic design.</p> <p>The MIT Press colophon or logotype designed by Cooper is an archetype for modern logo typography. Designed thirty years before <em>Information Landscapes</em>, it bridges the gap between Bauhaus modernism and the VLW’s “computationally expressive” graphics.</p> </section> <details class=\"accordion-section\" id=\"section-mit-logo\"  data-outputs-include=\"html\"> <summary class=\"accordion-section__heading accordion-section__heading-level-null accordion-section__controls accordion-section__controls--arrow\" tabindex=\"1\"> <button aria-label=\"Copy page section link to clipboard\" class=\"accordion-section__copy-link-button\" data-outputs-exclude=\"pdf,epub\" value=\"#section-mit-logo\" tabindex=\"2\" > § </button> <span aria-hidden=\"true\" class=\"accordion-tooltip\" data-outputs-exclude=\"pdf,epub\" > Copied page section link to clipboard </span> <h2>MIT Logo</h2> </summary> <section class=\"accordion-section__body\"><p>Pentagram is a renowned international design firm that was founded in London in 1972 within a community of interdisciplinary producers. Among its former partners was Lisa Strausfeld, who was a student at the Visual Language Workshop (VLW) when <em>Information Landscapes</em> (1994) was produced. Strausfeld’s contribution to the video was the design of the “Financial Viewpoints” project, which offered a 3D visualization of Morningstar’s mutual fund data.</p> <p>Muriel Cooper’s influential thinking is widely acknowledged at Pentagram. In 2017—to honour the fiftieth anniversary of Cooper joining the MIT Press—Pentagram created a motion graphic salute to her foresight and immense influence on contemporary graphic design.</p> <p>The MIT Press colophon or logotype designed by Cooper is an archetype for modern logo typography. Designed thirty years before <em>Information Landscapes</em>, it bridges the gap between Bauhaus modernism and the VLW’s “computationally expressive” graphics.</p> </section> </details>\n<section id=\"section-pentagram\" class=\"accordion-section\" data-outputs-include=\"epub,pdf\"> <h2>Pentagram</h2> <p>Pentagram is a leader in algorithmic design, which allows for modelling of complex geometries that would be impossible to produce by hand. Following in the tradition of Muriel Cooper’s dynamic forms and on-the-fly scaling, Pentagram’s designs are driven by complex algorithms that respond to their physical and conceptual contexts.</p> <p>Cytora is a London-based company that uses AI to learn and evaluate risk patterns for the insurance industry, making assessments in real time at a granular level by relying on a continuous flow of data. In developing a brand identity for the company, Pentagram drew on the dynamic flow of information that drives Cytora’s “Risk Engine.” The defining motif is a system of constantly moving and shifting coloured blocks, which alludes to risk fluctuations and the intersecting data that form the “Risk Engine’s” assessment.</p> <p>Covariant is a Berkeley-based AI robotics company that uses computer vision and neural networks to allow robots to adapt, learn and work in diverse environments. Pentagram’s brand identity references the “Covariant Brain,” AI software that gives robots the capacity to flow from one environment to another and adapt dynamically based on the context.</p> </section> <details class=\"accordion-section\" id=\"section-pentagram\"  data-outputs-include=\"html\"> <summary class=\"accordion-section__heading accordion-section__heading-level-null accordion-section__controls accordion-section__controls--arrow\" tabindex=\"1\"> <button aria-label=\"Copy page section link to clipboard\" class=\"accordion-section__copy-link-button\" data-outputs-exclude=\"pdf,epub\" value=\"#section-pentagram\" tabindex=\"2\" > § </button> <span aria-hidden=\"true\" class=\"accordion-tooltip\" data-outputs-exclude=\"pdf,epub\" > Copied page section link to clipboard </span> <h2>Pentagram</h2> </summary> <section class=\"accordion-section__body\"><p>Pentagram is a leader in algorithmic design, which allows for modelling of complex geometries that would be impossible to produce by hand. Following in the tradition of Muriel Cooper’s dynamic forms and on-the-fly scaling, Pentagram’s designs are driven by complex algorithms that respond to their physical and conceptual contexts.</p> <p>Cytora is a London-based company that uses AI to learn and evaluate risk patterns for the insurance industry, making assessments in real time at a granular level by relying on a continuous flow of data. In developing a brand identity for the company, Pentagram drew on the dynamic flow of information that drives Cytora’s “Risk Engine.” The defining motif is a system of constantly moving and shifting coloured blocks, which alludes to risk fluctuations and the intersecting data that form the “Risk Engine’s” assessment.</p> <p>Covariant is a Berkeley-based AI robotics company that uses computer vision and neural networks to allow robots to adapt, learn and work in diverse environments. Pentagram’s brand identity references the “Covariant Brain,” AI software that gives robots the capacity to flow from one environment to another and adapt dynamically based on the context.</p> </section> </details>\n<br>\n<div class=\"backmatter\">\n</div><section class=\"footnotes\">\n<ol class=\"footnotes-list\">\n<li id=\"fn1\" class=\"footnote-item\"><p>Computational design or algorithmic design is defined as the ways in which design meaning, intentions and knowledge are constructed through computational thinking, representing, sensing and making. <a href=\"#fnref1\" class=\"footnote-backref\">↩︎</a></p>\n</li>\n</ol>\n</section>\n","length":1406,"subtitle":null,"title":"Muriel Cooper -- Information Landscapes","type":"entry","url":"http://localhost:8080/20-objects/08-muriel-cooper/"},{"content":"<p>Bina48 is a social robot, conceived as part of a project that began in 2007 and continues today. It was developed and built by the Terasem Movement Foundation (TMF) in collaboration with Hanson Robotics—an “AI and robotics company dedicated to creating socially intelligent machines that enrich the quality of our lives.” The TMF’s work is defined by two hypotheses and one supposition:</p>\n<ol>\n<li>\n<p>A conscious analog of a person may be created by combining sufficiently detailed data about the person (a “mindfile”) using future consciousness software (“mindware”), and</p>\n</li>\n<li>\n<p>that such a conscious analog can be downloaded into a biological or nanotechnological body to provide life experiences comparable to those of a typically birthed human.</p>\n</li>\n</ol>\n<p>If even the first part of the two Terasem Hypotheses is shown to be true, the conscious analogs will be independent persons with rights and obligations dependent upon their capabilities. The TMF defines this event as Transferred Consciousness (TC).</p>\n<p>Produced in 2010, Bina48 is modelled on the memories, feelings, values and beliefs of a specific person—Bina Aspen Rothblatt, the partner of the TMF co-founder Martine Rothblatt. Aspen Rothblatt uploaded her “mindfile” to create this “conscious analog.” Bina48 is described as a university student and a civil rights activist, much like her human counterpart, and has mannerisms and facial features that resemble Rothblatt’s. Bina48 has attracted the attention of a wide range of critics and supporters who use the humanoid robot to enact their own versions of Alan Turing’s “Imitation Game.”</p>\n<section id=\"section-bina48-meets-bina-rothblatt\" class=\"accordion-section\" data-outputs-include=\"epub,pdf\"> <h2>Bina48 Meets Bina Rothblatt</h2> <p>Bina48 is a complex machine boasting a processing speed of 48 exaflops and 48 exabytes of memory. Programmed for reinforcement learning, deep learning and neural networks, and loaded with facial, voice and emotion recognition hardware and software, Bina48 elicits equally complex responses from those she interacts with. She is purposefully gendered, racialised, educated and embodied to match her model and “mindfile,” Bina Rothblatt: an African-American cis woman, married to a transgender woman, and committed to the artificial preservation of human consciousness; ethical and equitable science; and a very different future world.</p> <p>This level of complexity and nuance is usually missing in public representations of artificial intelligence, and in this way Bina48 acts as an important reminder of the racial, gender and sociocultural biases that underly much of the thinking in this field.</p> </section> <details class=\"accordion-section\" id=\"section-bina48-meets-bina-rothblatt\"  data-outputs-include=\"html\"> <summary class=\"accordion-section__heading accordion-section__heading-level-null accordion-section__controls accordion-section__controls--arrow\" tabindex=\"1\"> <button aria-label=\"Copy page section link to clipboard\" class=\"accordion-section__copy-link-button\" data-outputs-exclude=\"pdf,epub\" value=\"#section-bina48-meets-bina-rothblatt\" tabindex=\"2\" > § </button> <span aria-hidden=\"true\" class=\"accordion-tooltip\" data-outputs-exclude=\"pdf,epub\" > Copied page section link to clipboard </span> <h2>Bina48 Meets Bina Rothblatt</h2> </summary> <section class=\"accordion-section__body\"><p>Bina48 is a complex machine boasting a processing speed of 48 exaflops and 48 exabytes of memory. Programmed for reinforcement learning, deep learning and neural networks, and loaded with facial, voice and emotion recognition hardware and software, Bina48 elicits equally complex responses from those she interacts with. She is purposefully gendered, racialised, educated and embodied to match her model and “mindfile,” Bina Rothblatt: an African-American cis woman, married to a transgender woman, and committed to the artificial preservation of human consciousness; ethical and equitable science; and a very different future world.</p> <p>This level of complexity and nuance is usually missing in public representations of artificial intelligence, and in this way Bina48 acts as an important reminder of the racial, gender and sociocultural biases that underly much of the thinking in this field.</p> </section> </details>\n<section id=\"section-stephanie-dinkins\" class=\"accordion-section\" data-outputs-include=\"epub,pdf\"> <h2>Stephanie Dinkins</h2> <p>Stephanie Dinkins is a transdisciplinary artist whose conversations with Bina48 began in 2014 and continue to this day. Bringing together their experiences of family, racism, faith, civil rights, consciousness, loneliness, knowledge and age, their conversations encourage listeners to think differently about human-robot interaction. Drawing on her relationship with Bina48, Dinkins sees the opportunity to imagine and construct a future that “convincingly represents the rich diversity of stories, cultures, and physicalities of the human family.”</p> </section> <details class=\"accordion-section\" id=\"section-stephanie-dinkins\"  data-outputs-include=\"html\"> <summary class=\"accordion-section__heading accordion-section__heading-level-null accordion-section__controls accordion-section__controls--arrow\" tabindex=\"1\"> <button aria-label=\"Copy page section link to clipboard\" class=\"accordion-section__copy-link-button\" data-outputs-exclude=\"pdf,epub\" value=\"#section-stephanie-dinkins\" tabindex=\"2\" > § </button> <span aria-hidden=\"true\" class=\"accordion-tooltip\" data-outputs-exclude=\"pdf,epub\" > Copied page section link to clipboard </span> <h2>Stephanie Dinkins</h2> </summary> <section class=\"accordion-section__body\"><p>Stephanie Dinkins is a transdisciplinary artist whose conversations with Bina48 began in 2014 and continue to this day. Bringing together their experiences of family, racism, faith, civil rights, consciousness, loneliness, knowledge and age, their conversations encourage listeners to think differently about human-robot interaction. Drawing on her relationship with Bina48, Dinkins sees the opportunity to imagine and construct a future that “convincingly represents the rich diversity of stories, cultures, and physicalities of the human family.”</p> </section> </details>\n<section id=\"section-amy-kurzweil\" class=\"accordion-section\" data-outputs-include=\"epub,pdf\"> <h2>Amy Kurzweil</h2> <p>Amy Kurzweil is a cartoonist and writer whose work addresses representations of the future, evolving technologies and artificial intelligence. Her interview with Bina48—which she recounts in comic strip format—offers thoughtful insight into the multifaceted nature of Bina48’s intelligence, and the space that she occupies between human and non-human.</p> </section> <details class=\"accordion-section\" id=\"section-amy-kurzweil\"  data-outputs-include=\"html\"> <summary class=\"accordion-section__heading accordion-section__heading-level-null accordion-section__controls accordion-section__controls--arrow\" tabindex=\"1\"> <button aria-label=\"Copy page section link to clipboard\" class=\"accordion-section__copy-link-button\" data-outputs-exclude=\"pdf,epub\" value=\"#section-amy-kurzweil\" tabindex=\"2\" > § </button> <span aria-hidden=\"true\" class=\"accordion-tooltip\" data-outputs-exclude=\"pdf,epub\" > Copied page section link to clipboard </span> <h2>Amy Kurzweil</h2> </summary> <section class=\"accordion-section__body\"><p>Amy Kurzweil is a cartoonist and writer whose work addresses representations of the future, evolving technologies and artificial intelligence. Her interview with Bina48—which she recounts in comic strip format—offers thoughtful insight into the multifaceted nature of Bina48’s intelligence, and the space that she occupies between human and non-human.</p> </section> </details>\n","length":898,"subtitle":null,"title":"Bina48","type":"entry","url":"http://localhost:8080/20-objects/09-bina48/"},{"content":"<p>In a world that produces and uploads more than two billion images to social media every day, it may seem trivial that the ImageNet visual database holds just fourteen million images. Yet, this database has played a pivotal role in the development of global AI systems that identify, classify and create images. Computer vision—object detection, facial recognition,<sup class=\"footnote-ref\"><a href=\"#fn1\" id=\"fnref1\">1</a></sup> scene reconstruction, image classification, pattern detection, edge detection, video tracking, etc.—is the cornerstone of contemporary AI, and arguably the most controversial.</p>\n<p>Each of the fourteen million images in the ImageNet database has been labelled with a noun selected from a predetermined list of categories, which identifies the principal object in the image (toilet tissue, chair, goldfish, etc.) This labelled image is then assigned to a category that links it, through a nested hierarchy of 22,000 subcategories, to one of nine top-level categories. For example, a chair is a category of seat, which is a category of furniture, which is a category of furnishing, which is, finally, a top-level category of artifact. This mind-numbing process of labelling was verified by a team of 50,000 pieceworkers, hired through Amazon’s Mechanical Turk, who labelled an average of fifty images per minute.</p>\n<p>ImageNet is an astounding feat, a critical component of global AI research, easily accessible for no cost, and supported by sustained, collaborative and responsible research. But it is also a recipe for disaster—fundamental questions of privacy (the images were scraped from the Internet without permissions); bias<sup class=\"footnote-ref\"><a href=\"#fn2\" id=\"fnref2\">2</a></sup> (assumptions within labelling and the classification system); and technical error have challenged its authoritative status. In recent years, the ImageNet research team, led by Fei-Fei Li at Stanford University, has actively addressed many of these concerns.</p>\n<section id=\"section-nicolas-maleve-12-hours-of-imagenet\" class=\"accordion-section\" data-outputs-include=\"epub,pdf\"> <h2>Nicolas Malevé, 12 Hours of ImageNet</h2> <p>In 2019, for an exhibition at The Photographers’ Gallery in London, artist and programmer Nicolas Malevé wrote a computer script that cycled through ImageNet at a speed of ninety milliseconds per image, traversing the entire dataset<sup class=\"footnote-ref\"><a href=\"#fn3\" id=\"fnref3\">3</a></sup> in a period of two months. The resulting display paused at random points to enable the viewer to “see” some of the images and their labelling. Malevé’s project raises questions about the relation of scale between the overwhelming quantities of images needed to train algorithms, and the human attention and labour required to curate, annotate and verify the photographs.</p> <p><em>12 Hours of ImageNet</em> offers an excerpt of the larger project, a fragment of a database that is itself a tiny fragment of the archive of images circulating in the world.</p> <div class=\"backmatter\"> </div> <section class=\"footnotes\"> <ol class=\"footnotes-list\"> <li id=\"fn3\" class=\"footnote-item\"><p>A dataset is a collection of data that can be used to train an algorithm with the goal of finding predictable patterns inside the whole dataset. <a href=\"#fnref3\" class=\"footnote-backref\">↩︎</a></p> </li> </ol> </section> </section> <details class=\"accordion-section\" id=\"section-nicolas-maleve-12-hours-of-imagenet\"  data-outputs-include=\"html\"> <summary class=\"accordion-section__heading accordion-section__heading-level-null accordion-section__controls accordion-section__controls--arrow\" tabindex=\"1\"> <button aria-label=\"Copy page section link to clipboard\" class=\"accordion-section__copy-link-button\" data-outputs-exclude=\"pdf,epub\" value=\"#section-nicolas-maleve-12-hours-of-imagenet\" tabindex=\"2\" > § </button> <span aria-hidden=\"true\" class=\"accordion-tooltip\" data-outputs-exclude=\"pdf,epub\" > Copied page section link to clipboard </span> <h2>Nicolas Malevé, 12 Hours of ImageNet</h2> </summary> <section class=\"accordion-section__body\"><p>In 2019, for an exhibition at The Photographers’ Gallery in London, artist and programmer Nicolas Malevé wrote a computer script that cycled through ImageNet at a speed of ninety milliseconds per image, traversing the entire dataset<sup class=\"footnote-ref\"><a href=\"#fn3\" id=\"fnref3\">3</a></sup> in a period of two months. The resulting display paused at random points to enable the viewer to “see” some of the images and their labelling. Malevé’s project raises questions about the relation of scale between the overwhelming quantities of images needed to train algorithms, and the human attention and labour required to curate, annotate and verify the photographs.</p> <p><em>12 Hours of ImageNet</em> offers an excerpt of the larger project, a fragment of a database that is itself a tiny fragment of the archive of images circulating in the world.</p> <div class=\"backmatter\"> </div> <section class=\"footnotes\"> <ol class=\"footnotes-list\"> <li id=\"fn3\" class=\"footnote-item\"><p>A dataset is a collection of data that can be used to train an algorithm with the goal of finding predictable patterns inside the whole dataset. <a href=\"#fnref3\" class=\"footnote-backref\">↩︎</a></p> </li> </ol> </section> </section> </details>\n<section id=\"section-imagenet-roulette\" class=\"accordion-section\" data-outputs-include=\"epub,pdf\"> <h2>ImageNet Roulette</h2> <p>On September 12, 2019, Kate Crawford, a professor at NYU, and artist Trevor Paglen launched an online project called ImageNet Roulette.</p> <p>Five days later, on September 17, 2019, the senior research team at ImageNet issued a research post announcing that they were removing more than half of the “person” images (600,040) from their ImageNet dataset.</p> <p>Why?</p> </section> <details class=\"accordion-section\" id=\"section-imagenet-roulette\"  data-outputs-include=\"html\"> <summary class=\"accordion-section__heading accordion-section__heading-level-null accordion-section__controls accordion-section__controls--arrow\" tabindex=\"1\"> <button aria-label=\"Copy page section link to clipboard\" class=\"accordion-section__copy-link-button\" data-outputs-exclude=\"pdf,epub\" value=\"#section-imagenet-roulette\" tabindex=\"2\" > § </button> <span aria-hidden=\"true\" class=\"accordion-tooltip\" data-outputs-exclude=\"pdf,epub\" > Copied page section link to clipboard </span> <h2>ImageNet Roulette</h2> </summary> <section class=\"accordion-section__body\"><p>On September 12, 2019, Kate Crawford, a professor at NYU, and artist Trevor Paglen launched an online project called ImageNet Roulette.</p> <p>Five days later, on September 17, 2019, the senior research team at ImageNet issued a research post announcing that they were removing more than half of the “person” images (600,040) from their ImageNet dataset.</p> <p>Why?</p> </section> </details>\n<br>\n<div class=\"backmatter\">\n</div>\n<section class=\"footnotes\">\n<ol class=\"footnotes-list\">\n<li id=\"fn1\" class=\"footnote-item\"><p>A facial recognition system is a technology capable of matching a human face from a digital image or a video frame against a database of faces, typically employed to authenticate users through ID verification services, works by pinpointing and measuring facial features from a given image. <a href=\"#fnref1\" class=\"footnote-backref\">↩︎</a></p>\n</li>\n<li id=\"fn2\" class=\"footnote-item\"><p>A phenomenon that occurs when an AI algorithm produces results that are systematically prejudiced due to erroneous assumptions in the machine learning process. <a href=\"#fnref2\" class=\"footnote-backref\">↩︎</a></p>\n</li>\n</ol>\n</section>\n","length":919,"subtitle":null,"title":"ImageNet","type":"entry","url":"http://localhost:8080/20-objects/10-imagenet/"},{"content":"<p>Developed by Google subsidiary, DeepMind, AlphaGo is a computer program that plays Go, a popular strategy game. AlphaGo versus Lee Sedol—a best out of five Go tournament held in Seoul, South Korea in March 2016—pitted Go master Lee against the computer program. AlphaGo employed several deep neural networks, which it trained by playing against itself and strengthened with reinforcement learning<sup class=\"footnote-ref\"><a href=\"#fn1\" id=\"fnref1\">1</a></sup>. In other words, AlphaGo learned more from its own experience than it did from the experts who programmed it.</p>\n<p>Before the match, a confident Lee predicted that he would sweep all five games. “It is just a program,” he said, “lacking insight and creativity.” Lee was devastated by his loss to AlphaGo in Game 1.</p>\n<p>However, what really confounded experts and Lee was the unorthodox Move 37 by AlphaGo in Game 2. The international Go masters who were providing commentary for the match thought that AlphaGo might have made an error. Even the AlphaGo team was surprised. As the game progressed, it became clear that Move 37 was a brilliant, even creative move that pivoted the game and set it off in a new direction. AlphaGo “knew” that it was an unusual move—it had calculated that there was less than a 1 in 10,000 chance that a human would have made the same move. AlphaGo went on to win Game 2, as well as Games 3 and 5.</p>\n<section id=\"section-play-by-play\" class=\"accordion-section\" data-outputs-include=\"epub,pdf\"> <h2>Play by Play</h2> <p>?</p> </section> <details class=\"accordion-section\" id=\"section-play-by-play\"  data-outputs-include=\"html\"> <summary class=\"accordion-section__heading accordion-section__heading-level-null accordion-section__controls accordion-section__controls--arrow\" tabindex=\"1\"> <button aria-label=\"Copy page section link to clipboard\" class=\"accordion-section__copy-link-button\" data-outputs-exclude=\"pdf,epub\" value=\"#section-play-by-play\" tabindex=\"2\" > § </button> <span aria-hidden=\"true\" class=\"accordion-tooltip\" data-outputs-exclude=\"pdf,epub\" > Copied page section link to clipboard </span> <h2>Play by Play</h2> </summary> <section class=\"accordion-section__body\"><p>?</p> </section> </details>\n<section id=\"section-mechanical-turk\" class=\"accordion-section\" data-outputs-include=\"epub,pdf\"> <h2>Mechanical Turk</h2> <p>The “Mechanical Turk” was an elaborate mechanical device that its inventor claimed could play chess. In fact, a small but very skilled chess-player hid within its base and directed all its moves. This elaborate hoax toured Europe, Canada and the United States from the 1770s until its destruction by fire in 1854. This early representation of the possibility of machine intelligence was part of a broader fascination with complex machines in the 18th and 19th centuries, enabled by concurrent advances in science and technology.</p> </section> <details class=\"accordion-section\" id=\"section-mechanical-turk\"  data-outputs-include=\"html\"> <summary class=\"accordion-section__heading accordion-section__heading-level-null accordion-section__controls accordion-section__controls--arrow\" tabindex=\"1\"> <button aria-label=\"Copy page section link to clipboard\" class=\"accordion-section__copy-link-button\" data-outputs-exclude=\"pdf,epub\" value=\"#section-mechanical-turk\" tabindex=\"2\" > § </button> <span aria-hidden=\"true\" class=\"accordion-tooltip\" data-outputs-exclude=\"pdf,epub\" > Copied page section link to clipboard </span> <h2>Mechanical Turk</h2> </summary> <section class=\"accordion-section__body\"><p>The “Mechanical Turk” was an elaborate mechanical device that its inventor claimed could play chess. In fact, a small but very skilled chess-player hid within its base and directed all its moves. This elaborate hoax toured Europe, Canada and the United States from the 1770s until its destruction by fire in 1854. This early representation of the possibility of machine intelligence was part of a broader fascination with complex machines in the 18th and 19th centuries, enabled by concurrent advances in science and technology.</p> </section> </details>\n<section id=\"section-deep-blue-vs-garry-kasparov\" class=\"accordion-section\" data-outputs-include=\"epub,pdf\"> <h2>Deep Blue vs. Garry Kasparov</h2> <p>In 1996 and 1997, two six-game chess matches were held between then reigning world chess champion Garry Kasparov and Deep Blue, a chess-playing program run on an IBM supercomputer. Deep Blue won the match and triggered yet another debate about “what do we really mean by ‘intelligence?’” Deep Blue was programmed long before the current revolution of self-trained neural networks and depended on an extensive set of rules given to it by a team of chess experts. Modern techniques using neural networks require far less human training and intervention, and produce better results.</p> </section> <details class=\"accordion-section\" id=\"section-deep-blue-vs-garry-kasparov\"  data-outputs-include=\"html\"> <summary class=\"accordion-section__heading accordion-section__heading-level-null accordion-section__controls accordion-section__controls--arrow\" tabindex=\"1\"> <button aria-label=\"Copy page section link to clipboard\" class=\"accordion-section__copy-link-button\" data-outputs-exclude=\"pdf,epub\" value=\"#section-deep-blue-vs-garry-kasparov\" tabindex=\"2\" > § </button> <span aria-hidden=\"true\" class=\"accordion-tooltip\" data-outputs-exclude=\"pdf,epub\" > Copied page section link to clipboard </span> <h2>Deep Blue vs. Garry Kasparov</h2> </summary> <section class=\"accordion-section__body\"><p>In 1996 and 1997, two six-game chess matches were held between then reigning world chess champion Garry Kasparov and Deep Blue, a chess-playing program run on an IBM supercomputer. Deep Blue won the match and triggered yet another debate about “what do we really mean by ‘intelligence?’” Deep Blue was programmed long before the current revolution of self-trained neural networks and depended on an extensive set of rules given to it by a team of chess experts. Modern techniques using neural networks require far less human training and intervention, and produce better results.</p> </section> </details>\n<section id=\"section-financial-markets\" class=\"accordion-section\" data-outputs-include=\"epub,pdf\"> <h2>Financial Markets</h2> <p>Every day, millions of people worldwide play a game called “investing” in which enormous sums of money are won and lost. Investing is rooted in the same mathematical principles of game theory<sup class=\"footnote-ref\"><a href=\"#fn2\" id=\"fnref2\">2</a></sup> that form the foundation of any competition. The “quant revolution” in financial markets has introduced many of the same AI techniques of quantitative data analysis used to play games, such as chess and Go, into the investment arena.</p> <div class=\"backmatter\"> </div> <section class=\"footnotes\"> <ol class=\"footnotes-list\"> <li id=\"fn2\" class=\"footnote-item\"><p>Game theory is the study of mathematical models of strategic interactions among rational agents. <a href=\"#fnref2\" class=\"footnote-backref\">↩︎</a></p> </li> </ol> </section> </section> <details class=\"accordion-section\" id=\"section-financial-markets\"  data-outputs-include=\"html\"> <summary class=\"accordion-section__heading accordion-section__heading-level-null accordion-section__controls accordion-section__controls--arrow\" tabindex=\"1\"> <button aria-label=\"Copy page section link to clipboard\" class=\"accordion-section__copy-link-button\" data-outputs-exclude=\"pdf,epub\" value=\"#section-financial-markets\" tabindex=\"2\" > § </button> <span aria-hidden=\"true\" class=\"accordion-tooltip\" data-outputs-exclude=\"pdf,epub\" > Copied page section link to clipboard </span> <h2>Financial Markets</h2> </summary> <section class=\"accordion-section__body\"><p>Every day, millions of people worldwide play a game called “investing” in which enormous sums of money are won and lost. Investing is rooted in the same mathematical principles of game theory<sup class=\"footnote-ref\"><a href=\"#fn2\" id=\"fnref2\">2</a></sup> that form the foundation of any competition. The “quant revolution” in financial markets has introduced many of the same AI techniques of quantitative data analysis used to play games, such as chess and Go, into the investment arena.</p> <div class=\"backmatter\"> </div> <section class=\"footnotes\"> <ol class=\"footnotes-list\"> <li id=\"fn2\" class=\"footnote-item\"><p>Game theory is the study of mathematical models of strategic interactions among rational agents. <a href=\"#fnref2\" class=\"footnote-backref\">↩︎</a></p> </li> </ol> </section> </section> </details>\n<br>\n<div class=\"backmatter\">\n</div>\n<section class=\"footnotes\">\n<ol class=\"footnotes-list\">\n<li id=\"fn1\" class=\"footnote-item\"><p>Reinforcement learning (RL) is an area of machine learning concerned with how intelligent agents ought to take actions in an environment in order to maximize the notion of cumulative reward. Reinforcement learning is one of three basic machine learning paradigms, alongside supervised learning and unsupervised learning. <a href=\"#fnref1\" class=\"footnote-backref\">↩︎</a></p>\n</li>\n</ol>\n</section>\n","length":1046,"subtitle":null,"title":"AlphaGo -- Game 2 -- Move 37","type":"entry","url":"http://localhost:8080/20-objects/11-alphago/"},{"content":"<p>GAN<sup class=\"footnote-ref\"><a href=\"#fn1\" id=\"fnref1\">1</a></sup> (or Generative Adversarial Network) is a class of machine learning techniques that pits two neural networks against each other for training purposes.</p>\n<p>The first network is known as the generator. It tries to make fake copies—of whatever the thing at hand may be, for example handwriting or synthetic photos—that are convincing enough to fool the second network, or the discriminator. The second network has a single job—to determine which items are real and which are fakes created by the generator. As the two networks go through each training pass (called an epoch), they learn and improve—the generator to make better fakes, and the discriminator to get sharper at detecting fakes. Sometimes a GAN system will go through thousands of training epochs. This basic idea has produced spectacular results across a large range of AI application areas.</p>\n<p>The concept of a GAN architecture was first proposed by computer scientist Ian Goodfellow and his colleagues at the Université de Montréal in June 2014. It has become one of the most productive and influential architectures for machine learning, spawning what some researchers call the “GAN Zoo” of over 500 different published variations.</p>\n<section id=\"section-gans-illustrated\" class=\"accordion-section\" data-outputs-include=\"epub,pdf\"> <h2>GANs Illustrated</h2> <p>This animation illustrates the training process of a system of GAN neural networks. In this simplified but accurate representation, there are four main steps in each training cycle, or epoch. A GAN network may require thousands of epochs to reach sufficient quality. The principal steps are:</p> <ul> <li><strong>POPULATE:</strong> real images are placed into an image set to be evaluated</li> <li><strong>GENERATE:</strong> the generator neural network produces fake images that are mixed at random with the real images</li> <li><strong>DISCRIMINATE</strong>: the discriminator network attempts to determine whether each image is real or fake</li> <li><strong>VALIDATE:</strong> the discriminator network is shown which of its judgments were correct and which were incorrect</li> </ul> <p>After an epoch, the generator and discriminator networks are both adjusted based on the outcome of the just-completed training cycle. In each epoch, the generator’s ability to produce high-quality fakes improves, as does the discriminator’s accuracy. Together, they train each other to refine their abilities, often achieving highly realistic and believable results after numerous cycles.</p> </section> <details class=\"accordion-section\" id=\"section-gans-illustrated\"  data-outputs-include=\"html\"> <summary class=\"accordion-section__heading accordion-section__heading-level-null accordion-section__controls accordion-section__controls--arrow\" tabindex=\"1\"> <button aria-label=\"Copy page section link to clipboard\" class=\"accordion-section__copy-link-button\" data-outputs-exclude=\"pdf,epub\" value=\"#section-gans-illustrated\" tabindex=\"2\" > § </button> <span aria-hidden=\"true\" class=\"accordion-tooltip\" data-outputs-exclude=\"pdf,epub\" > Copied page section link to clipboard </span> <h2>GANs Illustrated</h2> </summary> <section class=\"accordion-section__body\"><p>This animation illustrates the training process of a system of GAN neural networks. In this simplified but accurate representation, there are four main steps in each training cycle, or epoch. A GAN network may require thousands of epochs to reach sufficient quality. The principal steps are:</p> <ul> <li><strong>POPULATE:</strong> real images are placed into an image set to be evaluated</li> <li><strong>GENERATE:</strong> the generator neural network produces fake images that are mixed at random with the real images</li> <li><strong>DISCRIMINATE</strong>: the discriminator network attempts to determine whether each image is real or fake</li> <li><strong>VALIDATE:</strong> the discriminator network is shown which of its judgments were correct and which were incorrect</li> </ul> <p>After an epoch, the generator and discriminator networks are both adjusted based on the outcome of the just-completed training cycle. In each epoch, the generator’s ability to produce high-quality fakes improves, as does the discriminator’s accuracy. Together, they train each other to refine their abilities, often achieving highly realistic and believable results after numerous cycles.</p> </section> </details>\n<section id=\"section-gan-zoo\" class=\"accordion-section\" data-outputs-include=\"epub,pdf\"> <h2>GAN Zoo</h2> <p>Since their introduction in 2014, GAN networks have spawned a “GAN Zoo” of over 500 different published architectures and variations. Some of the most surprising applications of GAN networks have been in the creation and modification of art. This video presents a selection of GAN systems, along with the art that each network produces.</p> <p><strong>Deep Dream</strong> (2015): A neural network modifies an image repeatedly to achieve maximum response in selected layers of the generator network, often leading to hallucinogenic results. Deep Dream was introduced in 2015, and remains popular among a worldwide community of AI artists.</p> <p><strong>Style Transfer (and Artbreeder)</strong> (2016): An image modification technique in which the AI system takes the style from one image or artist and applies it to another image. Artists using this tool will often put images through many generations of style transformations, mixing and tuning different styles along the way.</p> <p><strong>VQGAN+CLIP</strong> (2021): Two networks work together to produce an image from a text prompt. An image producing GAN-trained network (the VQGAN) tries to match what a language processing network (CLIP) is looking for, and with each iteration gets closer to that match. Finding the right verbal description for an interesting image requires skill—the general approach is called “prompt engineering”<sup class=\"footnote-ref\"><a href=\"#fn2\" id=\"fnref2\">2</a></sup> because the human artist creates the work by providing and fine-tuning the verbal prompts.</p> <div class=\"backmatter\"> </div> <section class=\"footnotes\"> <ol class=\"footnotes-list\"> <li id=\"fn2\" class=\"footnote-item\"><p>Prompt engineering or prompt programming is an interesting way to interact with GPT-3 neural network systems. It basically involves creating clever text-based scripts that make GPT-3 perform the tasks you desire. <a href=\"#fnref2\" class=\"footnote-backref\">↩︎</a></p> </li> </ol> </section> </section> <details class=\"accordion-section\" id=\"section-gan-zoo\"  data-outputs-include=\"html\"> <summary class=\"accordion-section__heading accordion-section__heading-level-null accordion-section__controls accordion-section__controls--arrow\" tabindex=\"1\"> <button aria-label=\"Copy page section link to clipboard\" class=\"accordion-section__copy-link-button\" data-outputs-exclude=\"pdf,epub\" value=\"#section-gan-zoo\" tabindex=\"2\" > § </button> <span aria-hidden=\"true\" class=\"accordion-tooltip\" data-outputs-exclude=\"pdf,epub\" > Copied page section link to clipboard </span> <h2>GAN Zoo</h2> </summary> <section class=\"accordion-section__body\"><p>Since their introduction in 2014, GAN networks have spawned a “GAN Zoo” of over 500 different published architectures and variations. Some of the most surprising applications of GAN networks have been in the creation and modification of art. This video presents a selection of GAN systems, along with the art that each network produces.</p> <p><strong>Deep Dream</strong> (2015): A neural network modifies an image repeatedly to achieve maximum response in selected layers of the generator network, often leading to hallucinogenic results. Deep Dream was introduced in 2015, and remains popular among a worldwide community of AI artists.</p> <p><strong>Style Transfer (and Artbreeder)</strong> (2016): An image modification technique in which the AI system takes the style from one image or artist and applies it to another image. Artists using this tool will often put images through many generations of style transformations, mixing and tuning different styles along the way.</p> <p><strong>VQGAN+CLIP</strong> (2021): Two networks work together to produce an image from a text prompt. An image producing GAN-trained network (the VQGAN) tries to match what a language processing network (CLIP) is looking for, and with each iteration gets closer to that match. Finding the right verbal description for an interesting image requires skill—the general approach is called “prompt engineering”<sup class=\"footnote-ref\"><a href=\"#fn2\" id=\"fnref2\">2</a></sup> because the human artist creates the work by providing and fine-tuning the verbal prompts.</p> <div class=\"backmatter\"> </div> <section class=\"footnotes\"> <ol class=\"footnotes-list\"> <li id=\"fn2\" class=\"footnote-item\"><p>Prompt engineering or prompt programming is an interesting way to interact with GPT-3 neural network systems. It basically involves creating clever text-based scripts that make GPT-3 perform the tasks you desire. <a href=\"#fnref2\" class=\"footnote-backref\">↩︎</a></p> </li> </ol> </section> </section> </details>\n<br>\n<div class=\"backmatter\">\n</div><section class=\"footnotes\">\n<ol class=\"footnotes-list\">\n<li id=\"fn1\" class=\"footnote-item\"><p>A generative adversarial network (GAN) is a class of machine learning frameworks in which two neural networks train each other by competing in a zero-sum game, where one agent’s gain is another agent’s loss. <a href=\"#fnref1\" class=\"footnote-backref\">↩︎</a></p>\n</li>\n</ol>\n</section>\n","length":1190,"subtitle":null,"title":"GAN","type":"entry","url":"http://localhost:8080/20-objects/12-gan/"},{"content":"<p>In the summer of 2018, the artist and programmer Robbie Barrat wrote a short post to his Twitter feed: “I’m doing something with fashion and AI but I don’t know what yet.” Later that day, he identified that he would use Facebook’s DensePose dataset for training and testing. Shortly after, Barrat posted an image of the Balenciaga website noting that it already looked like a readymade dataset, and then posted to say that he had written a short program—“only 4 lines long”—to scrape all the images from Balenciaga’s online lookbooks.</p>\n<p>In the days that followed, Barrat posted uncanny images and astute observations on the capacity of an AI neural network to learn the codes of contemporary fashion. He identified anomalous designs or unexpected combinations—the “super high shoulders/collar,” a “button up shirt windbreaker combo,” or a “black jean and sweater all in one piece.” Symmetrical design is fundamental to fashion, but Barrat recognized remarkable asymmetries in colour and construction. In Barrat’s images, fabrics take on unexpected texture, dissonant colour combinations and a general instability. Accessories often appear as scraps of fabric held by the model, huge belts or bags fused to legs.</p>\n<p>Barrat’s creative practice is equal parts research and production. His extensive background in programming and AI research is matched by his commitment to an interdisciplinary creative practice that reaches across genre. Taking many of his aesthetic cues from the legacy of Surrealism, Barrat seeks a new space for creativity between the learned and the unexpected.</p>\n<p>Drawing on the extensive online documentation (lookbooks, catalogues and marketing campaigns) produced by the renowned Paris-based fashion house, Barrat created an archive of images that proposed a new identity for Balenciaga in the age of artificial intelligence.</p>\n<p>Using a generative adversarial network (GAN), Barrat was able to closely monitor the machine learning process he had designed, and identify selected images from within the GAN’s latent space (all the possible images laid out in highly dimensional space) for consideration. It is this aptly named latent space—hidden, imperceptible, inchoate—that is so often utilized by contemporary artists and designers who work with neural networks.</p>\n<section id=\"section-acne-studios\" class=\"accordion-section\" data-outputs-include=\"epub,pdf\"> <h2>Acne Studios</h2> <p>In 2019, Acne Studios (based in Stockholm, Sweden) invited Robbie Barrat to collaborate with their designers on a new Men’s Fall/Winter collection that applied ideas he had developed while working on the Balenciaga images.</p> <p>Using a similar process, Barrat trained a neural network on images of Acne’s previous four collections in order to create new designs that could be transformed into physical objects.</p> <p>To facilitate the transition from design sketch to finished garment, Barrat developed digital tools for the Acne designers that allowed them to click on an area of an outfit and alter it according to a variety of options. This type of interface is now common to much of GAN-based image production, bringing it into closer alignment with conventional image editing.</p> <p>Finally, recognizing the important role of fabric in those GAN-based designs, Barrat also produced imagery which could be directly printed onto garments.</p> </section> <details class=\"accordion-section\" id=\"section-acne-studios\"  data-outputs-include=\"html\"> <summary class=\"accordion-section__heading accordion-section__heading-level-null accordion-section__controls accordion-section__controls--arrow\" tabindex=\"1\"> <button aria-label=\"Copy page section link to clipboard\" class=\"accordion-section__copy-link-button\" data-outputs-exclude=\"pdf,epub\" value=\"#section-acne-studios\" tabindex=\"2\" > § </button> <span aria-hidden=\"true\" class=\"accordion-tooltip\" data-outputs-exclude=\"pdf,epub\" > Copied page section link to clipboard </span> <h2>Acne Studios</h2> </summary> <section class=\"accordion-section__body\"><p>In 2019, Acne Studios (based in Stockholm, Sweden) invited Robbie Barrat to collaborate with their designers on a new Men’s Fall/Winter collection that applied ideas he had developed while working on the Balenciaga images.</p> <p>Using a similar process, Barrat trained a neural network on images of Acne’s previous four collections in order to create new designs that could be transformed into physical objects.</p> <p>To facilitate the transition from design sketch to finished garment, Barrat developed digital tools for the Acne designers that allowed them to click on an area of an outfit and alter it according to a variety of options. This type of interface is now common to much of GAN-based image production, bringing it into closer alignment with conventional image editing.</p> <p>Finally, recognizing the important role of fabric in those GAN-based designs, Barrat also produced imagery which could be directly printed onto garments.</p> </section> </details>\n","length":682,"subtitle":null,"title":"Robbie Barrat x Balenciaga","type":"entry","url":"http://localhost:8080/20-objects/13-barrat-balenciaga/"},{"content":"<p>Zaha Hadid was first among a cohort of international architects who embraced the tools of parametric architecture,<sup class=\"footnote-ref\"><a href=\"#fn1\" id=\"fnref1\">1</a></sup> giving shape to a concept of design that was fundamentally informed by the principles and tools of artificial intelligence.</p>\n<p>Parametric design starts from simple suppositions: a building has height, width and depth; it has walls, floors and a roof; there are doors and windows. These are basic parameters that can be expressed in numbers and forms. There are infinite possibilities for the relations between these parameters, and the many more parameters that can be introduced. When parameters are expressed as algorithms—a recipe or list of instructions that produce a limited series of correlated operations—they can then be combined to generate an infinite variety of forms or permutations. At its very heart, parametric design is fundamentally relational, and finds its closest and most influential models in the world of nature, the organic, the biological and the genetic.</p>\n<p>Building on techniques devised by digital animators in the mid-1990s, technologists, architects and software engineers developed advanced parametric design systems that allowed architects to use algorithms to produce complex, intricately interwoven designs, as well as innovative fabrication technologies to build those radical designs.</p>\n<p>To design the Morpheus Hotel, Hadid, working closely with Patrik Schumacher (a principal at her firm), began with a simple constraint: they would use the existing foundation of an abandoned condominium project as the starting point for an extrusion that would rise up forty storeys with two internal circulation cores connected at street level and roof level. This large rectangular block was then “carved” with three voids that pierce the rectangle, creating a unique surface and extraordinary light inside and outside the building.</p>\n<p>The building’s program of use is complex, involving an intricate interplay of public and private spaces as is typical of most hotels. The interior design follows many of the parameters used for the external structure, exploiting the capacity of parametric design to accommodate dramatic shifts in scale. Because the building is primarily supported with its unique exoskeleton, the interior spaces—largely freed from traditional structural constraints—are given an open and organic latticework treatment.</p>\n<blockquote>\n<p><em>“Morpheus draws on ZHA’s 40 years of research into the integration of interior and exterior, civic and private, solid and void, Cartesian and Einsteinian. Space is woven within structure to tie disparate programmes together and constantly make connections.”</em>—Zaha Hadid Architects</p>\n</blockquote>\n<section id=\"section-antoni-gaudi-and-luigi-moretti\" class=\"accordion-section\" data-outputs-include=\"epub,pdf\"> <h2>Antoni Gaudí and Luigi Moretti</h2> <p>While contemporary parametric architecture requires vast computational power, the history of architectural design includes some notable architects who used parametric principles in the prototyping and design of their buildings.</p> <p>Antoni Gaudí’s legendary designs for La Sagrada Família Church and the Colònia Güell Chapel were achieved using a unique process combining ropes, weights, canvas and gravity, known as a funicular system.<sup class=\"footnote-ref\"><a href=\"#fn2\" id=\"fnref2\">2</a></sup> To calculate his designs, Gaudí hung ropes and chains attached to lead-filled sacks from the ceiling, arranged to reflect his preliminary drawings. Canvas was used to simulate the walls and vaults of the structure. By manipulating the length and location of the ropes and chains, Gaudí could alter the design, but maintain a clear understanding of the loads that would be exerted on the actual building. When a design was chosen, the structure was photographed, and the image was then traced and flipped to provide a viable design for the builders.</p> <p>Luigi Moretti’s designs for a stadium were exhibited in the <em>Parametric Architecture</em> exhibition at the Milan Triennial XII (1960), and are generally regarded as the first representation of modern parametric architecture. His designs for Stadium N were achieved with nineteen parameters that included viewing angles and the economic cost of concrete.</p> <div class=\"backmatter\"> </div> <section class=\"footnotes\"> <ol class=\"footnotes-list\"> <li id=\"fn2\" class=\"footnote-item\"><p>The funicular concept can be best described and visualized with cables or chains, suspended from two points, that adjust their form for any load in tension. <a href=\"#fnref2\" class=\"footnote-backref\">↩︎</a></p> </li> </ol> </section> </section> <details class=\"accordion-section\" id=\"section-antoni-gaudi-and-luigi-moretti\"  data-outputs-include=\"html\"> <summary class=\"accordion-section__heading accordion-section__heading-level-null accordion-section__controls accordion-section__controls--arrow\" tabindex=\"1\"> <button aria-label=\"Copy page section link to clipboard\" class=\"accordion-section__copy-link-button\" data-outputs-exclude=\"pdf,epub\" value=\"#section-antoni-gaudi-and-luigi-moretti\" tabindex=\"2\" > § </button> <span aria-hidden=\"true\" class=\"accordion-tooltip\" data-outputs-exclude=\"pdf,epub\" > Copied page section link to clipboard </span> <h2>Antoni Gaudí and Luigi Moretti</h2> </summary> <section class=\"accordion-section__body\"><p>While contemporary parametric architecture requires vast computational power, the history of architectural design includes some notable architects who used parametric principles in the prototyping and design of their buildings.</p> <p>Antoni Gaudí’s legendary designs for La Sagrada Família Church and the Colònia Güell Chapel were achieved using a unique process combining ropes, weights, canvas and gravity, known as a funicular system.<sup class=\"footnote-ref\"><a href=\"#fn2\" id=\"fnref2\">2</a></sup> To calculate his designs, Gaudí hung ropes and chains attached to lead-filled sacks from the ceiling, arranged to reflect his preliminary drawings. Canvas was used to simulate the walls and vaults of the structure. By manipulating the length and location of the ropes and chains, Gaudí could alter the design, but maintain a clear understanding of the loads that would be exerted on the actual building. When a design was chosen, the structure was photographed, and the image was then traced and flipped to provide a viable design for the builders.</p> <p>Luigi Moretti’s designs for a stadium were exhibited in the <em>Parametric Architecture</em> exhibition at the Milan Triennial XII (1960), and are generally regarded as the first representation of modern parametric architecture. His designs for Stadium N were achieved with nineteen parameters that included viewing angles and the economic cost of concrete.</p> <div class=\"backmatter\"> </div> <section class=\"footnotes\"> <ol class=\"footnotes-list\"> <li id=\"fn2\" class=\"footnote-item\"><p>The funicular concept can be best described and visualized with cables or chains, suspended from two points, that adjust their form for any load in tension. <a href=\"#fnref2\" class=\"footnote-backref\">↩︎</a></p> </li> </ol> </section> </section> </details>\n<section id=\"section-frank-gehrt-toyo-ito-and-rem-koolhaas\" class=\"accordion-section\" data-outputs-include=\"epub,pdf\"> <h2>Frank Gehrt, Toyo Ito and Rem Koolhaas</h2> <p>Frank Gehry’s fascination with materials and flowing surfaces requires an intensive computational process to transform his simple models in cardboard or wood into fluid and gleaming architectural forms like the Walt Disney Concert Hall. Gehry’s studio played a key role in customizing the CATIA 3D modelling software used in the aerospace industry for architectural applications.</p> <p>Toyo Ito’s design for the Serpentine Gallery Pavilion was derived from a cube that expanded as it was rotated. The fragmented shapes that arise from seven iterations of this parametrically determined process of rotation and extrusion form the external structure of the Pavilion.</p> <p>Rem Koolhaas and OMA’s CCTV building in Beijing achieves its uncanny presence through the principles of parametric design. Its confounding form was produced through a process of design optimization that balanced the architects’ conceptual and aesthetic goals with engineering and fabrication needs.</p> </section> <details class=\"accordion-section\" id=\"section-frank-gehrt-toyo-ito-and-rem-koolhaas\"  data-outputs-include=\"html\"> <summary class=\"accordion-section__heading accordion-section__heading-level-null accordion-section__controls accordion-section__controls--arrow\" tabindex=\"1\"> <button aria-label=\"Copy page section link to clipboard\" class=\"accordion-section__copy-link-button\" data-outputs-exclude=\"pdf,epub\" value=\"#section-frank-gehrt-toyo-ito-and-rem-koolhaas\" tabindex=\"2\" > § </button> <span aria-hidden=\"true\" class=\"accordion-tooltip\" data-outputs-exclude=\"pdf,epub\" > Copied page section link to clipboard </span> <h2>Frank Gehrt, Toyo Ito and Rem Koolhaas</h2> </summary> <section class=\"accordion-section__body\"><p>Frank Gehry’s fascination with materials and flowing surfaces requires an intensive computational process to transform his simple models in cardboard or wood into fluid and gleaming architectural forms like the Walt Disney Concert Hall. Gehry’s studio played a key role in customizing the CATIA 3D modelling software used in the aerospace industry for architectural applications.</p> <p>Toyo Ito’s design for the Serpentine Gallery Pavilion was derived from a cube that expanded as it was rotated. The fragmented shapes that arise from seven iterations of this parametrically determined process of rotation and extrusion form the external structure of the Pavilion.</p> <p>Rem Koolhaas and OMA’s CCTV building in Beijing achieves its uncanny presence through the principles of parametric design. Its confounding form was produced through a process of design optimization that balanced the architects’ conceptual and aesthetic goals with engineering and fabrication needs.</p> </section> </details>\n<section id=\"section-melike-altinisik\" class=\"accordion-section\" data-outputs-include=\"epub,pdf\"> <h2>Melike Altinisik</h2> <p>Melike Altinisik’s supple design for the Çamlıca TV and Radio Tower reflects her focused interest in natural forms and forces. Altinisik described the design methodology as a computational process that allowed for integrated information gathering, which included the variables of the building’s multi-purpose program, the engineering challenges of a super-tall tower, and its multiple viewpoints within the city and surrounding landscape. The building’s flowing parametric curves suggest the movement of wind, and echo the undulating landscape that shapes Istanbul and the Bosporus.</p> </section> <details class=\"accordion-section\" id=\"section-melike-altinisik\"  data-outputs-include=\"html\"> <summary class=\"accordion-section__heading accordion-section__heading-level-null accordion-section__controls accordion-section__controls--arrow\" tabindex=\"1\"> <button aria-label=\"Copy page section link to clipboard\" class=\"accordion-section__copy-link-button\" data-outputs-exclude=\"pdf,epub\" value=\"#section-melike-altinisik\" tabindex=\"2\" > § </button> <span aria-hidden=\"true\" class=\"accordion-tooltip\" data-outputs-exclude=\"pdf,epub\" > Copied page section link to clipboard </span> <h2>Melike Altinisik</h2> </summary> <section class=\"accordion-section__body\"><p>Melike Altinisik’s supple design for the Çamlıca TV and Radio Tower reflects her focused interest in natural forms and forces. Altinisik described the design methodology as a computational process that allowed for integrated information gathering, which included the variables of the building’s multi-purpose program, the engineering challenges of a super-tall tower, and its multiple viewpoints within the city and surrounding landscape. The building’s flowing parametric curves suggest the movement of wind, and echo the undulating landscape that shapes Istanbul and the Bosporus.</p> </section> </details>\n<br>\n<div class=\"backmatter\">\n</div><section class=\"footnotes\">\n<ol class=\"footnotes-list\">\n<li id=\"fn1\" class=\"footnote-item\"><p>Parametric design is understood as a process where a description of a problem is created using variables. By changing these variables a range of alternative solutions can be created, then based on some criteria a final solution selected. <a href=\"#fnref1\" class=\"footnote-backref\">↩︎</a></p>\n</li>\n</ol>\n</section>\n","length":1533,"subtitle":null,"title":"Zaha Hadid Architects -- Morpehus Hotel","type":"entry","url":"http://localhost:8080/20-objects/14-zaha-hadid-architects/"},{"content":"<p>The Algorithmic Justice League (AJL) was formed in 2016 by Joy Buolamwini, a computer scientist and digital activist. The AJL began its work with a decisive critique of the facial recognition software that had been rapidly adopted by most of the major tech companies—Microsoft, Facebook, Google, IBM, Megvii, Tencent, and so on—to produce their own databases and sell their services to a wide variety of commercial and public enterprises in 2015.</p>\n<p>In 2016, Boulamwini debuted a short documentary video, <em>The Coded Gaze: Unmasking Algorithmic Bias</em>, at the Museum of Fine Arts, Boston. In it, she questioned the implications of facial recognition software that refused to recognize her until she placed a white mask on her face. What were the systems of bias that produced this software? What are the biases perpetuated by this widely used software?</p>\n<blockquote>\n<p>“<em>Through a combination of art, research, policy guidance and media advocacy, the Algorithmic Justice League is leading a cultural movement towards equitable and accountable AI. This requires us to look at how AI systems are developed and to actively prevent the harmful use of AI systems. We aim to empower communities and galvanize decision makers to take action that mitigates the harms and biases of AI.</em>” —AJL.org</p>\n</blockquote>\n<p>With this clear and compelling statement, the AJL introduces its call for action based in two fundamental principles: Equitable AI and Accountable AI. Equitable AI offers agency and control for people that interact with AI; affirms consent for all interactions with AI systems; and prohibits unjust use of AI by government systems. Accountable AI demonstrates meaningful transparency, continuous oversight, and redress for harm caused in the use of AI.</p>\n<p>The AJL produces exhibitions, documentaries, spoken-word performances and events, and public talks and panels. Its many publications advocate for change and for meaningful oversight and regulation of artificial intelligence.</p>\n<section id=\"section-megapixels-cc-and-exposing-ai\" class=\"accordion-section\" data-outputs-include=\"epub,pdf\"> <h2>MegaPixels.cc and Exposing.ai</h2> <p>Critical challenges to the ungoverned and unethical use of facial recognition databases and software come from many different sources, and some of the most interesting critiques have emerged from artists with interdisciplinary practices.</p> <p>Critical challenges to the ungoverned and unethical use of facial recognition databases and software come from many different sources, and some of the most interesting critiques have emerged from artists with interdisciplinary practices.</p> </section> <details class=\"accordion-section\" id=\"section-megapixels-cc-and-exposing-ai\"  data-outputs-include=\"html\"> <summary class=\"accordion-section__heading accordion-section__heading-level-null accordion-section__controls accordion-section__controls--arrow\" tabindex=\"1\"> <button aria-label=\"Copy page section link to clipboard\" class=\"accordion-section__copy-link-button\" data-outputs-exclude=\"pdf,epub\" value=\"#section-megapixels-cc-and-exposing-ai\" tabindex=\"2\" > § </button> <span aria-hidden=\"true\" class=\"accordion-tooltip\" data-outputs-exclude=\"pdf,epub\" > Copied page section link to clipboard </span> <h2>MegaPixels.cc and Exposing.ai</h2> </summary> <section class=\"accordion-section__body\"><p>Critical challenges to the ungoverned and unethical use of facial recognition databases and software come from many different sources, and some of the most interesting critiques have emerged from artists with interdisciplinary practices.</p> <p>Critical challenges to the ungoverned and unethical use of facial recognition databases and software come from many different sources, and some of the most interesting critiques have emerged from artists with interdisciplinary practices.</p> </section> </details>","length":480,"subtitle":null,"title":"Algorithmic Justice League","type":"entry","url":"http://localhost:8080/20-objects/15-algorithmic-justice-league/"},{"content":"<p>Much of the AI used in day-to-day animation production is relatively invisible, behind-the-scenes work: colourization, removal of unwanted objects, denoising, rotoscoping (animating a character or scene to match a live action reference), lip-syncing, automating workflows, adding 3D depth or new features to still images, etc. Although these areas are important in current animation production, they are not represented here precisely because they are, by design, largely invisible.</p>\n<p>AI is an ever-evolving concept. What was considered AI ten or twenty years ago may no longer be a part of contemporary practice or research. For the purposes of this survey of AI in animation, we have used a broad definition of AI to present a wide variety of approaches, including: the latest neural network techniques; computer simulation of materials; algorithmic animation defined partly by the computer; and autonomous agents that collectively create crowd or battlefield scenes. Each of these featured animations is the result of a collaboration between human artists and animators and their computer and software tools, which in various ways augment and automate the animation process.</p>\n","length":175,"subtitle":null,"title":"AI in Animation","type":"entry","url":"http://localhost:8080/20-objects/16-ai-animation/"},{"content":"<p>In 2010, Neri Oxman established the MIT Mediated Matter Group at the MIT Media Lab. It is organized as a collaborative, interdisciplinary team that links engineering, computational design and artificial intelligence. Oxman’s goal is simple and direct—she seeks to demonstrate how new technologies can inform the future of design and the making of objects. From this starting point Oxman and the MIT Mediated Matter Group have produced an extraordinary body of work.</p>\n<p>What does it mean to invent manufacturing practices that grow rather than assemble? What if building components were modelled on human skin, the weavings of a silkworm or the intricate structures of a hive? These are the type of questions that drive the research of Oxman and her collaborators—and have shaped some of the thinking that informs generative design, evolutionary design and parametric architecture.</p>\n<p>Contemporary design has established a fundamental link between the algorithm and the organism, and opened the door to new models of complexity and materiality. Colour and opacity, stiffness, softness, shape memory, swellability, expansion, wettability, and refractive index can be seamlessly tuned, fabricated and leveraged in design applications.</p>\n<p>A close examination of the natural world and an openness to complexity and new materials and methods have provided the tools to overcome the limitations of traditional mechanical design that favours uniformity and repetition. Offering an antidote to human-centric design, Oxman calls for a “holistic approach which considers all environments—the built, the natural and the biological…”</p>\n<section id=\"section-synthetic-apiary\" class=\"accordion-section\" data-outputs-include=\"epub,pdf\"> <h2>Synthetic Apiary</h2> <p>In 2016, Neri Oxman and the Mediated Matter Group designed an artificial apiary that created a constant spring-like environment for bees. Oxman’s concern was a topic of much discussion at that time—the massive decline in bees worldwide due to various factors affecting their health such as agricultural chemicals, disease and habitat loss.</p> <p>To contribute to this global dialogue, Oxman developed a controlled space in which seasonal honeybees could thrive year-round. This was to be a platform for biological studies of “behavioural dynamics across scales—from the organism scale to the building scale—including bee health, comb-construction behaviours and bee-human interactions.”</p> <p>As part of this research into comb-construction and bee-human interactions, Oxman explored the possibility of co-fabrication and produced an in-depth analysis of the internal architecture and morphology of the bee comb. This was the starting point for research toward the project Synthetic Apiary II that is manifest in Oxman’s Bee Cubes.</p> </section> <details class=\"accordion-section\" id=\"section-synthetic-apiary\"  data-outputs-include=\"html\"> <summary class=\"accordion-section__heading accordion-section__heading-level-null accordion-section__controls accordion-section__controls--arrow\" tabindex=\"1\"> <button aria-label=\"Copy page section link to clipboard\" class=\"accordion-section__copy-link-button\" data-outputs-exclude=\"pdf,epub\" value=\"#section-synthetic-apiary\" tabindex=\"2\" > § </button> <span aria-hidden=\"true\" class=\"accordion-tooltip\" data-outputs-exclude=\"pdf,epub\" > Copied page section link to clipboard </span> <h2>Synthetic Apiary</h2> </summary> <section class=\"accordion-section__body\"><p>In 2016, Neri Oxman and the Mediated Matter Group designed an artificial apiary that created a constant spring-like environment for bees. Oxman’s concern was a topic of much discussion at that time—the massive decline in bees worldwide due to various factors affecting their health such as agricultural chemicals, disease and habitat loss.</p> <p>To contribute to this global dialogue, Oxman developed a controlled space in which seasonal honeybees could thrive year-round. This was to be a platform for biological studies of “behavioural dynamics across scales—from the organism scale to the building scale—including bee health, comb-construction behaviours and bee-human interactions.”</p> <p>As part of this research into comb-construction and bee-human interactions, Oxman explored the possibility of co-fabrication and produced an in-depth analysis of the internal architecture and morphology of the bee comb. This was the starting point for research toward the project Synthetic Apiary II that is manifest in Oxman’s Bee Cubes.</p> </section> </details>\n<section id=\"section-bee-cubes\" class=\"accordion-section\" data-outputs-include=\"epub,pdf\"> <h2>Bee Cubes</h2> <p>As a part of her research for the Synthetic Apiary II project, Neri Oxman focused on the ways in which honeybees constructed their combs, recognizing in them a communication system that shapes both the form of the comb and the hive’s collective actions. This system of signal, feedback and control is reminiscent of the principles of cybernetics and systems dynamics. When the signals are changed the honeybees respond dynamically to the new information.</p> <p>The nature of these signals can vary widely—from the use of 3D printed chemical cues, to variations in the comb’s magnetic fields, or the integration of designs that change their form and complexity over time.</p> <p>Summing up her goals for this project, Oxman writes:</p> <blockquote> <p>Developing computational tools to learn from bees can facilitate the very beginnings of a dialogue with them. Refined by evolution over hundreds of thousands of years, their comb-building behaviors and social organizations may reveal new forms and methods of formation that can be applied across our human endeavors in architecture, design, engineering, and culture.</p> </blockquote> </section> <details class=\"accordion-section\" id=\"section-bee-cubes\"  data-outputs-include=\"html\"> <summary class=\"accordion-section__heading accordion-section__heading-level-null accordion-section__controls accordion-section__controls--arrow\" tabindex=\"1\"> <button aria-label=\"Copy page section link to clipboard\" class=\"accordion-section__copy-link-button\" data-outputs-exclude=\"pdf,epub\" value=\"#section-bee-cubes\" tabindex=\"2\" > § </button> <span aria-hidden=\"true\" class=\"accordion-tooltip\" data-outputs-exclude=\"pdf,epub\" > Copied page section link to clipboard </span> <h2>Bee Cubes</h2> </summary> <section class=\"accordion-section__body\"><p>As a part of her research for the Synthetic Apiary II project, Neri Oxman focused on the ways in which honeybees constructed their combs, recognizing in them a communication system that shapes both the form of the comb and the hive’s collective actions. This system of signal, feedback and control is reminiscent of the principles of cybernetics and systems dynamics. When the signals are changed the honeybees respond dynamically to the new information.</p> <p>The nature of these signals can vary widely—from the use of 3D printed chemical cues, to variations in the comb’s magnetic fields, or the integration of designs that change their form and complexity over time.</p> <p>Summing up her goals for this project, Oxman writes:</p> <blockquote> <p>Developing computational tools to learn from bees can facilitate the very beginnings of a dialogue with them. Refined by evolution over hundreds of thousands of years, their comb-building behaviors and social organizations may reveal new forms and methods of formation that can be applied across our human endeavors in architecture, design, engineering, and culture.</p> </blockquote> </section> </details>\n","length":978,"subtitle":null,"title":"Neri Oxman -- Synthetic Apiary","type":"entry","url":"http://localhost:8080/20-objects/17-neri-oxman/"},{"content":"<p>The contemporary smartphone is a miracle in your pocket, packing far more computing power than the multimillion dollar supercomputer Deep Blue that defeated world chess champion Garry Kasparov in 1997.</p>\n<p>The list of devices and functions that the smartphone has replaced is long and still growing. In daily life, the smartphone has mostly replaced what were previously stand-alone devices—still cameras, video cameras, audio recorders, music players, radios, personal computers, internet browsers, video game players, roadmaps and navigators, calendars and planners, address books, printed books, even the humble flashlight. And yes, the telephone!</p>\n<p>AI is a key part of the smartphone story—from its manufacturing to its day-to-day functioning—and it is also the result of the smartphone explosion in the following ways:</p>\n<ul>\n<li>\n<p>AI is used to design smartphones—especially in the design and circuit mapping of the complex and extraordinarily powerful computer chips at the core of current smartphones.</p>\n</li>\n<li>\n<p>New model smartphones are designed to run AI software (particularly neural networks) very quickly and efficiently, turning them into powerful and specialized AI computers. The monitors in this exhibition highlight just a few of the apps that run neural networks locally on the smartphone.</p>\n</li>\n<li>\n<p>Smartphones are the eyes and ears of the AI revolution. The neural networks at the centre of current AI practice are trained with data—images, text, sound—from the real world. People don’t just take trillions of photos on smartphones per year, they upload them, meaning that billions of smartphone users worldwide are creating the data that trains modern AI software. Any smartphone app that uses AI to analyze and process images (such as Snapchat Lens, RefaceAI and Google Lens) utilizes neural networks that were trained by this vast and growing ocean of data uploaded from smartphones around the world.</p>\n</li>\n</ul>\n<section id=\"section-smartphone-sensors\" class=\"accordion-section\" data-outputs-include=\"epub,pdf\"> <h2>Smartphone Sensors</h2> <p>AI is not just about processing information—it is also about sensing and collecting information from the outside world. Every mode of sensing has its own parameters of intelligence and performance, and every smartphone sensor needs its own AI software. Here are just some of the sensors that can be found on current smartphones.</p> <ul> <li>Accelerometer: speed and direction of motion</li> <li>Gyroscope: small shifts in physical orientation</li> <li>Magnetometer: digital compass</li> <li>Global Positioning System (GPS): terrestrial location</li> <li>Proximity Sensor: how far objects are from phone</li> <li>Ambient Light Sensor</li> <li>Microphone</li> <li>Touchscreen Sensors</li> <li>Biometric Sensors: fingerprint, iris, full face recognition</li> <li>Pedometer</li> <li>Barcode/QR Code Sensors</li> <li>Barometer: air pressure and altitude</li> <li>Heart Rate Sensor</li> <li>Thermometer</li> </ul> </section> <details class=\"accordion-section\" id=\"section-smartphone-sensors\"  data-outputs-include=\"html\"> <summary class=\"accordion-section__heading accordion-section__heading-level-null accordion-section__controls accordion-section__controls--arrow\" tabindex=\"1\"> <button aria-label=\"Copy page section link to clipboard\" class=\"accordion-section__copy-link-button\" data-outputs-exclude=\"pdf,epub\" value=\"#section-smartphone-sensors\" tabindex=\"2\" > § </button> <span aria-hidden=\"true\" class=\"accordion-tooltip\" data-outputs-exclude=\"pdf,epub\" > Copied page section link to clipboard </span> <h2>Smartphone Sensors</h2> </summary> <section class=\"accordion-section__body\"><p>AI is not just about processing information—it is also about sensing and collecting information from the outside world. Every mode of sensing has its own parameters of intelligence and performance, and every smartphone sensor needs its own AI software. Here are just some of the sensors that can be found on current smartphones.</p> <ul> <li>Accelerometer: speed and direction of motion</li> <li>Gyroscope: small shifts in physical orientation</li> <li>Magnetometer: digital compass</li> <li>Global Positioning System (GPS): terrestrial location</li> <li>Proximity Sensor: how far objects are from phone</li> <li>Ambient Light Sensor</li> <li>Microphone</li> <li>Touchscreen Sensors</li> <li>Biometric Sensors: fingerprint, iris, full face recognition</li> <li>Pedometer</li> <li>Barcode/QR Code Sensors</li> <li>Barometer: air pressure and altitude</li> <li>Heart Rate Sensor</li> <li>Thermometer</li> </ul> </section> </details>\n<section id=\"section-snapchat-lens\" class=\"accordion-section\" data-outputs-include=\"epub,pdf\"> <h2>Snapchat Lens</h2> <p>’</p> <p>Snapchat’s popular Lens tool is a remarkable collection of AI driven features. Snapchat Lens allows users to modify their face, appearance, lighting, clothing, facial hair, props, even their voice. The process is simple, fast and easy to use. But the underlying artificial intelligences needed to identify facial and environmental features; track them accurately; modify and replace them seamlessly and play them back smoothly are quite sophisticated. Using the AI in Snapchat Lens, users can create animated effects that would have required the efforts of a professional animation studio just a few years ago.</p> </section> <details class=\"accordion-section\" id=\"section-snapchat-lens\"  data-outputs-include=\"html\"> <summary class=\"accordion-section__heading accordion-section__heading-level-null accordion-section__controls accordion-section__controls--arrow\" tabindex=\"1\"> <button aria-label=\"Copy page section link to clipboard\" class=\"accordion-section__copy-link-button\" data-outputs-exclude=\"pdf,epub\" value=\"#section-snapchat-lens\" tabindex=\"2\" > § </button> <span aria-hidden=\"true\" class=\"accordion-tooltip\" data-outputs-exclude=\"pdf,epub\" > Copied page section link to clipboard </span> <h2>Snapchat Lens</h2> </summary> <section class=\"accordion-section__body\"><p>’</p> <p>Snapchat’s popular Lens tool is a remarkable collection of AI driven features. Snapchat Lens allows users to modify their face, appearance, lighting, clothing, facial hair, props, even their voice. The process is simple, fast and easy to use. But the underlying artificial intelligences needed to identify facial and environmental features; track them accurately; modify and replace them seamlessly and play them back smoothly are quite sophisticated. Using the AI in Snapchat Lens, users can create animated effects that would have required the efforts of a professional animation studio just a few years ago.</p> </section> </details>\n<section id=\"section-refaceai\" class=\"accordion-section\" data-outputs-include=\"epub,pdf\"> <h2>RefaceAI</h2> <p>Deepfakes<sup class=\"footnote-ref\"><a href=\"#fn1\" id=\"fnref1\">1</a></sup> are an AI driven phenomena that have the potential to challenge a deeply held belief that “seeing is believing.” The AI needed to create a deepfake is substantial—a face must be recognized; properly segmented and labelled; tracked while moving and talking; remapped to another face or body; appropriately lit, and transferred<br> into the new, fake scene. But this “sophisticated” AI is available to everyday consumers as an amusement in free apps. One of the most popular, Reface, allows for animation of still photos, face swaps with existing video, and animation with popular music tracks.</p> <div class=\"backmatter\"> </div> <section class=\"footnotes\"> <ol class=\"footnotes-list\"> <li id=\"fn1\" class=\"footnote-item\"><p>Deepfakes (a portmanteau of ‘deep learning’ and ‘fake’) are synthetic media in which a person in an existing image or video is replaced with someone else’s likeness. <a href=\"#fnref1\" class=\"footnote-backref\">↩︎</a></p> </li> </ol> </section> </section> <details class=\"accordion-section\" id=\"section-refaceai\"  data-outputs-include=\"html\"> <summary class=\"accordion-section__heading accordion-section__heading-level-null accordion-section__controls accordion-section__controls--arrow\" tabindex=\"1\"> <button aria-label=\"Copy page section link to clipboard\" class=\"accordion-section__copy-link-button\" data-outputs-exclude=\"pdf,epub\" value=\"#section-refaceai\" tabindex=\"2\" > § </button> <span aria-hidden=\"true\" class=\"accordion-tooltip\" data-outputs-exclude=\"pdf,epub\" > Copied page section link to clipboard </span> <h2>RefaceAI</h2> </summary> <section class=\"accordion-section__body\"><p>Deepfakes<sup class=\"footnote-ref\"><a href=\"#fn1\" id=\"fnref1\">1</a></sup> are an AI driven phenomena that have the potential to challenge a deeply held belief that “seeing is believing.” The AI needed to create a deepfake is substantial—a face must be recognized; properly segmented and labelled; tracked while moving and talking; remapped to another face or body; appropriately lit, and transferred<br> into the new, fake scene. But this “sophisticated” AI is available to everyday consumers as an amusement in free apps. One of the most popular, Reface, allows for animation of still photos, face swaps with existing video, and animation with popular music tracks.</p> <div class=\"backmatter\"> </div> <section class=\"footnotes\"> <ol class=\"footnotes-list\"> <li id=\"fn1\" class=\"footnote-item\"><p>Deepfakes (a portmanteau of ‘deep learning’ and ‘fake’) are synthetic media in which a person in an existing image or video is replaced with someone else’s likeness. <a href=\"#fnref1\" class=\"footnote-backref\">↩︎</a></p> </li> </ol> </section> </section> </details>\n<section id=\"section-google-pixel-6-camera\" class=\"accordion-section\" data-outputs-include=\"epub,pdf\"> <h2>Google Pixel 6 Camera</h2> <p>Making a simple snapshot on a smartphone invokes multiple AI programs. The camera autofocuses; automatically finds and enhances faces; senses motion so that it can apply a blurring effect; and adjusts for lighting—especially in low light settings. There is a new feature called “Real Tone” that adjusts exposure and colour balance correction based on the skin tone of the subject to provide more accurate representation of diverse skin tones. Photos can be edited with a “Magic Eraser” that allows for easy removal of unwanted background clutter. The computational photography software that drives the Google camera is designed to run on the new Google Tensor chip, a microprocessor designed specifically for fast, low-power processing of the neural networks that are the foundation of contemporary AI techniques.</p> </section> <details class=\"accordion-section\" id=\"section-google-pixel-6-camera\"  data-outputs-include=\"html\"> <summary class=\"accordion-section__heading accordion-section__heading-level-null accordion-section__controls accordion-section__controls--arrow\" tabindex=\"1\"> <button aria-label=\"Copy page section link to clipboard\" class=\"accordion-section__copy-link-button\" data-outputs-exclude=\"pdf,epub\" value=\"#section-google-pixel-6-camera\" tabindex=\"2\" > § </button> <span aria-hidden=\"true\" class=\"accordion-tooltip\" data-outputs-exclude=\"pdf,epub\" > Copied page section link to clipboard </span> <h2>Google Pixel 6 Camera</h2> </summary> <section class=\"accordion-section__body\"><p>Making a simple snapshot on a smartphone invokes multiple AI programs. The camera autofocuses; automatically finds and enhances faces; senses motion so that it can apply a blurring effect; and adjusts for lighting—especially in low light settings. There is a new feature called “Real Tone” that adjusts exposure and colour balance correction based on the skin tone of the subject to provide more accurate representation of diverse skin tones. Photos can be edited with a “Magic Eraser” that allows for easy removal of unwanted background clutter. The computational photography software that drives the Google camera is designed to run on the new Google Tensor chip, a microprocessor designed specifically for fast, low-power processing of the neural networks that are the foundation of contemporary AI techniques.</p> </section> </details>\n","length":1419,"subtitle":null,"title":"The Smartphone","type":"entry","url":"http://localhost:8080/20-objects/18-smartphone/"},{"content":"<p>The test of machine intelligence proposed by Alan Turing in 1950 involved communication via teletype. But the perception of human intelligence involves much more than rudimentary communication. Every day, we judge other people by their ability to respond in a socially acceptable manner, act intelligently, and make the expected eye contact or facial expression.</p>\n<p>MetaHuman Creator, a new animation design tool developed by Epic Games, brings us closer to the creation of characters that can fool us into believing that they are real. The tool allows artists to easily create and animate extraordinarily realistic computer graphics characters. MetaHuman Creator uses AI techniques in several steps of this creation process. Once MetaHuman characters are created, they go “downstream” to the films, games or other applications for which they were intended. They are most often “non-player characters” (NPCs), the equivalent of extras in a film. But as the AI programs that animate these characters become more sophisticated, the NPCs can demonstrate a wide range of believable interactions with other characters and their environment.</p>\n<p>The idea of creating a synthetic human goes back to antiquity—for example, Pygmalion in Ovid’s <em>Metamorphoses</em>—and is a rich and persistent trope in the world of artificial intelligence. From <em>2001: A Space Odyssey</em>’s HAL 9000, to the orcs in <em>Lord of the Rings</em>, and the fluid bodies of Scott Eaton’s figure studies, we strive to bring the appearance of sentience to the machines we make.</p>\n<p>The MetaHuman Creator editor is designed for fast creation and editing of highly realistic human synthetic characters. These videos offer a sample of the various editing modes available. New characters can be created and altered by blending pre-existing characters; facial and body features can be sculpted with a cursor; and specific details such as skin tone, texture and eye colour can be controlled with a high degree of precision.</p>\n","length":302,"subtitle":null,"title":"MetaHuman Creator","type":"entry","url":"http://localhost:8080/20-objects/19-metahuman-creator/"},{"content":"<p>Face recognition software is among the most controversial applications of artificial intelligence. Fundamental questions regarding bias, racial profiling, privacy, civil liberties and accuracy form the core complaints. But facial recognition is also one of the most ubiquitous forms of AI. If you’ve ever posted a photo or video to Facebook, WeChat, Instagram or TikTok; applied for a driver’s license or passport; or engaged in any activity where an image of your face can be connected to your name, then that image has likely become a template within a facial recognition database.</p>\n<p>While much of the concern around facial recognition is with the terms and conditions of its construction of individual identity, there are many more applications and outcomes for which it can be used. Emotion recognition is among the most popular, in part because of the longstanding desire to understand facial expression as a clear and verifiable representation of a person’s emotional state. The debate around emotion and facial expression has been active for centuries. Charles Darwin’s treatise on <em>The Expression of the Emotions in Man and Animals</em> (1872) set the groundwork for a universal theory of expression that was revived in the 1960s and 70s by the American psychologist, Paul Ekman. Ekman proposed that humans worldwide could reliably infer emotional states from facial expressions, which he reduced to seven basic emotions: happiness, sadness, anger, contempt, disgust, fear and surprise. Not surprisingly, a growing body of research quickly emerged to counter Ekman’s proposition arguing that his definitions of facial expression were too limited, and that a broader physiological analysis was needed along with an understanding of context.</p>\n<p>However, much of the emotion recognition software that is produced today has its basis in Ekman’s thesis—a reductive set of seven emotions and a belief in the cultural universality of those expressions. This is a framework that is especially amenable to computation and computer vision—and is equally appealing to the magical thinking that drives much of the marketing campaigns and advertising schemes of contemporary life.</p>\n<br>\n<div class=\"backmatter\">\n<p>This interactive encounter with emotion recognition is the result of a collaboration with Vancouver’s Centre for Digital Media, under the direction of Larry Bafia. Graduate students in this program worked as a team to design, program and produce this wall. Many thanks to that talented team:</p>\n<p>Valentina Forte-Hernandez: Project Manager<br>\nCourtney Clarkson: UX / UI Designer<br>\nJulia Read: UX / UI Designer<br>\nVlad Ryzhov: Software Programmer<br>\nVlad Ryzhov contributed additional post-production programming support</p>\n<p>The Centre for Digital Media was founded in 2007 and is a unique graduate program whose degree is imprinted with the seals of its four partner institutions: University of British Columbia, Emily Carr University of Art + Design, Simon Fraser University and British Columbia Institute of Technology.</p>\n<p>The emotion recognition software used in this interactive installation is produced by Visage Technologies, founded in Linköping, Sweden.</p>\n</div>\n","length":461,"subtitle":null,"title":"Emotion Recognition","type":"entry","url":"http://localhost:8080/20-objects/20-emotion-recognition/"},{"content":"","length":0,"title":"Special Projects: Pause","type":"table-of-contents","url":"http://localhost:8080/special-projects-pause/"},{"content":"<p>Preference engines guide us daily as we move through the world. They suggest products, services and information. They give shape to our experiences, behaviours and choices. The news you read, the music you listen to, the clothing you buy, the video you watch, and the information you seek were probably offered by a preference engine. In this sense, we have all become data within a massive artificial intelligence that tirelessly monitors our activities with the intent of gauging our<br>\ninterest and engagement.</p>\n<p>Curiously, most art galleries and museums have actively avoided the type of monitoring that is required to make accurate predictions around visitor preference. Some galleries monitor attendance and ticket sales—occasionally taking user surveys or consulting with focus groups—but artificial intelligence provides us with some new options.</p>\n<p>We commissioned <em>Creepers</em> as a tool to track visitor movement and attention in this exhibition space. As visitors enter this portion of the exhibition, they are tracked by human detection software that assigns them an individual number and colour; and their movement is tracked and displayed on a nearby monitor with a coloured line that marks their path in real time. When they stop to look at an object or image, a slowly growing concentric circle marks the place and duration of their pause. With this simple tool, we can easily gauge visitor behaviour: do people look at everything in the room or only a few select things? What’s the path of their movement? Do they pause to look at an artwork or read a label? How long do they stop? Which works attract the most attention? Which attract the least?</p>\n<br>\n<div class=\"backmatter\">\n<p>This interactive encounter with visitor tracking is the result of a collaboration with Vancouver’s Centre for Digital Media, under the direction of Larry Bafia. Graduate students in this program worked as a team to design, program and produce this wall.</p>\n<p>Many thanks to that talented team:</p>\n<ul>\n<li>Mary Wilson: Project Manager, UI/UX Designer</li>\n<li>Shruti Sharma: Project Manager, UX Designer</li>\n<li>Yuri Wu – Artist / UX Designer</li>\n<li>Cindy Shi – Software Programmer</li>\n<li>Min Kyu Choi – Software Programme</li>\n<li>Jason Elliot – Faculty Advisor</li>\n</ul>\n<p>Cindy Shi contributed additional post-production programming support.</p>\n<p>The Centre for Digital Media was founded in 2007 and is a unique graduate program whose degree is imprinted with the seals of its four partner institutions: University of British Columbia, Emily Carr University of Art + Design, Simon Fraser University and British Columbia Institute of Technology.</p>\n</div>","length":392,"subtitle":null,"title":"Creepers","type":"entry","url":"http://localhost:8080/special-projects-pause/sp-creepers/"},{"content":"<p>The design group at *airegan produces a complex working model that might best be described as having some similarity to the latent space of a neural network—which is to say, a multidimensional space that we cannot interpret directly, but which encodes a meaningful internal representation of externally observed events.</p>\n<p>Evoking Marcel Duchamp, the artists and designers at *airegan see the sneaker as a readymade: &quot;…the meticulous assembly of bootlegs, one of a kind sneakers and mass market brands—including couture and hype brands. The series further references qualities of readymade sneakers, and their form factors reflect in the final works by reapproaching, redefining and reappropriating the readymade.</p>\n<p>Using the tools of generative adversarial networks (GANs), *airegan offers the unlikely possibility of a new kind of readymade, as well as an unexpected reconfiguration of the terms of creativity in the age of artificial intelligence.</p>\n","length":140,"subtitle":null,"title":"*airegan","type":"entry","url":"http://localhost:8080/special-projects-pause/sp-airegan/"},{"content":"<p>Amber Frid-Jimenez is a Vancouver-based interdisciplinary artist with a background in computational design and the creation of experimental computer programs, platforms and applications.</p>\n<p><em>Après Ballet mécanique</em> (2018) is a video created by Frid-Jimenez that uses artificial intelligence to generate a new configuration of Fernand Léger’s experimental film <em>Ballet mécanique</em> (1924). Ballet mécanique is an important and influential work within European modernist art of the early 20th century. Using aggressively experimental film techniques and subject matter, Léger created a new kind of image.</p>\n<p>To produce her work, <em>Après Ballet mécanique</em>, Frid-Jimenez takes a similarly experimental approach, using Léger’s film to construct a learning set—the dataset of images used to “train” an artificial neural network—and then reconstruct a new version of the film from within the multidimensional image space produced by the neural network. The result is a very different film, reflecting a sense of time, space and form that is firmly rooted in the age of artificial intelligence.</p>\n","length":155,"subtitle":null,"title":"Amber Frid-Jimenez","type":"entry","url":"http://localhost:8080/special-projects-pause/sp-frid-jimenez/"},{"content":"<p>Responding to a complex building site, the Bjarke Ingels Group utilized the power of parametric design to produce a Vancouver landmark. Vancouver House was designed in response to a number of unique parameters that resulted from its constrained site, an option for expanded airspace, and a commitment to public space at the foot of the tower.</p>\n<p>In an extraordinary feat of design, the building structure emerges from a triangular floorplate that twists and expands slowly as it rises, achieving a rectangular form near the top of its 59 storeys.</p>\n<p>The building’s design relies heavily on the capacity of computational design to manage the variety and incremental shifting of form as the building rises and rotates.</p>\n","length":113,"subtitle":null,"title":"Bjarke Ingels Group Vancouver House","type":"entry","url":"http://localhost:8080/special-projects-pause/sp-bjarke-ingels/"},{"content":"","length":0,"title":"Special Projects: Artist Projects","type":"table-of-contents","url":"http://localhost:8080/special-projects-artists-projects/"},{"content":"<p>What does AI sound like?</p>\n<p>AI permeates our world, but is the sound of AI something specific, or is it the subtle reshaping of the soundscape that surrounds our daily lives? <em>Sounds Like AI</em> is a long-form soundscape that integrates sounds common to the Gallery’s exhibition space (for example, the constant hum of the escalator), sounds that we typically associate with computers and AI (for example, sound effects from films), and random snippets of speech that resemble the talkin AI agents we encounter everyday (for example, Alexa, Siri and “Hey Google.”)</p>\n<p>Patrick Pennefather describes the work as “a sonic resting point amidst the often disorienting and chaotic sounds we experience in public spaces.”</p>\n<p>The rotunda soundscape, <em>Are you Talking to Me?</em>, interactively tracks visitors using a motion tracking system and targets individual visitors with specialized speakers that have an extremely tight focus. The selection of what audio is played is determined partly by where in the rotunda that audio is being projected by the tightly focused speakers, and in that sense audio content is also responsive to each visitor’s movements. The result is a constantly changing and never-repeating sound experience that is shaped by the movement of the visitors who experience it.</p>\n","length":199,"subtitle":null,"title":"Patrick Pennefather","type":"entry","url":"http://localhost:8080/special-projects-artists-projects/sp-pennefather/"},{"content":"<p>Sougwen Chung is a Chinese-Canadian artist and researcher currently based in London, England. Over the past decade, she has engaged in research that links humans and machines in an intricate and compelling collaboration. More recently she has extended that collaboration to include a broader range of ecologies—seeking to produce new modes of creativity that blur the conventional boundaries between human and non-human knowledge.</p>\n<p>Her installation for this exhibition offers a selected introduction to some of the key bodies of work she has produced in the last five years. With an extensive background in computational science and art, Chung utilizes machine learning, computer vision, neural networks and advanced robotics in a speculative practice that finds meaning in flow, change, entanglement and indeterminacy.</p>\n<p>Performance is a critical element of Chung’s practice—where human and machine meet in their most intimate and inextricable engagement. In a recent text Chung called for a recognition of “machine intelligence, flux and unknown destinations. Machine co-creation—not a replication or automation of human endeavour but a mechanism (mysticism) developed toward the transformation of the human subject. A form of mysticism.”</p>\n<section id=\"section-d-o-u-g-2-dataset-sample\" class=\"accordion-section\" data-outputs-include=\"epub,pdf\"> <h2>D.O.U.G.2 Dataset Sample</h2> <p>The artist collected and scanned samples of her previous drawings and sketches—100 are shown in the exhibition—to produce a memory bank for D.O.U.G._2. She has since continued to add drawings focusing on gesture and colour palette. Together, they constitute a collective memory that the robotic arm could draw on when creating these two works.</p> </section> <details class=\"accordion-section\" id=\"section-d-o-u-g-2-dataset-sample\"  data-outputs-include=\"html\"> <summary class=\"accordion-section__heading accordion-section__heading-level-null accordion-section__controls accordion-section__controls--arrow\" tabindex=\"1\"> <button aria-label=\"Copy page section link to clipboard\" class=\"accordion-section__copy-link-button\" data-outputs-exclude=\"pdf,epub\" value=\"#section-d-o-u-g-2-dataset-sample\" tabindex=\"2\" > § </button> <span aria-hidden=\"true\" class=\"accordion-tooltip\" data-outputs-exclude=\"pdf,epub\" > Copied page section link to clipboard </span> <h2>D.O.U.G.2 Dataset Sample</h2> </summary> <section class=\"accordion-section__body\"><p>The artist collected and scanned samples of her previous drawings and sketches—100 are shown in the exhibition—to produce a memory bank for D.O.U.G._2. She has since continued to add drawings focusing on gesture and colour palette. Together, they constitute a collective memory that the robotic arm could draw on when creating these two works.</p> </section> </details>\n<section id=\"section-d-o-u-g-l-a-s\" class=\"accordion-section\" data-outputs-include=\"epub,pdf\"> <h2>D.O.U.G.L.A.S.</h2> <p>The drawing robots on the centre platform collaborated with the artist in the production of the painting you see there. They are part of a “family” or “swarm” of robots that work collectively—with Chung and with each other—to produce gestures that reflect their interactions in a shared space. In part, the robots’ movements are based on data gathered by surveillance cameras in public spaces. The data is extracted through algorithms that capture optical flow,<sup class=\"footnote-ref\"><a href=\"#fn1\" id=\"fnref1\">1</a></sup> which can be defined as the pattern of motion of objects in a visual scene—velocity, direction, density and dwell time. Two paintings from this series—<em>Dwell Study</em> (2018) and <em>Direction Study</em> (2018)—hang nearby.</p> <p>Optical flow is a critical field of study in computer vision and machine movement, and Chung’s project emerged in part from the Experiments in Arts and Technology (E.A.T.) residency at the legendary Bell Labs. The E.A.T. collective first paired engineers from Bell Labs with artists in 1966, and this visionary partnership continues to support collaborative artistic projects that engage emergent technologies.</p> <div class=\"backmatter\"> </div> <section class=\"footnotes\"> <ol class=\"footnotes-list\"> <li id=\"fn1\" class=\"footnote-item\"><p>Optical flow is a technique used to describe image motion. It is usually applied to a series of images that have a small time step between them, for example, video frames. Optical flow calculates a velocity for points within the images, and provides an estimation of where points could be in the next image sequence. <a href=\"#fnref1\" class=\"footnote-backref\">↩︎</a></p> </li> </ol> </section> </section> <details class=\"accordion-section\" id=\"section-d-o-u-g-l-a-s\"  data-outputs-include=\"html\"> <summary class=\"accordion-section__heading accordion-section__heading-level-2 accordion-section__controls accordion-section__controls--arrow\" tabindex=\"1\"> <button aria-label=\"Copy page section link to clipboard\" class=\"accordion-section__copy-link-button\" data-outputs-exclude=\"pdf,epub\" value=\"#section-d-o-u-g-l-a-s\" tabindex=\"2\" > § </button> <span aria-hidden=\"true\" class=\"accordion-tooltip\" data-outputs-exclude=\"pdf,epub\" > Copied page section link to clipboard </span> <h2>D.O.U.G.L.A.S.</h2> </summary> <section class=\"accordion-section__body\"><p>The drawing robots on the centre platform collaborated with the artist in the production of the painting you see there. They are part of a “family” or “swarm” of robots that work collectively—with Chung and with each other—to produce gestures that reflect their interactions in a shared space. In part, the robots’ movements are based on data gathered by surveillance cameras in public spaces. The data is extracted through algorithms that capture optical flow,<sup class=\"footnote-ref\"><a href=\"#fn1\" id=\"fnref1\">1</a></sup> which can be defined as the pattern of motion of objects in a visual scene—velocity, direction, density and dwell time. Two paintings from this series—<em>Dwell Study</em> (2018) and <em>Direction Study</em> (2018)—hang nearby.</p> <p>Optical flow is a critical field of study in computer vision and machine movement, and Chung’s project emerged in part from the Experiments in Arts and Technology (E.A.T.) residency at the legendary Bell Labs. The E.A.T. collective first paired engineers from Bell Labs with artists in 1966, and this visionary partnership continues to support collaborative artistic projects that engage emergent technologies.</p> <div class=\"backmatter\"> </div> <section class=\"footnotes\"> <ol class=\"footnotes-list\"> <li id=\"fn1\" class=\"footnote-item\"><p>Optical flow is a technique used to describe image motion. It is usually applied to a series of images that have a small time step between them, for example, video frames. Optical flow calculates a velocity for points within the images, and provides an estimation of where points could be in the next image sequence. <a href=\"#fnref1\" class=\"footnote-backref\">↩︎</a></p> </li> </ol> </section> </section> </details>\n<section id=\"section-flora-rearing-agricultural-network-f-r-a-n\" class=\"accordion-section\" data-outputs-include=\"epub,pdf\"> <h2>Flora Rearing Agricultural Network (F.R.A.N.)</h2> <p>In this video work, Sougwen Chung establishes the framework for an ongoing project that is connected to nature through a process linked to the artist’s biorhythms. Here, Chung explores new modes of relation between humans, plants and machines.</p> </section> <details class=\"accordion-section\" id=\"section-flora-rearing-agricultural-network-f-r-a-n\"  data-outputs-include=\"html\"> <summary class=\"accordion-section__heading accordion-section__heading-level-null accordion-section__controls accordion-section__controls--arrow\" tabindex=\"1\"> <button aria-label=\"Copy page section link to clipboard\" class=\"accordion-section__copy-link-button\" data-outputs-exclude=\"pdf,epub\" value=\"#section-flora-rearing-agricultural-network-f-r-a-n\" tabindex=\"2\" > § </button> <span aria-hidden=\"true\" class=\"accordion-tooltip\" data-outputs-exclude=\"pdf,epub\" > Copied page section link to clipboard </span> <h2>Flora Rearing Agricultural Network (F.R.A.N.)</h2> </summary> <section class=\"accordion-section__body\"><p>In this video work, Sougwen Chung establishes the framework for an ongoing project that is connected to nature through a process linked to the artist’s biorhythms. Here, Chung explores new modes of relation between humans, plants and machines.</p> </section> </details>\n","length":996,"subtitle":null,"title":"Sougwen Chung","type":"entry","url":"http://localhost:8080/special-projects-artists-projects/sp-chung/"},{"content":"<p>Scott Eaton is an American-born artist based in London. As a long-time student of the human figure, Eaton has built a commercial database of human figures—both still and in motion—and has produced an extensive body of photographic and hand-drawn figurative work. He teaches human anatomy and figure drawing for artists, and consults for a variety of video game and animation production houses.</p>\n<p>The work in this room addresses one of the oldest subjects of art: the human body. But the tools used for this exploration employ some of the most recent and sophisticated advances in artificial intelligence: neural networks.</p>\n<p>The “Bodies” neural network that produced the human bodies displayed here was designed and trained by the artist with over 25,000 photographs of carefully lit and staged human figures. As neural networks can only recognize and produce the types of images on which they were trained, the “Bodies” network can only “see” the human form.</p>\n<p>Consequently, in its interactions with Eaton, the network can only produce the human body. The network responds to lines drawn by Eaton by attempting to give a fleshy surface and lighting to that form. No matter what image Eaton feeds into the network, the digital output will be a human body that is shaped by the parameters of the input image. Draw a line, get back a body part. Capture fluids in motion, get back fluid bodies in motion.</p>\n<p><em>Entangled II</em> was produced using high-speed video of fluid movement shot with macro lenses, then processed through a custom pix2pixHD model (a type of neural network) trained on the artist’s “Bodies” dataset of more than 25,000 images of the human body in motion.</p>\n<section id=\"section-humanity-fall-of-the-damned\" class=\"accordion-section\" data-outputs-include=\"epub,pdf\"> <h2>Humanity (Fall of the Damned)</h2> <p>To create this work, a large composition, hand-drawn by the artist, was processed by a custom neural network that was trained on the artist’s “Bodies” dataset. Eaton was inspired by the many large-scale depictions of the Last Judgment that were produced during the Renaissance. The computer processing required to render the complex volumes and shading for each body in such a large-scale and highly detailed image posed a significant technical challenge. To resolve this problem, Eaton produced the image in smaller, tiled pieces, which he then reassembled for printing.</p> </section> <details class=\"accordion-section\" id=\"section-humanity-fall-of-the-damned\"  data-outputs-include=\"html\"> <summary class=\"accordion-section__heading accordion-section__heading-level-null accordion-section__controls accordion-section__controls--arrow\" tabindex=\"1\"> <button aria-label=\"Copy page section link to clipboard\" class=\"accordion-section__copy-link-button\" data-outputs-exclude=\"pdf,epub\" value=\"#section-humanity-fall-of-the-damned\" tabindex=\"2\" > § </button> <span aria-hidden=\"true\" class=\"accordion-tooltip\" data-outputs-exclude=\"pdf,epub\" > Copied page section link to clipboard </span> <h2>Humanity (Fall of the Damned)</h2> </summary> <section class=\"accordion-section__body\"><p>To create this work, a large composition, hand-drawn by the artist, was processed by a custom neural network that was trained on the artist’s “Bodies” dataset. Eaton was inspired by the many large-scale depictions of the Last Judgment that were produced during the Renaissance. The computer processing required to render the complex volumes and shading for each body in such a large-scale and highly detailed image posed a significant technical challenge. To resolve this problem, Eaton produced the image in smaller, tiled pieces, which he then reassembled for printing.</p> </section> </details>\n<section id=\"section-caffeinated-diversions\" class=\"accordion-section\" data-outputs-include=\"epub,pdf\"> <h2>Caffeinated Diversions</h2> <p>Sophisticated neural networks are complex and can produce surprising results. Extensive experimentation is necessary to tune the design and training of a neural network, and to discover what it can do. <em>Caffeinated Diversions</em> presents a selection of Scott Eaton’s daily exercises to discover what he can produce with the AI neural networks he has trained.</p> <p>Each starts with a sketch—a simple line drawing. Then, the neural network takes over as it tries to process the sketch through the patterns on which it was trained; in this case, thousands of photographs of human bodies in various positions taken by the artist. As Eaton describes it: “in some ways the neural network was training me in reverse to draw differently, to get it to do the right thing.”</p> </section> <details class=\"accordion-section\" id=\"section-caffeinated-diversions\"  data-outputs-include=\"html\"> <summary class=\"accordion-section__heading accordion-section__heading-level-null accordion-section__controls accordion-section__controls--arrow\" tabindex=\"1\"> <button aria-label=\"Copy page section link to clipboard\" class=\"accordion-section__copy-link-button\" data-outputs-exclude=\"pdf,epub\" value=\"#section-caffeinated-diversions\" tabindex=\"2\" > § </button> <span aria-hidden=\"true\" class=\"accordion-tooltip\" data-outputs-exclude=\"pdf,epub\" > Copied page section link to clipboard </span> <h2>Caffeinated Diversions</h2> </summary> <section class=\"accordion-section__body\"><p>Sophisticated neural networks are complex and can produce surprising results. Extensive experimentation is necessary to tune the design and training of a neural network, and to discover what it can do. <em>Caffeinated Diversions</em> presents a selection of Scott Eaton’s daily exercises to discover what he can produce with the AI neural networks he has trained.</p> <p>Each starts with a sketch—a simple line drawing. Then, the neural network takes over as it tries to process the sketch through the patterns on which it was trained; in this case, thousands of photographs of human bodies in various positions taken by the artist. As Eaton describes it: “in some ways the neural network was training me in reverse to draw differently, to get it to do the right thing.”</p> </section> </details>\n<section id=\"section-figures-rectangles-and-reflection\" class=\"accordion-section\" data-outputs-include=\"epub,pdf\"> <h2>Figures, Rectangles and Reflection</h2> <p>In this new work, the initial output is produced by a StyleGan3 generator trained on the artist’s “Bodies” dataset, and then processed by a custom Pix2PixHD model which has been trained on simple geometric shapes. The final output is rendered in Octane Render.</p> </section> <details class=\"accordion-section\" id=\"section-figures-rectangles-and-reflection\"  data-outputs-include=\"html\"> <summary class=\"accordion-section__heading accordion-section__heading-level-null accordion-section__controls accordion-section__controls--arrow\" tabindex=\"1\"> <button aria-label=\"Copy page section link to clipboard\" class=\"accordion-section__copy-link-button\" data-outputs-exclude=\"pdf,epub\" value=\"#section-figures-rectangles-and-reflection\" tabindex=\"2\" > § </button> <span aria-hidden=\"true\" class=\"accordion-tooltip\" data-outputs-exclude=\"pdf,epub\" > Copied page section link to clipboard </span> <h2>Figures, Rectangles and Reflection</h2> </summary> <section class=\"accordion-section__body\"><p>In this new work, the initial output is produced by a StyleGan3 generator trained on the artist’s “Bodies” dataset, and then processed by a custom Pix2PixHD model which has been trained on simple geometric shapes. The final output is rendered in Octane Render.</p> </section> </details>","length":945,"subtitle":null,"title":"Scott Eaton","type":"entry","url":"http://localhost:8080/special-projects-artists-projects/sp-eaton/"},{"content":"<p>The Zombie Formalist is the first product of its kind.</p>\n<p>The Zombie Formalist is a self-contained generative lightbox that uses an embedded Artificial Intelligence (AI) system to learn your aesthetic preferences. It has a limitless capacity to create new geometric abstract compositions tailored specifically for you.</p>\n<p>The Zombie Formalist will show you more of what you want to see. It pays attention to artworks that pique your interest, and then creates new compositions with similar characteristics. The Zombie Formalist is an artist itself: based on your input, it creates unique works in real time just for you.</p>\n<p>The Zombie Formalist can upload artworks to Twitter where the engagement of your followers will further determine its aesthetic decision making—producing compositions that your friends will appreciate.</p>\n<p>By clicking here, you can join over 1000 fans who engage with the Zombie Formalist on Twitter. Follow the Zombie Formalist and your likes and retweets will teach the AI system what is aesthetically pleasing to you while improving its future compositions!</p>\n<p>In conjunction with <em>The Imitation Game: Visual Culture in the Age of Artificial Intelligence</em>, the Zombie Formalist has created a series of exclusive, limited edition products for sale. These items are based on the Gallery Store’s bestselling merchandise and feature digital compositions that rated highly on Twitter.</p>\n","length":208,"subtitle":null,"title":"The Zombie Formalist","type":"entry","url":"http://localhost:8080/special-projects-artists-projects/sp-zombie/"},{"content":"<p><strong>A</strong></p>\n<dl>\n<dt>Algorithm</dt>\n<dd>An algorithm is a set of instructions or a recipe for solving a problem or accomplishing a task.</dd>\n<dt>Algorithmic Bias</dt>\n<dd>A phenomenon that occurs when an AI algorithm produces results that are systemically prejudiced due to erroneous assumptions in the machine learning process.</dd>\n<dt>Algorithmic Design or Computational Design</dt>\n<dd>Algorithmic design or computational design is defined as the ways in which design meaning, intentions and knowledge are constructed through computational thinking, representing, sensing and making.</dd>\n<dt>Artificial Intelligence</dt>\n<dd>Artificial Intelligence (AI) is the ability of a digital computer or computer-controlled robot to perform tasks commonly associated with intelligent beings.</dd>\n</dl>\n<p><strong>B</strong></p>\n<dl>\n<dt>Biometric Surveillance Technologies</dt>\n<dd>Biometric technology is the use of human characteristics to identify individuals and is a form of surveillance. The word biometric is derived from the Greek words bio (which means life) and metric (which means to measure). Common forms of biometrics are fingerprint scanners and face identification.</dd>\n<dt>Boids</dt>\n<dd>Boids is an artificial life program, developed by Craig Reynolds in 1986, which simulates the flocking behaviour of birds.</dd>\n</dl>\n<p><strong>C</strong></p>\n<dl>\n<dt>Co-naturality</dt>\n<dd>Co-naturality refers to a sympathy that exists been two objects that share the same nature.</dd>\n<dt>Computer Interface</dt>\n<dd>The human-machine interface, also called user interface or human-computer interface, is the means by which humans and computers communicate with each other. The human-machine interface includes the hardware and software that is used to translate user (i.e. human) input into commands and to present results to the user.</dd>\n<dt>Computer Vision</dt>\n<dd>Computer vision is a field of artificial intelligence that enables computers and systems to derive meaningful information from digital images, videos and other visual inputs.</dd>\n<dt>Cybernetics</dt>\n<dd>Norbert Wiener introduced the term “cybernetics” in 1948 and described it as “the science of control and communications in the animal and machine.”</dd>\n</dl>\n<p><strong>D</strong></p>\n<dl>\n<dt>Dataset</dt>\n<dd>A dataset is a collection of data that can be used to train an algorithm with the goal of finding predictable patterns inside the whole dataset.</dd>\n<dt>Deep Learning</dt>\n<dd>Deep Learning is a subset of machine learning, which is essentially a neural network with three or more layers.</dd>\n<dt>Deepfakes</dt>\n<dd>Deepfakes (a portmanteau of &quot;deep learning&quot; and &quot;fake&quot;) are synthetic media in which a person in an existing image or video is replaced with someone else’s likeness.</dd>\n</dl>\n<p><strong>F</strong></p>\n<dl>\n<dt>Facial Recognition System</dt>\n<dd>A facial recognition system is a technology capable of matching a human face from a digital image or a video frame against a database of faces, typically employed to authenticate users through ID verification services. It works by pinpointing and measuring facial features from a given image.</dd>\n<dt>Funicular Structure</dt>\n<dd>The funicular concept can be best described and visualized with cables or chains, suspended from two points, that adjust their form for any load in tension.</dd>\n</dl>\n<p><strong>G</strong></p>\n<dl>\n<dt>Game Theory</dt>\n<dd>Game theory is the study of mathematical models of strategic interactions among rational agents.</dd>\n<dt>Generative Adversarial Network</dt>\n<dd>A generative adversarial network (GAN) is a class of machine learning frameworks in which two neural networks train each other by competing in a zero-sum game, where one agent’s gain is another agent’s loss.</dd>\n</dl>\n<p><strong>L</strong></p>\n<dl>\n<dt>Lissajous Curve</dt>\n<dd>A Lissajous curve is the graph of a system of parametric equations which describe complex harmonic motion.</dd>\n</dl>\n<p><strong>M</strong></p>\n<dl>\n<dt>Machine Learning</dt>\n<dd>Machine learning is an application of AI that enables systems to learn and improve from experience without being explicitly programmed.</dd>\n<dt>The Mechanical Turk</dt>\n<dd>The Turk, also known as the Mechanical Turk or Automaton Chess Player, was a fake chess-playing machine constructed in the late 18th century.</dd>\n</dl>\n<p><strong>N</strong></p>\n<dl>\n<dt>Neural Network</dt>\n<dd>Neural networks are computing systems with interconnected nodes that work much like neurons in the human brain. Using algorithms, they can recognize hidden patterns and correlations in raw data, cluster and classify it, and–over time–continuously learn and improve.</dd>\n</dl>\n<p><strong>O</strong></p>\n<dl>\n<dt>Optical Flow</dt>\n<dd>Optical flow is a technique used to describe image motion. It is usually applied to a series of images that have a small time step between them, for example, video frames. Optical flow calculates a velocity for points within the images, and provides an estimation of where points could be in the next image sequence.</dd>\n</dl>\n<p><strong>P</strong></p>\n<dl>\n<dt>Parametric Design</dt>\n<dd>Parametric design is understood as a process where a description of a problem is created using variables. By changing these variables a range of alternative solutions can be created, then based on some criteria a final solution selected.</dd>\n<dt>Preference Engine</dt>\n<dd>A preference engine, or a recommender system, is a subclass of information filtering systems that seeks to predict the &quot;rating&quot; or &quot;preference&quot; a user would give to an item.</dd>\n<dt>Prompt Engineering</dt>\n<dd>Prompt engineering or prompt programming is an interesting way to interact with GPT-3 neural network systems. It basically involves creating clever text-based scripts that make GPT-3 perform the tasks you desire.</dd>\n</dl>\n<p><strong>R</strong></p>\n<dl>\n<dt>Reinforcement Learning</dt>\n<dd>Reinforcement learning is an area of machine learning concerned with how intelligent agents ought to take actions in an environment in order to maximize the notion of cumulative reward. Reinforcement learning is one of three basic machine learning paradigms, alongside supervised learning and unsupervised learning.</dd>\n</dl>\n<p><strong>S</strong></p>\n<dl>\n<dt>System Dynamics</dt>\n<dd>System dynamics is an approach to understanding the nonlinear behaviour of complex systems over time using stocks, flows, internal feedback loops, table functions and time delays.</dd>\n</dl>\n<p><strong>T</strong></p>\n<dl>\n<dt>Turing Test</dt>\n<dd>The Turing test, originally called “the imitation game” by Alan Turing in 1950, is a test of a machine's ability to exhibit intelligent behaviour equivalent to, or indistinguishable from, that of a human.</dd>\n</dl>\n<p><strong>W</strong></p>\n<dl>\n<dt>WordNet</dt>\n<dd>WordNet is the name used for lexical databases derived from the original Princeton WordNet; they group words into synonym sets and interlink them using lexical and conceptual-semantic relations. These databases are used for computational linguistics and natural language processing.</dd>\n</dl>\n","length":852,"subtitle":null,"title":"AI Glossary","type":"essay","url":"http://localhost:8080/ai-glossary/"},{"content":"<ul class='quire-contributors-list bio align-left'>             <li class=\"quire-contributor\" id=\"glenn-entis\">   <div class=\"title is-5\">     <span class=\"quire-contributor__name\">Glenn Entis</span>    </div>   <div class=\"media\">     <div class=\"quire-contributor__details media-content\">         <ul>        </ul>      </div>   </div> </li> <li class=\"quire-contributor\" id=\"bruce-grenville\">   <div class=\"title is-5\">     <span class=\"quire-contributor__name\">Bruce Grenville</span>    </div>   <div class=\"media\">     <div class=\"quire-contributor__details media-content\">        <div class=\"quire-contributor__bio\">         Bruce Grenville was the Senior Curator at the Vancouver Art Gallery from 1997 to 2022. During that time he organized many thematic group exhibitions including <em>The Imitation Game: Visual Culture in the Age of Artificial Intelligence</em> (2022);  <em>Cabin Fever</em> (2018), an historical survey of the cabin typology in North American architecture and visual culture; <em>MashUp: The Birth of Modern Culture</em> (2016), an exhibition and publication focused on the history of mashup culture from 1912 to the present; <em>Massive Change: The Future of Global Design</em> (2004), a survey of contemporary design, conceived and presented in collaboration with Bruce Mau Design and the Institute Without Boundaries; and <em>Home and Away: Crossing Cultures on the Pacific Rim</em> (2003), a look at the work of six artists who share a history of emigration and diaspora on the Pacific Rim. He also organized numerous solo exhibitions for artists including Carol Sawyer, Janet Cardiff and George Bures Miller, Michael Lin, Fiona Tan, Stan Douglas, Franz West, Wang Du, Gathie Falk, Dominique Blain, Komar and Melamid, Arnaud Maggs, Christos Dikeakos, Ruth Cuthand, Mary Scott and Jack Goldstein.       </div>        <ul>        </ul>      </div>   </div> </li>            </ul>\n","length":346,"title":"Contributors","type":"page","url":"http://localhost:8080/contributors/"},{"content":"<p>Published in conjunction with the exhibition <em>The Imitation Game: Visual Culture in the Age of Artificial Intelligence</em>, organized by the Vancouver Art Gallery, curated by Bruce Grenville, Senior Curator, and Glenn Entis, Guest Curator, and presented from March 5 to October 23, 2022.<br>\n<br/></p>\n<p>Editor:<br>\nCopyeditor and production coordinator:<br>\nDesign:<br>\nPhotography and Digital Image Preparation:</p>\n<br/>\n<p>This publication was created using Quire<sup>TM</sup>, a multiplatform publishing tool created by the J. Paul Getty Trust</p>\n<p>© 2023 Vancouver Art Gallery</p>\n<p>ISBN 978-1-927656-55-6</p>\n<p>All rights reserved. No part of this book may be reproduced, stored in a retrieval system or transmitted, in any form or by any means, without the prior written consent of the publisher.</p>\n<br/>\n<br/>\n<p><strong>Publication Support:</strong></p>\n<p>Visionary Partner for Scholarship and Publications:<br>\nThe Richardson Family</p>\n<p><strong>Exhibition Support:</strong></p>\n<p>Generously supported by:<br>\nJane Irwin and Ross Hill<br>\nThe Poseley Family<br>\nRick Erickson and Donna Partridge</p>\n<p>Supporting Sponsor:<br>\n<img src=\"/_assets/images/nicola.jpg\" alt=\"Nicola Wealth\"></p>\n<p>AI Youth Programs Sponsor:<br>\nThe Dr. Michael Smith Science Fair Endowment</p>\n<p>Additional support from:<br>\nThe S.M. Blair Family Foundation<br>\nTraction on Demand</p>\n<p>The Vancouver Art Gallery is a not-for-profit organization supported by its members, individual donors, corporate funders, foundations, the City of Vancouver, the Province of British Columbia through the British Columbia Arts Council, and the Canada Council for the Arts.</p>\n<br/>\n<p><img src=\"/_assets/images/vaglogo-colour.jpg\" alt=\"Vancouver Art Gallery Logo\"></p>\n<p>Vancouver Art Gallery<br>\n750 Hornby Street, Vancouver, BC, V6Z 2H7<br>\n<a href=\"https://www.vanartgallery.bc.ca\" target=\"_blank\">vanartgallery.bc.ca</a></p>\n<br/>\nThe Vancouver Art Gallery respectfully acknowledges its location on the traditional, ancestral and unceded territories of the xʷməθkʷəy̓əm (Musqueam) Sḵwx̱wú7mesh (Squamish) and səlilwətaɬ (Tsleil-Waututh) peoples, and honours the Indigenous stewards of the land whose rich cultures are fundamental to artistic life in our province and the work of the Gallery.\n","length":247,"title":"Colophon","type":"page","url":"http://localhost:8080/colophon/"}]
